
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>三. 神经网络BP算法原理和推导 - 大白的知识库</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1bp" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="大白的知识库" class="md-header__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            大白的知识库
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              三. 神经网络BP算法原理和推导
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../machine%20learning/" class="md-tabs__link">
        机器学习
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        机器学习补充
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../recommender%20systems%20foundation/" class="md-tabs__link">
        推荐系统基础
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="大白的知识库" class="md-nav__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    大白的知识库
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/1.%20linear%20regression-1%20v/" class="md-nav__link">
        一. 单变量线性回归
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/2.%20linear%20regression-m%20v/" class="md-nav__link">
        二. 多变量线性回归
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/3.%20logistic%20regression/" class="md-nav__link">
        三. 逻辑回归
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/4.%20regularization/" class="md-nav__link">
        四. 正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/5.%20Neural%20Networks-R/" class="md-nav__link">
        五. 神经网络-表达
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../machine%20learning/6.%20Neural%20Networks-L/" class="md-nav__link">
        六. 神经网络-学习
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          机器学习补充
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习补充" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          机器学习补充
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../1.%20M%20derivation%20essence/" class="md-nav__link">
        一. 矩阵求导本质
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2.%20M%20derivation/" class="md-nav__link">
        二. 矩阵求导
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          三. 神经网络BP算法原理和推导
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        三. 神经网络BP算法原理和推导
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1bp" class="md-nav__link">
    1、反向传播算法和BP网络简介
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2、信息向前传播
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3、误差反向传播
  </a>
  
    <nav class="md-nav" aria-label="3、误差反向传播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    3.1 输出层的权重参数更新
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    3.2 隐藏层的权重参数更新
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    3.3 输出层和隐藏层的偏置参数更新
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-bp" class="md-nav__link">
    3.4 BP算法的四个核心公式
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35" class="md-nav__link">
    3.5 计算代价函数的偏导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-bp" class="md-nav__link">
    3.6 BP算法总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          推荐系统基础
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="推荐系统基础" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          推荐系统基础
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../recommender%20systems%20foundation/" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../recommender%20systems%20foundation/1.%20introduction/" class="md-nav__link">
        一. 简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1bp" class="md-nav__link">
    1、反向传播算法和BP网络简介
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2、信息向前传播
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3、误差反向传播
  </a>
  
    <nav class="md-nav" aria-label="3、误差反向传播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    3.1 输出层的权重参数更新
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    3.2 隐藏层的权重参数更新
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    3.3 输出层和隐藏层的偏置参数更新
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-bp" class="md-nav__link">
    3.4 BP算法的四个核心公式
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35" class="md-nav__link">
    3.5 计算代价函数的偏导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-bp" class="md-nav__link">
    3.6 BP算法总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


  <h1>三. 神经网络BP算法原理和推导</h1>

<div class="admonition info">
<p>参考链接:</p>
<p><a href="https://blog.csdn.net/qq_32865355/article/details/80260212">神经网络BP反向传播算法原理和详细推导流程</a></p>
</div>
<!--
下面这个css用于控制p标签的两端对齐
-->
<style type="text/css">
p {
    text-align: justify;  /*文本两端对齐*/
}
center img{
    border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);
}
center div{
    color:orange; 
    border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;
}
</style>

<h2 id="1bp">1、反向传播算法和BP网络简介</h2>
<p>误差反向传播算法简称反向传播算法（即BP算法）。使用反向传播算法的多层感知器又称为BP神经网络。BP算法是一个迭代算法, 它的基本思想为：</p>
<ul>
<li>
<p>先计算每一层的状态和激活值，直到最后一层 (即信号是前向传播的);</p>
</li>
<li>
<p>计算每一层的误差, 误差的计算过程是从最后一层向前推进的 (这就是反向传播算法名字的由来);</p>
</li>
<li>
<p>更新参数 (目标是误差变小)。迭代前面两个步骤, 直到满足停止准则 (比如相邻两次迭代的误差的差别很小)。</p>
</li>
</ul>
<p>本文的记号说明:</p>
<ul>
<li>
<p><span class="arithmatex">\(n_l\)</span> 表示第 <span class="arithmatex">\(l\)</span> 层神经元的个数;</p>
</li>
<li>
<p><span class="arithmatex">\(f(\cdot)\)</span> 表示神经元的激活函数</p>
</li>
<li>
<p><span class="arithmatex">\(W^{(l)} \in \mathbb{R}^{(n_l\times n_{l-1})}\)</span> 表示 <span class="arithmatex">\(l-1\)</span> 层到第 <span class="arithmatex">\(l\)</span> 层的权重矩阵;</p>
</li>
<li>
<p><span class="arithmatex">\(w_{ij}^{(l)}\)</span> 是权重矩阵 <span class="arithmatex">\(W^{(l)}\)</span> 中的元素, 表示第 <span class="arithmatex">\(l-1\)</span> 层第 <span class="arithmatex">\(j\)</span> 个神经元到第 <span class="arithmatex">\(l\)</span> 层第 <span class="arithmatex">\(i\)</span> 个神经元的权重 (注意标号的顺序);</p>
</li>
<li>
<p><span class="arithmatex">\(b^{(l)}=(b_1^{(l)},b_2^{(l)},\cdots ,b_{n_l}^{(l)})^T \in \mathbb{R}^{n_l}\)</span> 表示 <span class="arithmatex">\(l-1\)</span> 层到第 <span class="arithmatex">\(l\)</span> 层的偏置;</p>
</li>
<li>
<p><span class="arithmatex">\(z^{(l)}=(z_1^{(l)},z_2^{(l)},\cdots ,z_{n_l}^{(l)})^T \in \mathbb{R}^{n_l}\)</span> 表示 <span class="arithmatex">\(l\)</span> 层神经元的状态;</p>
</li>
<li>
<p><span class="arithmatex">\(a^{(l)}=(a_1^{(l)},a_2^{(l)},\cdots ,a_{n_l}^{(l)})^T \in \mathbb{R}^{n_l}\)</span> 表示 <span class="arithmatex">\(l\)</span> 层神经元的激活值 (即输出值);</p>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">关于记号的特别注意</p>
<p>不同文献所采用的记号可能不同, 这将导致不同文献的公式结论可能不同。如 Andrew Ng 的教程中用 <span class="arithmatex">\(W^{(l)}\)</span> 表示的是第 <span class="arithmatex">\(l\)</span> 层到第 <span class="arithmatex">\(l+1\)</span> 层的权重矩阵。又如, 本文用 "下标" 来标记一个向量的不同分量, 而有一些资料却用 "上标" 来标记向量的不同分量。</p>
</div>
<p>下面以三层感知器 (即只含有一个隐藏层的多层感知器) 为例介绍反向传播算法 (BP) 算法。</p>
<p>三层感知器如图 1 所示。例子中, 输入数据 <span class="arithmatex">\(x=(x_1,x_2,x_3)^T\)</span> 是三维的 (对于第一层, 可以认为 <span class="arithmatex">\(a_i^{(1)}=x_i\)</span>), 唯一的隐藏层有三个节点, 输出数据是 2 维的。</p>
<p><center>
    <img src="../../assets/images/201805092206.png">
    <br>
    <div>Figure 1: 三层感知器实例</div>
</center></p>
<h2 id="2">2、信息向前传播</h2>
<p>显然, 图 1 所示的神经网络的第 2 层神经元的状态及激活值可以通过下面的计算得到:</p>
<div class="arithmatex">\[
\begin{aligned}
z_1^{(2)} &amp;=w_{11}^{(2)}x_1+w_{12}^{(2)}x_2+w_{13}^{(2)}x_3+b_1^{(2)} \\\\
z_2^{(2)}&amp;=w_{21}^{(2)}x_1+w_{22}^{(2)}x_2+w_{23}^{(2)}x_3+b_2^{(2)}\\\\
z_3^{(2)}&amp;=w_{31}^{(2)}x_1+w_{32}^{(2)}x_2+w_{33}^{(2)}x_3+b_2^{(3)}\\\\
a_1^{(2)}&amp;=f(z_1^{(2)})\\\\
a_2^{(2)}&amp;=f(z_2^{(2)})\\\\
a_3^{(2)}&amp;=f(z_3^{(2)})
\end{aligned}
\]</div>
<p>类似地, 第 3 层神经元地状态及激活值可以通过下面计算得到:</p>
<div class="arithmatex">\[
\begin{aligned}
z_1^{(3)}&amp;=w_{11}^{(3)}a_1^{(2)}+w_{12}^{(3)}a_2^{(2)}+w_{13}^{(3)}a_3^{(2)}+b_1^{(3)}\\\\
z_2^{(3)}&amp;=w_{21}^{(3)}a_1^{(2)}+w_{22}^{(3)}a_2^{(2)}+w_{23}^{(3)}a_3^{(2)}+b_2^{(3)}\\\\
a_1^{(3)}&amp;=f(z_1^{(3)})\\\\
a_2^{(3)}&amp;=f(z_2^{(3)})
\end{aligned}
\]</div>
<p>可总结出, 第 <span class="arithmatex">\(l(2\leq l \leq L)\)</span> 层神经元的状态及激活值为 (下面式子是向量表示形式):</p>
<div class="arithmatex">\[
\begin{aligned}
z^{(l)}&amp;=W^{(l)}a^{(l-1)}+b^{(l)} \\\\
a^{(l)}&amp;=f(z^{(l)})
\end{aligned}
\]</div>
<p>对于 <span class="arithmatex">\(L\)</span> 层感知器, 网络的最终输出为 <span class="arithmatex">\(a^{(L)}\)</span>。前馈神经网络中信息的前向传递过程如下:</p>
<div class="arithmatex">\[
x = a^{(1)} \rightarrow z^{(2)} \rightarrow \cdots a^{(L-1)} \rightarrow  z^{(L)} \rightarrow a^{(L)}=y
\]</div>
<h2 id="3">3、误差反向传播</h2>
<p>"信息前向传播"讲的是已知各个神经元的参数后, 如何得到神经网络的输出。但怎么得到各个神经元的参数呢？"误差反向传播" 算法解决的就是这个问题。</p>
<p>假设训练数据为 <span class="arithmatex">\(\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots ,(x^{(i)},y^{(i)}),\cdots ,(x^{(N)},y^{(N)})\}\)</span>, 即共有 <span class="arithmatex">\(N\)</span> 个。又假设输出数据 <span class="arithmatex">\(n_L\)</span> 维的, 即 <span class="arithmatex">\(y^{(i)}=(y_1^{(i)},\cdots ,y_{n_L}^{(i)})^T\)</span>。</p>
<p>对于某一个训练数据 <span class="arithmatex">\((x^{(i)},y^{(i)})\)</span> 来说, 其代价函数可写为 :</p>
<div class="arithmatex">\[
\begin{aligned}
E_{(i)}&amp;=\frac{1}{2}||y^{(i)}-o^{(i)}|| \\\\
&amp;=\frac{1}{2}\sum_{k=1}^{n_L}(y^{(i)}-o^{(i)})^2
\end{aligned}
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(y^{(i)}\)</span> 为期望的输出 (是训练数据给出的已知值), <span class="arithmatex">\(o^{(i)}\)</span> 为神经网络对输入 <span class="arithmatex">\(x^{(i)}\)</span> 产生的实际输出。</p>
</li>
<li>
<p>代价函数中的系数 <span class="arithmatex">\(\frac{1}{2}\)</span> 不是必要的, 它的存在仅仅是为了后续计算时更方便。</p>
</li>
<li>
<p>以图 1 所示的神经网络为例, <span class="arithmatex">\(n_L=2\)</span>, <span class="arithmatex">\(y^{(i)}=(y_1^{(i)},y_2^{(i)})^T\)</span>, 从而有 <span class="arithmatex">\(E_{(i)}=\frac{1}{2}(y_1^{(i)}-a_1^{(3)})^2+\frac{1}{2}(y_2^{(i)}-a_2^{(3)})^2\)</span>, 如果展开到隐藏层, 则有, <span class="arithmatex">\(E_{(i)}=\frac{1}{2}\big(y_1^{(i)}-f(w_{11}^{(3)}a_1^{(2)}+w_{12}^{(3)}a_2^{(2)}+w_{13}^{(3)}a_3^{(2)}+b_1^{(3)})\big)^2+\frac{1}{2}\big(y_2^{(i)}-f(w_{21}^{(3)}a_1^{(2)}+w_{22}^{(3)}a_2^{(2)}+w_{23}^{(3)}a_3^{(2)}+b_2^{(3)})\big)^2\)</span>, 还可以进一步展开到输入层 (替换掉 <span class="arithmatex">\(a_1^{(2)}\)</span>, <span class="arithmatex">\(a_2^{(2)}\)</span>, <span class="arithmatex">\(a_3^{(2)}\)</span> 即可), 最后可得: <strong>代价函数 <span class="arithmatex">\(E_{(i)}\)</span> 仅和权重矩阵 <span class="arithmatex">\(W^{(i)}\)</span> 和偏置向量 <span class="arithmatex">\(b^{(l)}\)</span> 相关, 调整权重和偏置可以减少或增大代价(误差)</strong>。</p>
</li>
</ul>
<p>显然, 所有训练数据的总体 (平均) 代价可写为:</p>
<div class="arithmatex">\[
E_{total}=\frac{1}{N}\sum_{i=1}^N{E_{(i)}}
\]</div>
<p>如果采用梯度下降法 (文献里一般叫批量梯度下降法), 可以用下面公式更新参数 <span class="arithmatex">\(w_{ij}^{(l)}\)</span>, <span class="arithmatex">\(b_i{(l)}\)</span>, <span class="arithmatex">\(2 \leq l \leq L\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
W^{(l)} &amp;= W^{(l)}- \mu \frac{\partial E_{total}}{\partial W^{(l)}} \\\\
&amp;=W^{(l)}- \frac{\mu}{N}\sum_{i=1}^N \frac{\partial E_{(i)}}{\partial W^{(l)}}
\end{aligned}
\]</div>
<div class="arithmatex">\[
\begin{aligned}
b^{(l)} &amp;= b^{(l)}- \mu \frac{\partial E_{total}}{\partial b^{(l)}} \\\\
&amp;=b^{(l)}- \frac{\mu}{N}\sum_{i=1}^N \frac{\partial E_{(i)}}{\partial b^{(l)}}
\end{aligned}
\]</div>
<p>由上面公式可知, 只需求得每一个训练数据得代价函数 <span class="arithmatex">\(E_{(i)}\)</span> 对参数的偏导数 <span class="arithmatex">\(\frac{\partial E_{(i)}}{\partial W^{(l)}}\)</span>, <span class="arithmatex">\(\frac{\partial E_(i)}{\partial b^{(l)}}\)</span> 即可得到参数的迭代更新公式。</p>
<p>为简单起见, 在下文的推到中, 我们去掉 <span class="arithmatex">\(E_{(i)}\)</span> 的下标, 直接记为 <span class="arithmatex">\(E\)</span> <strong>(假设训练数据有且只有一个数据)</strong></p>
<p>下面将介绍用"反向传播算法"求解单个训练数据误差对参数的偏导 <span class="arithmatex">\(\frac{\partial E}{\partial W^{(l)}}\)</span> 和 <span class="arithmatex">\(\frac{\partial E}{\partial b^{(l)}}\)</span> 的过程。我们求解一个简单的情况: 图 1 所示神经网络, 最后再归纳出通用公式。</p>
<h3 id="31">3.1 输出层的权重参数更新</h3>
<p>把 <span class="arithmatex">\(E\)</span> 展开到隐藏层, 有:</p>
<div class="arithmatex">\[
\begin{aligned}
E &amp;= \frac{1}{2}||y-o|| \\\\
&amp;= \frac{1}{2}||y-a^{(3)}||\\\\
&amp;=\frac{1}{2} \big((y_1-a_1^{(3)})^{2}+(y_2-a_2^{(3)})^{2}\big)\\\\
&amp;=\frac{1}{2} \Big(\big(y_1-f(z_1^{(3)})\big)^{2}+\big(y_2-f(z_2^{(3)})\big)^{2}\Big)\\\\
&amp;= \frac{1}{2} \Big(\big(y_1-f(w_{11}^{(3)}a_1^{(2)}+w_{12}^{(3)}a_2^{(2)}+w_{13}^{(3)}a_3^{(2)}+b_1^{(3)})\big)^{2}+\big(y_2-f(w_{21}^{(3)}a_1^{(2)}+w_{22}^{(3)}a_2^{(2)}+w_{23}^{(3)}a_3^{(2)}+b_2^{(3)})\big)^{2}\Big)\\\\
\end{aligned}
\]</div>
<p>由求导的链式法则, 对 "输出层神经元的权重参数"求偏导, 有:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial w_{11}^{(3)}}&amp;= \frac{1}{2}\cdot 2 (y_1-a_1^{(3)})(-\frac{\partial a_1^{(3)}}{\partial w_{11}^{(3)}})\\\\
&amp;=-(y_1-a_1^{(3)})f'(z_1^{(3)})\frac{\partial z_1^{(3)}}{\partial w_{11}^{(3)}}\\\\
&amp;=-(y_1-a_1^{(3)})f'(z_1^{(3)})a_1^{(2)}
\end{aligned}
\]</div>
<p>如果我们把 <span class="arithmatex">\(\frac{\partial E}{\partial z_i^{(l)}}\)</span> 记为 <span class="arithmatex">\(\delta_i^{(l)}\)</span>, 即做下面的定义:</p>
<div class="arithmatex">\[
\frac{\partial E}{\partial z_i^{(l)}}=\delta_i^{(l)} 
\]</div>
<p>则 <span class="arithmatex">\(\frac{\partial E}{\partial w_{11}^{(3)}}\)</span> 显然可以写为:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial w_{11}^{(3)}}&amp;= \frac{\partial E}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial w_{11}^{(3)}}\\\\
&amp;=\delta_1^{(3)}a_1^{(2)}
\end{aligned}
\]</div>
<p>其中: <span class="arithmatex">\(\delta_1^{(3)}=\frac{\partial E}{\partial z_1^{(3)}}=\frac{\partial E}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}=-(y_1-a_1^{(3)})f'(z_1^{(3)})\)</span></p>
<p>对于输出层神经元的其他权重参数, 同样可求得:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial w_{12}^{(3)}}&amp;=\delta_1^{(3)}a_2^{(2)} \\\\
\frac{\partial E}{\partial w_{13}^{(3)}}&amp;=\delta_1^{(3)}a_3^{(2)} \\\\
\frac{\partial E}{\partial w_{21}^{(3)}}&amp;=\delta_2^{(3)}a_1^{(2)} \\\\
\frac{\partial E}{\partial w_{22}^{(3)}}&amp;=\delta_2^{(3)}a_2^{(2)} \\\\
\frac{\partial E}{\partial w_{23}^{(3)}}&amp;=\delta_2^{(3)}a_3^{(2)} \\\\
\end{aligned}
\]</div>
<p>其中: <span class="arithmatex">\(\delta_2^{(3)}=-\big(y_2-a_2^{(3)}f'(z_2^{(3)})\big)\)</span></p>
<blockquote>
<p>之所以要引入记号 <span class="arithmatex">\(\delta_i^{(l)}\)</span>, 除了它能简化 <span class="arithmatex">\(\frac{\partial E}{\partial w_{ij}^{(l)}}\)</span> 和 <span class="arithmatex">\(\frac{\partial E}{\partial b_i^{(l)}}\)</span> 的表达形式外; 更重要的是我们可以通过 <span class="arithmatex">\(\delta_i^{(l+1)}\)</span> 来求解 <span class="arithmatex">\(\delta_i^{(l)}\)</span> (后文将说明), 这样可以充分利用之前计算过的结果来加快整个计算过程。</p>
</blockquote>
<p>推广到一般情况, 假设神经网络共 <span class="arithmatex">\(L\)</span> 层, 则:</p>
<div class="arithmatex">\[
\begin{aligned}
\delta_i^{(l)} =-(y_i-a_i^{(L)})f'(z_i^{(L)})\ \  &amp;(1 \leq i \leq n_L) \\\\
\frac{\partial E}{\partial w_{ij}^{(L)}}=\delta_i^{(L)}a_j^{(L-1)}\ \ \ \ \ \ \ \ \  \ \  &amp;(1 \leq i \leq n_L, 1 \leq j \leq n_{L-1})
\end{aligned}
\]</div>
<p>如果把上面两式表达为矩阵 (向量), 则为:</p>
<div class="arithmatex">\[
\delta^{(L)} =-(y-a^{(L)})⊙f'(z^{(L)})
\]</div>
<div class="arithmatex">\[
\nabla _{W^{(L)}}E=\delta^{(L)}(a^{(L-1)})^T
\]</div>
<blockquote>
<p>符号 ⊙ 表示 Element-wise Product Operator, 又被称作 Hadamard product. 规则简单, 把对应位置的元素分别相乘即可。如:</p>
</blockquote>
<div class="arithmatex">\[
\left(
\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}\\
a_{31} &amp; a_{32}\\    
\end{array}
\right) ⊙ 
\left(
\begin{array}{cc}
b_{11} &amp; b_{12}\\
b_{21} &amp; b_{22}\\
b_{31} &amp; b_{32}\\    
\end{array}
\right)=
\left(
\begin{array}{cc}
a_{11}b_{11} &amp; a_{12}b_{12}\\
a_{21}b_{21} &amp; a_{22}b_{22}\\
a_{31}b_{31} &amp; a_{32}b_{32}\\    
\end{array}
\right)
\]</div>
<p>向量式子 <span class="arithmatex">\(\delta^{(L)} =-(y-a^{(L)})⊙f'(z^{(L)})\)</span> 在前面的例子中, 表达的就是下面这两个式子</p>
<div class="arithmatex">\[
\begin{aligned}
\delta_1^{(3)}&amp;=-\big(y_1-a_1^{(3)}f'(z_1^{(3)})\big) \\\\
\delta_2^{(3)}&amp;=-\big(y_2-a_2^{(3)}f'(z_2^{(3)})\big) 
\end{aligned}
\]</div>
<h3 id="32">3.2 隐藏层的权重参数更新</h3>
<p>对 "隐藏层神经元的权重参数" 求偏导, 利用 <span class="arithmatex">\(\delta_i^{(l)}\)</span> 的定义, 有:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial w_{ij}^{(l)}}&amp;=\frac{\partial E}{\partial z_i^{(l)}}\frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} \\\\
&amp;=\delta_i^{(l)}\frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}}\\\\
&amp;=\delta_i^{(l)}a_j^{(l-1)}
\end{aligned}
\]</div>
<p>其中 <span class="arithmatex">\(\delta_i^{(l)}\)</span>, <span class="arithmatex">\(2 \leq l \leq L-1\)</span> 的推导如下:</p>
<div class="arithmatex">\[
\begin{aligned}
\delta_i^{(l)}&amp;=\frac{\partial E}{\partial z_i^{(l)}} \\\\
&amp;=\sum_{j=1}^{n_{l+1}}\frac{\partial E}{\partial z_j^{(l+1)}}\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}\\\\
&amp;=\sum_{j=1}\delta_j^{(l+1)}\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}
\end{aligned}
\]</div>
<p><strong>全文最最关键的要来了</strong></p>
<p>上面的式子中为什么有 <span class="arithmatex">\(\frac{\partial E}{\partial z_i^{(l)}}=\sum_{j=1}^{n_{l+1}}\frac{\partial E}{\partial z_j^{(l+1)}}\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}\)</span> 呢? 其实利用的仅仅是 "函数之和的求导法则" 及 "求导的链式法则"。如果把 <span class="arithmatex">\(E\)</span> 从后往前展开, 当展开到 <span class="arithmatex">\(l+1\)</span> 层时, <span class="arithmatex">\(E\)</span> 可看作是 <span class="arithmatex">\(z^{(l+1)}\)</span> 的函数; 如果再往前展开到一层到 <span class="arithmatex">\(l\)</span> 层, <span class="arithmatex">\(E\)</span> 可看作是 <span class="arithmatex">\(z^{(l)}\)</span> 的函数。<span class="arithmatex">\(E\)</span> 对 <span class="arithmatex">\(l\)</span> 层的某个 <span class="arithmatex">\(z_i^{(l)}\)</span> 求导时, 由于 <span class="arithmatex">\(l+1\)</span> 层的每个神经元都和 <span class="arithmatex">\(z_i^{(l)}\)</span> 所在神经元有连接, 所以再函数 <span class="arithmatex">\(E\)</span> 中, 自变量 <span class="arithmatex">\(z_i^{(l)}\)</span> 出现了 <span class="arithmatex">\(n_{l+1}\)</span> 次, 出现的每一次对应一个 <span class="arithmatex">\(z_j^{(l+1)}\)</span>, <span class="arithmatex">\(1 \leq j \leq n_{l+1}\)</span>, 从而由 "函数之和的求导法则" 及 "求导的链式法则" 可得</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial z_i^{(l)}} &amp;= \frac{\partial E}{\partial z_1^{(l+1)}}\frac{\partial z_1^{(l+1)}}{\partial z_i^{(l)}}+\frac{\partial E}{\partial z_2^{(l+1)}}\frac{\partial z_2^{(l+1)}}{\partial z_i^{(l)}}+\cdots +\frac{\partial E}{\partial z_{n_{l+1}}^{(l+1)}}\frac{\partial z_{n_{l+1}}^{(l+1)}}{\partial z_i^{(l)}}\\\\
&amp;=\sum_{j=1}^{n_{l+1}}\frac{\partial E}{\partial z_j^{(l+1)}}\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}
\end{aligned}
\]</div>
<p>上面的推导过程可以从图 2 中更清楚地展示出来。</p>
<p><center>
    <img src="../../assets/images/201805092218.png">
    <br>
    <div>Figure 2: 由delta(l+1)求delta(l)</div>
</center></p>
<p>看一个简单地特例, 如图 1 所示的神经网络中, 有 <span class="arithmatex">\(\frac{\partial E}{\partial z_1^{2}}=\sum_{j=1}^{2}\frac{\partial E}{\partial z_j^{(3)}}\frac{\partial z_j^{(3)}}{\partial z_1^{(2)}}\)</span></p>
<p>由于 <span class="arithmatex">\(z_j^{(l+1)}=\sum_{i=1}^{n_l}w_{ji}^{(l+1)}a_i^{(l)}+b_j^{(l+1)}=\sum_{i=1}^{n_l}w_{ji}^{(l+1)}f(z_i^{(l)})+b_j^{(l+1)}\)</span>, 所以有 <span class="arithmatex">\(\frac{\partial z_j^{(l+1)}}{\partial z_i^{(l)}}=\frac{\partial z_j^{(l+1)}}{\partial a_i^{(l)}}\frac{\partial a_i^{(l)}}{\partial z_i^{(l)}}=w_{ji}^{(l+1)}f'(z_i^{(l)})\)</span> , 代入到前面计算的 <span class="arithmatex">\(\delta_i^{(l)}\)</span> 式中, 从而有:</p>
<div class="arithmatex">\[
\begin{aligned}
\delta_i^{(l)}&amp;= \sum_{j=1}^{n_{l+1}}\delta_j^{(l+1)}w_{ji}^{(l+1)}f'(z_i^{(l)}) \\\\
&amp;=\Big(\sum_{j=1}^{n_{l+1}}\delta_j^{(l+1)}w_{ji}^{(l+1)}\Big)f'(z_i^{(l)})
\end{aligned}
\]</div>
<p>上式是 BP 算法最核心的公式。它利用 <span class="arithmatex">\(l+1\)</span> 层的 <span class="arithmatex">\(\delta^{(l+1)}\)</span> 来计算 <span class="arithmatex">\(l\)</span> 层的 <span class="arithmatex">\(\delta^{(l)}\)</span>, 这就是 "误差反向传播算法" 名字的由来。如果把它表达为矩阵 (向量) 形式, 则为</p>
<div class="arithmatex">\[
\delta^{(l)}=\big((W^{(l+1)})^T\delta^{(l+1)}\big) ⊙ f'(z^{(l)})
\]</div>
<h3 id="33">3.3 输出层和隐藏层的偏置参数更新</h3>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial b_i^{(l)}} &amp;= \frac{\partial E}{\partial z_i^{(l)}}\frac{\partial z_i^{(l)}}{\partial b_i^{(l)}} \\\\
&amp;=\delta_i^{(l)}
\end{aligned}
\]</div>
<p>对应的矩阵 (向量) 形式为:</p>
<div class="arithmatex">\[
\nabla _{b^{(l)}}E=\delta^{l}
\]</div>
<h3 id="34-bp">3.4 BP算法的四个核心公式</h3>
<p>前面已经完整地介绍了误差反向传播算法, 可总结为下面四个公式:</p>
<div class="arithmatex">\[
\delta_i^{(L)} = -(y_i-a_i^{(L)})f'(z_i^{(L)}) \tag{BP-1}
\]</div>
<div class="arithmatex">\[
\delta_i^{(l)} =\Big(\sum_{j=1}^{n_{l+1}}\delta_j^{(l+1)}w_{ji}^{(l+1)}\Big)f'(z_i^{(l)}) \tag{BP-2} 
\]</div>
<div class="arithmatex">\[
\frac{\partial E}{\partial w_{ij}^{(l)}}=\delta_i^{(l)}a_j^{(l-1)} \tag{BP-3}
\]</div>
<div class="arithmatex">\[
\frac{\partial E}{\partial b_i^{(l)}}=\delta_i^{(l)} \tag{BP-4}
\]</div>
<p>这四个公式可以写成对应的矩阵 (向量) 形式:</p>
<div class="arithmatex">\[
\begin{aligned}
\delta^{(L)} &amp;= -(y-a^{(L)})⊙f'(z^{(L)}) \\\\
\delta^{(l)} &amp;=\big((W^{(l+1)})^T\delta^{(l+1)}\big) ⊙ f'(z^{(l)})\\\\
\frac{\partial E}{\partial w^{(l)}}&amp;=\delta^{(l)}(a^{(l-1)})^T\\\\
\frac{\partial E}{\partial b^{(l)}}&amp;=\delta^{(l)}
\end{aligned}
\]</div>
<p>或者表示为:</p>
<div class="arithmatex">\[
\begin{aligned}
\delta^{(L)} &amp;= -(y-a^{(L)})⊙f'(z^{(L)}) \\\\
\delta^{(l)} &amp;=\big((W^{(l+1)})^T\delta^{(l+1)}\big) ⊙ f'(z^{(l)})\\\\
\nabla _{W^{(l)}}E&amp;=\delta^{(l)}(a^{(l-1)})^T\\\\
\nabla _{b^{(l)}}E &amp;=\delta^{(l)}
\end{aligned}
\]</div>
<h3 id="35">3.5 计算代价函数的偏导数</h3>
<p>BP算法四个核心公式就是求某个训练数据的代价函数对参数的偏导数, 它的具体应用步骤总结如下:</p>
<ul>
<li>第一步, 初始化参数 <span class="arithmatex">\(W,\ b\)</span></li>
</ul>
<p>一般地, 把 <span class="arithmatex">\(w_{ij}^{(l)}, b_i^{(l)}, 2 \leq l \leq L\)</span> 初始化为一个很小的, 接近于0的随机值。</p>
<p>注意:  不要把 <span class="arithmatex">\(w_{ij}^{(l)}, b_i^{(l)}, 2 \leq l \leq L\)</span> 全部初始化为0或者相同的其他值, 这回导致对于所有的 <span class="arithmatex">\(i\)</span>, <span class="arithmatex">\(w_{ij}^{(l)}\)</span> 都会取相同的值。</p>
<ul>
<li>第二步, 利用下面的 "前向传播" 公式计算每层的状态和激活值:</li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
z^{(l)}&amp;=W^{(l)}a^{(l-1)}+b^{(l)} \\\\
a^{(l)}&amp;=f(z^{(l)})
\end{aligned}
\]</div>
<ul>
<li>第三步, 计算 <span class="arithmatex">\(\delta^{(l)}\)</span></li>
</ul>
<p>首先利用下面公式计算输出层的 <span class="arithmatex">\(\delta^{(L)}\)</span></p>
<div class="arithmatex">\[
\delta_i^{(L)} = -(y_i-a_i^{(L)})f'(z_i^{(L)}),\ \ (1\leq i \leq n_L)
\]</div>
<p>其中, <span class="arithmatex">\(y_i\)</span> 是期望的输出 (这是训练数据给出的已知值), <span class="arithmatex">\(a_i^{(L)}\)</span> 是神经网路对训练数据产生的实际输出。然后, 利用下面公式从 <span class="arithmatex">\(L-1\)</span> 层到第 2 层依次计算隐藏层的 <span class="arithmatex">\(\delta^{(l)}\)</span>, <span class="arithmatex">\(l=(L-1, L-2, L-3,\cdots , 2)\)</span></p>
<div class="arithmatex">\[
\delta_i^{(l)} =\Big(\sum_{j=1}^{n_{l+1}}\delta_j^{(l+1)}w_{ji}^{(l+1)}\Big)f'(z_i^{(l)}),\ \ (1 \leq i \leq n_l)
\]</div>
<ul>
<li>第四步, 按下面公式求这个训练数据的代价函数对参数的偏导数:</li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial E}{\partial w_{ij}^{(l)}}&amp;=\delta_i^{(l)}a_j^{(l-1)}\\\\
\frac{\partial E}{\partial b_i^{(l)}}&amp;=\delta_i^{(l)}  \\\\
\end{aligned}
\]</div>
<p>工程实现中注意事项:</p>
<p>在前面传播的过程中, 我们已经计算出了所有的 <span class="arithmatex">\(a_i^{(l)}\)</span>, 反向传播过程中的 <span class="arithmatex">\(f'(z_i^{(l)})\)</span> 可以直接用 <span class="arithmatex">\(a_i^{(l)}\)</span> 来计算。</p>
<p>假设使用的激活函数为 <span class="arithmatex">\(f(x)=\frac{1}{1+ep^{-x}}\)</span>, 则 <span class="arithmatex">\(f'(x)=- \frac{1}{(1+e^{-x})^2}\times e^{-x} \times (-1)=\frac{e^{-x}}{(1+e^{-x})^2}\)</span>, 容易验证 它又等于 <span class="arithmatex">\(f(x)\big(1-f(x)\big)\)</span>, 因此: <span class="arithmatex">\(f'(z_i^{(l)})=a_i^{(l)}(1-a_i^{(l)})\)</span></p>
<h3 id="36-bp">3.6 BP算法总结</h3>
<p>"批量梯度下降" 算法更新参数的总结如下:</p>
<p>(1) 用BP算法四个核心公式求得每一个训练数据的代价函数对参数的偏导数</p>
<p>(2) 按下面公式更新参数:</p>
<div class="arithmatex">\[
W^{(l)} =W^{(l)}- \frac{\mu}{N}\sum_{i=1}^N \frac{\partial E_{(i)}}{\partial W^{(l)}}
\]</div>
<div class="arithmatex">\[
b^{(l)} =b^{(l)}- \frac{\mu}{N}\sum_{i=1}^N \frac{\partial E_{(i)}}{\partial b^{(l)}}
\]</div>
<p>(3) 迭代执行第 (1), (2) 步, 直到满足停止准则。(比如相邻两次迭代误差的差别很小, 或者直接限制迭代的次数)。</p>
<blockquote>
<p>说明: 每对参数进行一次更新都要便利整个训练集数据集, 当训练数据不大时这不是问题, 当训练数据集非常巨大时, 可以采用随机梯度下降法 (每次仅使用一个训练数据来更新参数)。</p>
</blockquote>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../2.%20M%20derivation/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 二. 矩阵求导" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              二. 矩阵求导
            </div>
          </div>
        </a>
      
      
        
        <a href="../../recommender%20systems%20foundation/" class="md-footer__link md-footer__link--next" aria-label="下一页: 前言和目录" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              前言和目录
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top"], "search": "../../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c44cc438.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js"></script>
      
    
  </body>
</html>