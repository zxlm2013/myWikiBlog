
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>三. 逻辑回归 - 大白的知识库</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="大白的知识库" class="md-header__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            大白的知识库
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              三. 逻辑回归
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../math%20in%20ML/" class="md-tabs__link">
        机器学习中的数学知识
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="大白的知识库" class="md-nav__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    大白的知识库
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../1.%20linear%20regression-1%20v/" class="md-nav__link">
        一. 单变量线性回归
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2.%20linear%20regression-m%20v/" class="md-nav__link">
        二. 多变量线性回归
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          三. 逻辑回归
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        三. 逻辑回归
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1、分类问题
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2、假设函数描述
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3、决策边界
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4、代价函数
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5logistic-regression" class="md-nav__link">
    5、Logistic regression
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    6、代价函数简化和梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    7、进阶优化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    8、多类别分类: 一对多
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          机器学习中的数学知识
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习中的数学知识" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          机器学习中的数学知识
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../math%20in%20ML/" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../math%20in%20ML/1.%20M%20derivation%20essence/" class="md-nav__link">
        一. 矩阵求导本质
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../math%20in%20ML/2.%20M%20derivation/" class="md-nav__link">
        二. 矩阵求导
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1、分类问题
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2、假设函数描述
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3、决策边界
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4、代价函数
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5logistic-regression" class="md-nav__link">
    5、Logistic regression
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    6、代价函数简化和梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    7、进阶优化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    8、多类别分类: 一对多
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


  <h1>三. 逻辑回归</h1>

<div class="admonition info">
<p>参考链接:
https://scruel.gitee.io/ml-andrewng-notes/week3.html</p>
</div>
<h2 id="1">1、分类问题</h2>
<div class="admonition info">
<p>参考视频:
6 - 1 - Classification (8 min).mkv</p>
</div>
<!--
下面这个css用于控制p标签的两端对齐
-->
<style type="text/css">
p {
    text-align: justify;  /*文本两端对齐*/
}
center img{
    border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);
}
center div{
    color:orange; 
    border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;
}
</style>

<p>下面我们开始讨论分类问题。在分类问题中, 你要预测的变量 <span class="arithmatex">\(y\)</span> 是离散的值, 我们将学习一种叫做逻辑回归 (Logistic Regression) 的算法, 这是目前最流行使用最广泛的一种学习算法。所以说<strong>逻辑回归是解决分类问题</strong>。</p>
<p>在分类问题中, 我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有: 判断一封电子邮件是否是垃圾邮件; 判断一次金融交易是否是欺诈; 之前我们也谈到了肿瘤分类问题的例子, 区别一个肿瘤是恶性的还是良性的。</p>
<p><img alt="3_1_1_classification" src="../../assets/images/3_1_1_classification.png" />  </p>
<p>我们现在要从只包含两类0和1的二分类开始, 后面我们将讨论多分类问题, 例如变量y可以取 0, 1, 2, 3 这几个值。我们将因变量 (dependent variable) 可能属于的两个类分别称为负向类 (negative class) 和正向类 (positive class) ,则因变量, 其中 0 表示负向类, 1 表示正向类。</p>
<p>我们如何开发一个分类算法?</p>
<p>这个例子中的训练集是对肿瘤进行恶性和良性分类得到的数据, 注意到恶性与否只有两个值, 0 (No) 以及 1 (Yes)。所以对于这个训练集, 我们可以做的是把我们已经学会的线性回归算法应用到这个数据集: 用直线对数据进行拟合。</p>
<p><img alt="3_1_2_use_LR1" src="../../assets/images/3_1_2_use_LR1.png" /> </p>
<p>如果我们想做出预测，我们可以设置分类器的阈值为0.5, 即纵坐标为0.5。</p>
<ul>
<li>如果假设函数输出一个值 &gt;= 0.5, 则可以预测 y=1</li>
<li>如果假设函数输出一个值 &lt; 0.5, 则可以预测 y=0</li>
</ul>
<p>我们观察结果发现, 纵坐标0.5对应的横坐标右边我们将预测为正, 左边我们将预测为负。</p>
<p>在这个特定的例子中, 线性回归做得很好也很正确。我们尝试改变一下这个问题, 我们将横轴延长一点。假设我们有一个训练样本位于右边远处。</p>
<p><img alt="3_1_3_use_LR2" src="../../assets/images/3_1_3_use_LR2.png" /> </p>
<p>如果我们使用原来绿色的线, 仍然能做完成很好的分类(把右侧最远的点算进去)。但是随着右侧最远处的点加入, 我们运行我们的线性回归算法我们会得到另一条数据的拟合直线(假设变为图中蓝色的线)。我们继续将阈值设为0.5, 我们会发现左右两边的预测结果有问题了。</p>
<p><strong>总结一下就是, 加了最远处的训练集后, 使得线性回归对数据的拟合直线从绿色的线变到了蓝色的线, 从而生成了一个更坏的假设。所以把线性回归模型应用到分类问题，并不是一个好主意。</strong></p>
<p>对于分类问题, <span class="arithmatex">\(y\)</span> 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 <span class="arithmatex">\(y\)</span> 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。</p>
<p>顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但<strong>逻辑回归算法实际上是</strong>一种<strong>分类算法</strong>，它适用于标签取值离散的情况，如：0, 1。</p>
<p>接下来，我们将开始学习逻辑回归算法的细节。</p>
<h2 id="2">2、假设函数描述</h2>
<div class="admonition info">
<p>参考视频:
6 - 2 - Hypothesis Representation (7 min).mkv</p>
</div>
<p>为了使 <span class="arithmatex">\(h_{\theta}(x) \in (0,1)\)</span>, 引入逻辑回归模型, 定义假设函数</p>
<div class="arithmatex">\[
h_{\theta}(x) = g(z)=g(\theta^Tx)
\]</div>
<p>对比线性回归函数 <span class="arithmatex">\(h_\theta(x)=\theta^Tx\)</span>,  <span class="arithmatex">\(g\)</span> 表示逻辑函数 (logistic function), 复合起来, 则成为逻辑回归函数。</p>
<p>逻辑函数是 S 型函数, 会将所有实数映射到(0, 1)范围。</p>
<p>sigmoid 函数 (如下图) 是逻辑函数的特殊情况, 其公式为 <span class="arithmatex">\(g(z)=\frac{1}{1+e^{-z}}\)</span></p>
<p><img alt="3_2_1_sigmoid" src="../../assets/images/3_2_1_sigmoid.png" /> </p>
<p>应用sigmoid 函数, 则逻辑回归模型: <span class="arithmatex">\(h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\)</span></p>
<p>逻辑回归模型中, <span class="arithmatex">\(h_\theta(x)\)</span> 的作用是, 根据输入 <span class="arithmatex">\(x\)</span> 以及参数 <span class="arithmatex">\(\theta\)</span> , 计算得出 "输出 <span class="arithmatex">\(y=1\)</span> " 的可能性(estimated probability), 概率学中表示为: </p>
<div class="arithmatex">\[
h_\theta(x) = P(y=1|x;\theta) = 1-P(y=0|x;\theta) \\\\
P(y=1|x;\theta)+ P(y=0|x;\theta) = 1
\]</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>上面第二个式子是针对我们现在研究的二分类问题而说的, y 的取值只能是0 或 1。所以两者相加的概率为 100% 。</p>
</div>
<p><mark><span class="arithmatex">\(P(y=1|x;\theta)\)</span> 的含义: probability that y=1, given x, parameterized by <span class="arithmatex">\(\theta\)</span> 。</mark></p>
<p>以肿瘤诊断为例, <span class="arithmatex">\(h_\theta(x)=0.7\)</span> 表示病人有 <span class="arithmatex">\(70%\)</span> 的概率得了恶性肿瘤。</p>
<h2 id="3">3、决策边界</h2>
<div class="admonition info">
<p>参考视频: 6 - 3 - Decision Boundary (15 min).mkv</p>
</div>
<p>决策边界的概念, 可帮助我们更好地理解逻辑回归模型的拟合原理。</p>
<p>在逻辑回归中, 有假设函数 <span class="arithmatex">\(h_{\theta}(x) = g(z)=g(\theta^Tx)\)</span>。</p>
<p>为了得出分类的结果, 这里和前面一样, 规定以 <span class="arithmatex">\(0.5\)</span> 为阈值:</p>
<div class="arithmatex">\[
h_\theta(x) \geq 0.5 \rightarrow y=1 \\\\
h_\theta(x) &lt; 0.5 \rightarrow y=0 \\\\
\]</div>
<p>回忆一下sigmoid凸函数的图像 :</p>
<p><img alt="3_2_1_sigmoid" src="../../assets/images/3_2_1_sigmoid.png" /> </p>
<p>观察可得当 <span class="arithmatex">\(g(z) \geq 0.5\)</span> 时, 有 <span class="arithmatex">\(z \geq 0\)</span>, 即 <span class="arithmatex">\(\theta^T x \geq 0\)</span>。</p>
<p>sigmoid 函数的公式为 <span class="arithmatex">\(g(z)=\frac{1}{1+e^{-z}}\)</span></p>
<p>同线性回归模型的不同在于：</p>
<div class="arithmatex">\[
z \rightarrow +\infty, e^{-\infty} \rightarrow 0 \Rightarrow g(z)=1 \\\\
z \rightarrow -\infty, e^{+\infty} \rightarrow +\infty \Rightarrow g(z)=0 \\\\
\]</div>
<p>直观一点 举个栗子,</p>
<p><span class="arithmatex">\(h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)\)</span> 是下图模型的假设函数:</p>
<p><center>
    <img src="../../assets/images/3_3_1_descion_boundary1.png">
    <br>
    <div>图 3.3.1 决策边界1</div>
</center></p>
<p>根据上面的讨论, 要进行分类, 那么只要 <span class="arithmatex">\(\theta_0+\theta_1x+\theta_2x \geq 0\)</span> 时, 就预测 <span class="arithmatex">\(y=1\)</span>, 即预测为正向类。</p>
<p>如果取 <span class="arithmatex">\(\theta = [-3 \ \ 1\ \ 1]^T\)</span> , 则有 <span class="arithmatex">\(z=-3+x_1+x_2\)</span>, </p>
<p>当 <span class="arithmatex">\(z \geq 0\)</span>, 即 <span class="arithmatex">\(x_1+x_2 \geq 3\)</span>时, 易绘制出图中的品红色直线, 即<strong>决策边界</strong>, 为正向类 (以红叉标注的数据) 给出 <span class="arithmatex">\(y=1\)</span> 的分类预测结果。</p>
<p>上面讨论了逻辑回归模型中线性拟合的例子, 下面则是一个多项式拟合的例子, 和线性回归中的情况也是类似的。</p>
<p>为了拟合下图数据, 建模多项式假设函数: </p>
<div class="arithmatex">\[
h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2)
\]</div>
<p>这里取 <span class="arithmatex">\(\theta = [-1\ \ 0\ \ 0\ \ 1\ \ 1]^T\)</span>, 决策边界对应了一个在原点处的单位元 (<span class="arithmatex">\(x_1^2+x_2^2=1\)</span>), 如此便可给出分类结果, 如图中品红色曲线:</p>
<p><center>
    <img src="../../assets/images/3_3_2_descion_boundary2.png">
    <br>
    <div>图 3.3.2 决策边界2</div>
</center></p>
<p>当然, 通过一些更为复杂的多项式, 还能拟合那些图像显得非常怪异的数据, 使得决策边界形状似碗装、爱心状等等。</p>
<p>简单来说, 决策的边界就是<strong>分类的分界线</strong>, 分类现在实际就由 <span class="arithmatex">\(z\)</span> (中的 <span class="arithmatex">\(\theta\)</span> ) 决定啦。</p>
<h2 id="4">4、代价函数</h2>
<div class="admonition info">
<p>参考视频: 6 - 4 - Cost Function (11 min).mkv</p>
</div>
<p>那我们怎么知道决策边界时啥样? <span class="arithmatex">\(\theta\)</span> 多少时能很好的拟合数据? 当然, 见招拆招, 总要来个 <span class="arithmatex">\(J(\theta)\)</span>。</p>
<p>如果直接套用线性回归的代价函数: </p>
<div class="arithmatex">\[
J(\theta)=\frac{1}{2m} \sum_{i=1}^m{\big(h_\theta(x^{(i)})-y^{(i)}\big)^2}
\]</div>
<p>其中, <span class="arithmatex">\(h_\theta(x)=g(\theta^Tx)\)</span>, 可绘制关于 <span class="arithmatex">\(J(\theta)\)</span> 的图像, 如下图</p>
<p><center>
    <img src="../../assets/images/3_4_1_J_use_LRJ.png">
    <br>
    <div>图 3.4.1 逻辑回归代价函数直接用线性回归中的代价函数</div>
</center></p>
<p>回忆线性回归中的平方损失函数, 其实是一个二次凸函数 (碗状) , 二次凸函数的重要性质是只有一个局部最小点即全局最小点。上图中有许多局部最小点, 这样使得梯度下降算法无法确定哪个收敛点是全局最优。</p>
<p><center>
    <img src="../../assets/images/3_4_2_J.png">
    <br>
    <div>图 3.4.2 逻辑回归中理想的代价函数</div>
</center></p>
<p>如果此处的代价函数也是一个凸函数, 是否也有同样的性质, 从而最优化? 这类讨论凸函数最优值得问题, 被称为<strong>凸优化问题 (Convex optimization)</strong>。</p>
<p>当然, 损失函数不止平方损失函数一种。</p>
<p>对于逻辑回归, 更换平方损失函数为对数损失函数, 可由统计学中的<strong>最大似然估计方法推导出代价函数</strong> <span class="arithmatex">\(J(\theta)\)</span> <strong>(下一节英文版中给出具体的推导过程)</strong>:</p>
<div class="arithmatex">\[
J(\theta)=\frac{1}{m} \sum_{i=1}^m{Cost\big( h_\theta(x^{(i)}, y^{(i)}\big)}\\\\
Cost\big( h_\theta(x^{(i)}, y^{(i)}\big)=-\log \big(h_\theta(x)\big) \ \ \ \ \   if\ y=1 \\\\
Cost\big( h_\theta(x^{(i)}, y^{(i)}\big)=-\log \big(1-h_\theta(x)\big) \ \ \ \ if\ y=0 
\]</div>
<p>则关于 <span class="arithmatex">\(J(\theta)\)</span> 的图像如下:</p>
<p><center>
    <img src="../../assets/images/3_4_3_J_y_10.png">
    <br>
    <div>图 3.4.3 对数代价函数y=1&amp;y=0两种情况</div>
</center></p>
<p>如左上图, 当训练集的结果为 <span class="arithmatex">\(y=1\)</span> (正样本) 时, 随着假设函数趋向于1, 代价函数的值会趋向于0, 即意味着拟合程度很好。如果假设函数此时趋于0, 则会给出一个<strong>很高的代价</strong>, 拟合程度<strong>差</strong>, 算法会根据其迅速纠正 <span class="arithmatex">\(\theta\)</span> 值, 右图 <span class="arithmatex">\(y=0\)</span> 同理。</p>
<p>区别于平方损失函数, 对数损失函数也是一个凸函数, 他没有局部最优值, 只有全局最优值。</p>
<h2 id="5logistic-regression">5、Logistic regression</h2>
<p>里面包含: 代价函数和梯度的推导。(u1s1, 英文版真的写得很好)</p>
<p>Let's now talk about the classification problem. This is just like the regression problem, except that the values <span class="arithmatex">\(y\)</span> we now want to predict take on only a small number of discrete values. For now, we will focus on the <strong>binary classification</strong> problem in which <span class="arithmatex">\(y\)</span> can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then <span class="arithmatex">\(x^{(i)}\)</span> may be some features of a piece of emial, and <span class="arithmatex">\(y\)</span> may be 1 if it is a piece of spam mail, and 0 otherwise. O is also called the <strong>negative class</strong>, and 1 the <strong>positive class</strong>, and they are sometimes also denoted by the symbols "-" and "+". Given <span class="arithmatex">\(x^{(i)}\)</span>, the corresponding <span class="arithmatex">\(y^{(i)}\)</span> is also called the <strong>label</strong> for the training example.</p>
<p>We could approach the classification problem ignoring the fact that <span class="arithmatex">\(y\)</span> is discrete-valued, and use our linear regression algorithm to try to predict <span class="arithmatex">\(y\)</span> given <span class="arithmatex">\(x\)</span>. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn't make sense for <span class="arithmatex">\(h_\theta(x)\)</span> to take values larger than 1 or smaller than 0 when we know that <span class="arithmatex">\(y \in \{0, 1\}\)</span>. </p>
<p>&ensp; To fix this, let's change the form for our hypotheses <span class="arithmatex">\(h_\theta(x)\)</span>. We will choose</p>
<div class="arithmatex">\[
h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
\]</div>
<p>where</p>
<div class="arithmatex">\[
g(z)= \frac{1}{1+e^{-z}}
\]</div>
<p>is called the <strong>logistic function</strong> or the <strong>sigmoid function</strong>. Here is a plot showing <span class="arithmatex">\(g(z)\)</span> :</p>
<p><center>
    <img src="../../assets/images/3_5_1_sigmoid_function.png">
    <br>
    <div>图 3.5.1 sigmoid function</div>
</center></p>
<p>Notice that <span class="arithmatex">\(g(z)\)</span> tends towards 1 as <span class="arithmatex">\(z \rightarrow + \infty\)</span>, and <span class="arithmatex">\(g(z)\)</span> tends towards 0 as <span class="arithmatex">\(z \rightarrow - \infty\)</span>. Moreover, <span class="arithmatex">\(g(z)\)</span>, and hence also <span class="arithmatex">\(h(x)\)</span>, is always bounded between 0 and 1. As before, we are keeping the convention of letting <span class="arithmatex">\(x_0=1\)</span>, so that <span class="arithmatex">\(\theta^Tx=\theta_0+\sum_{j=1}^n \theta_jx_j\)</span>.</p>
<p>&ensp; For now, let's take the choice of <span class="arithmatex">\(g\)</span> as given. Other function that smoothly increase from 0 to 1 can also be used, but for a couple of reasons that we'll see later, the choice of the logistic function is a fairly natural one. Before moving on, here's a useful property of the derivative of the sigmoid function, which we write a <span class="arithmatex">\(g'\)</span>:</p>
<div class="arithmatex">\[
\begin{aligned}
g'(z) &amp;= \frac{d}{dz}\frac{1}{1+e^{-z}} \\\\
&amp;= \frac{1}{(1+e^{-z})^2}(e^{-z})\\\\
&amp;= \frac{1}{1+e^{-z}} \cdot \Big(1-\frac{1}{1+e^{-z}}\Big)\\\\
&amp;= g(z)\big(1-g(z)\big)
\end{aligned}
\]</div>
<p>&ensp; So, given the logistic regression model, how do we fit <span class="arithmatex">\(\theta\)</span> for it? Following how we saw least squares regression could be derived as the maximum likelihood estimator under a set of assumptions, let's endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likehood.</p>
<p>&ensp; Let us assume that</p>
<div class="arithmatex">\[
P(y=1|x;\theta) = h_\theta(x) \\\\
P(y=0|x;\theta) = 1-h_\theta(x)
\]</div>
<p>Notice that this can written more compactly as</p>
<div class="arithmatex">\[
p(y|x;\theta)=\big(h_\theta(x)\big)^y \big(1-h_\theta(x)\big)^{1-y}
\]</div>
<p>Assuming that the <span class="arithmatex">\(m\)</span> training examples were generated independently, we can then write down the likelihood of the parameters as </p>
<div class="arithmatex">\[
\begin{aligned}
L(\theta) &amp;= p(\vec y|X;\theta)\\\\
 &amp;= \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\theta)\\\\
 &amp;= \prod_{i=1}^{m} \big(h_\theta(x^{(i)}\big)^{y^{(i)}}\big(1-h_\theta(x^{(i)}\big)^{1-y^{(i)}}
\end{aligned}
\]</div>
<p>As before, it will be easier to maximize the log likelihood:</p>
<div class="arithmatex">\[
\begin{aligned}
l(\theta) &amp;= \log L(\theta) \\\\
&amp;= \sum_{i=1}^m{y^{(i)}\log h \big(x^{(i)}\big)}+(1-y^{(i)})\log \big(1-h(x^{(i)}\big)
\end{aligned}
\]</div>
<p>其实到了这里, 因为最大似然估计是求使 <span class="arithmatex">\(l(\theta)\)</span> 取最大值时的 <span class="arithmatex">\(\theta\)</span>, 其实原文到了这里是是用了梯度上升法求解, 求得的 <span class="arithmatex">\(\theta\)</span> 就是我们要求的最佳参数。</p>
<p>但是为了跟前一节的中文课程统一, 我们还是用梯度下降来做, 这里我们做一个转换。因为 <span class="arithmatex">\(l(\theta)\)</span> 要求最大等价于 <span class="arithmatex">\(-\frac{1}{m}l(\theta)\)</span> 取最小。所以, 我们可以令：</p>
<div class="arithmatex">\[
\begin{aligned}
J(\theta) &amp;= -\frac{1}{m}l(\theta) \\\\
&amp;= -\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}
\end{aligned}
\]</div>
<p>因为求梯度要求 <span class="arithmatex">\(\frac{\partial }{\partial \theta_j}J(\theta)\)</span>, 所以这里顺便推导一下。老套路, 为了简化, 先假设训练集只有一个。</p>
<p>Let's start by working with just one training example <span class="arithmatex">\((x, y)\)</span>。</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial }{\partial \theta_j}l(\theta) &amp;= \Big(y \frac{1}{g(\theta^Tx)}-(1-y) \frac{1}{1-g(\theta^Tx)}\Big) \frac{\partial }{\partial \theta_j}g(\theta^Tx)\\\\
&amp;=\Big(y \frac{1}{g(\theta^Tx)}-(1-y) \frac{1}{1-g(\theta^Tx)}\Big)g(\theta^Tx)\big(1-g(\theta^Tx)\big) \frac{\partial }{\partial \theta_j}\theta^Tx\\\\
&amp;=\Big(y\big(1-g(\theta^Tx)\big)-\big(1-y\big)g(\theta^Tx)\Big)x_j\\\\
&amp;=\big(y-h_\theta(x)\big)x_j
\end{aligned}
\]</div>
<p>所以</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial }{\partial \theta_j}J(\theta)&amp;=-\frac{1}{m}l(\theta) \\\\
&amp;=\big(h_\theta(x)-y\big)x_j
\end{aligned}
\]</div>
<p>其中, <span class="arithmatex">\(m=1\)</span>。</p>
<p>这个是不是很熟悉, 这个梯度其实跟线性回归模型中的梯度完全一样。</p>
<h2 id="6">6、代价函数简化和梯度下降</h2>
<div class="admonition info">
<p>参考视频: 6 - 5 - Simplified Cost Function and Gradient Descent (10 min).mkv</p>
</div>
<h2 id="7">7、进阶优化</h2>
<div class="admonition info">
<p>参考视频: 6 - 6 - Advanced Optimization (14 min).mkv</p>
</div>
<h2 id="8">8、多类别分类: 一对多</h2>
<div class="admonition info">
<p>参考视频: 6 - 7 - Multiclass Classification_ One-vs-all (6 min).mkv</p>
</div>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../2.%20linear%20regression-m%20v/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 二. 多变量线性回归" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              二. 多变量线性回归
            </div>
          </div>
        </a>
      
      
        
        <a href="../../math%20in%20ML/" class="md-footer__link md-footer__link--next" aria-label="下一页: 前言和目录" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              前言和目录
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top"], "search": "../../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c44cc438.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>