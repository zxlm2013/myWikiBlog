
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>二. 多变量线性回归 - 大白的知识库</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="大白的知识库" class="md-header__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            大白的知识库
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              二. 多变量线性回归
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../math%20in%20ML/" class="md-tabs__link">
        机器学习中的数学知识
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="大白的知识库" class="md-nav__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    大白的知识库
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../1.%20linear%20regression-1%20v/" class="md-nav__link">
        一. 单变量线性回归
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          二. 多变量线性回归
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        二. 多变量线性回归
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1. 多维特征
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2. 多变量梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-" class="md-nav__link">
    3. 梯度下降法 - 特征缩放
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-" class="md-nav__link">
    4. 梯度下降法 - 学习率
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5. 特征和多项式回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    6. 正规方程
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-the-normal-equations" class="md-nav__link">
    7. The normal equations
  </a>
  
    <nav class="md-nav" aria-label="7. The normal equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-matrix-derivatives" class="md-nav__link">
    7.1 Matrix derivatives
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-least-squares-revisited" class="md-nav__link">
    7.2 Least squares revisited
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    8. 正规方程及不可逆性
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    习题 &amp;&amp; 参考答案
  </a>
  
    <nav class="md-nav" aria-label="习题 &amp;&amp; 参考答案">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    第一题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    第二题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    第三题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    第 四 题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    第 五 题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    参考答案
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          机器学习中的数学知识
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习中的数学知识" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          机器学习中的数学知识
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../math%20in%20ML/" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../math%20in%20ML/1.%20M%20derivation%20essence/" class="md-nav__link">
        一. 矩阵求导本质
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../math%20in%20ML/2.%20M%20derivation/" class="md-nav__link">
        二. 矩阵求导
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1. 多维特征
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2. 多变量梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-" class="md-nav__link">
    3. 梯度下降法 - 特征缩放
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-" class="md-nav__link">
    4. 梯度下降法 - 学习率
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5. 特征和多项式回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    6. 正规方程
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-the-normal-equations" class="md-nav__link">
    7. The normal equations
  </a>
  
    <nav class="md-nav" aria-label="7. The normal equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-matrix-derivatives" class="md-nav__link">
    7.1 Matrix derivatives
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-least-squares-revisited" class="md-nav__link">
    7.2 Least squares revisited
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    8. 正规方程及不可逆性
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    习题 &amp;&amp; 参考答案
  </a>
  
    <nav class="md-nav" aria-label="习题 &amp;&amp; 参考答案">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    第一题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    第二题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    第三题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    第 四 题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    第 五 题
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    参考答案
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


  <h1>二. 多变量线性回归</h1>

<h2 id="1">1. 多维特征</h2>
<div class="admonition info">
<p>参考视频:
4 - 1 - Multiple Features (8 min).mkv</p>
</div>
<p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为<span class="arithmatex">\(\left( {x_{1}},{x_{2}},...,{x_{n}} \right)\)</span>。</p>
<table>
<thead>
<tr>
<th align="center">Size (feet2)</th>
<th align="center">Number   of bedrooms</th>
<th align="center">Number   of floors</th>
<th align="center">Age of home (years)</th>
<th align="center">Price   ($1000)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">2104</td>
<td align="center">5</td>
<td align="center">1</td>
<td align="center">45</td>
<td align="center">460</td>
</tr>
<tr>
<td align="center">1416</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">40</td>
<td align="center">232</td>
</tr>
<tr>
<td align="center">1534</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">30</td>
<td align="center">315</td>
</tr>
<tr>
<td align="center">852</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">36</td>
<td align="center">178</td>
</tr>
<tr>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
</tbody>
</table>
<p>增添更多特征后，我们引入一系列新的注释：</p>
<p><span class="arithmatex">\(n\)</span> 代表特征的数量</p>
<p><span class="arithmatex">\({x^{\left( i \right)}}\)</span> 代表第 <span class="arithmatex">\(i\)</span> 个训练实例，是特征矩阵中的第<span class="arithmatex">\(i\)</span>行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>比方说，上图的</p>
<p><span class="arithmatex">\({x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}\)</span>，</p>
<p><span class="arithmatex">\({x}_{j}^{\left( i \right)}\)</span>代表特征矩阵中第 <span class="arithmatex">\(i\)</span> 行的第 <span class="arithmatex">\(j\)</span> 个特征，也就是第 <span class="arithmatex">\(i\)</span> 个训练实例的第 <span class="arithmatex">\(j\)</span> 个特征。</p>
<p>如上图的<span class="arithmatex">\(x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2\)</span>，</p>
<p>支持多变量的假设 <span class="arithmatex">\(h\)</span> 表示为：<span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span>，</p>
<p>这个公式中有<span class="arithmatex">\(n+1\)</span>个参数和<span class="arithmatex">\(n\)</span>个变量，为了使得公式能够简化一些，引入<span class="arithmatex">\(x_{0}=1\)</span>，则公式转化为：<span class="arithmatex">\(h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span></p>
<p><img alt="2_1_1_theta_T_X" src="../../assets/images/2_1_1_theta_T_X.png" /></p>
<p>此时模型中的参数是一个<span class="arithmatex">\(n+1\)</span>维的向量，任何一个训练实例也都是<span class="arithmatex">\(n+1\)</span>维的向量，特征矩阵<span class="arithmatex">\(X\)</span>的维度是 <span class="arithmatex">\(m*(n+1)\)</span>。 因此公式可以简化为：<span class="arithmatex">\(h_{\theta} \left( x \right)={\theta^{T}}X\)</span>，其中上标  <span class="arithmatex">\(T\)</span> 代表矩阵转置。</p>
<p>注意：
从上图可知, <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(X\)</span> 我们是用<strong>列向量</strong>来标记。计算 <span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span> 的时候, 可以将 <span class="arithmatex">\(h\)</span> 表示为 <span class="arithmatex">\(\theta^TX.\)</span></p>
<h2 id="2">2. 多变量梯度下降</h2>
<div class="admonition info">
<p>参考视频:
4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv</p>
</div>
<p>快速回顾我们的记号，并用向量简化。</p>
<p>Hypothesis: <span class="arithmatex">\(h_\theta(x)=\theta_0+\theta_1x+\theta_2x+...+\theta_nx\)</span></p>
<p>简记为： <span class="arithmatex">\(h_\theta(x)=\theta^TX\)</span></p>
<p>Parameters: <span class="arithmatex">\(\theta_0,\theta_1,\theta_2,...\theta_n\)</span></p>
<p>简记为：<span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(n+1\ dimension\ vector\)</span></p>
<p>Cost Function: </p>
<div class="arithmatex">\[
J(\theta_0,\theta_1,\ \theta_2,...\theta_n)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\]</div>
<p>简记为：</p>
<div class="arithmatex">\[
J(\theta)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\]</div>
<p>Gradient descent:
<strong>Repeat {</strong></p>
<p>​                <span class="arithmatex">\({\theta_{j}}:={\theta_{j}}-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,...\theta_n)\)</span></p>
<p>​               <strong>}</strong> (simultaneously update for every j=0, 1, 2, ..., n)
简记为：把上面的<span class="arithmatex">\(J(\theta_0,...\theta_n)\)</span>换成<span class="arithmatex">\(J(\theta)\)</span></p>
<p><img alt="2_2_1_gradient_descent_v2" src="../../assets/images/2_2_1_gradient_descent_v2.png" /></p>
<p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：<span class="arithmatex">\(J\left( {\theta_{0}},{\theta_{1}}...{\theta_{n}} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}\)</span> ，</p>
<p>其中：<span class="arithmatex">\(h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span> ，</p>
<p>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。
多变量线性回归的批量梯度下降算法为：</p>
<p>当<span class="arithmatex">\(n&gt;=1\)</span>时，
<span class="arithmatex">\({{\theta }_{0}}:={{\theta }_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}\)</span></p>
<p><span class="arithmatex">\({{\theta }_{1}}:={{\theta }_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}\)</span></p>
<p><span class="arithmatex">\({{\theta }_{2}}:={{\theta }_{2}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}\)</span></p>
<p>...</p>
<p><span class="arithmatex">\({{\theta }_{n}}:={{\theta }_{n}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{n}^{(i)}\)</span></p>
<p>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。</p>
<p>代码示例：</p>
<p>计算代价函数
<span class="arithmatex">\(J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}\)</span>
其中：<span class="arithmatex">\({h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span></p>
<p><strong>Python</strong> 代码：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">inner</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(((</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">inner</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</code></pre></div>
<hr>

<p>下面，我们开始推导<span class="arithmatex">\(\frac{\partial}{\partial\theta}J(\theta)\)</span>。这里为了普遍性，还原了 <span class="arithmatex">\(\theta\)</span> 的普遍情况，也即有n个 <span class="arithmatex">\(\theta\)</span> 的情况。</p>
<p>Hypothesis:         <span class="arithmatex">\(h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\)</span></p>
<p>Parameters:         <span class="arithmatex">\(\theta_0,\theta_1,...,\theta_n\)</span></p>
<p>Cost function: </p>
<div class="arithmatex">\[
J(\theta_0,\theta_1,...,\theta_n)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\]</div>
<p>Gradient descent:
Repeat{
$$
\theta_j :=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n)
$$
​        }（simultaneously update for every j=0, 1, 2, ..., n)）</p>
<p>推导： <span class="arithmatex">\(\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n)=?\)</span></p>
<p>Let's first work it for the case if we have only one training example <span class="arithmatex">\((x,y)\)</span>, so that we can neglect the sum in the definition <span class="arithmatex">\(J\)</span>. We have:
$$
\begin{split}
\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n) &amp; =  \frac {1} {2}\frac{\partial}{\partial \theta_j}\Big(h_\theta(x)-y\Big)^2 \\
 &amp; = 2\cdot\frac{1}{2}\cdot(h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(h_\theta(x)-y) \\
 &amp; = (h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n) \\
 &amp; = (h_\theta(x)-y)\cdot x_j
\end{split}
$$</p>
<h2 id="3-">3. 梯度下降法 - 特征缩放</h2>
<div class="admonition info">
<p>参考视频:
4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv</p>
</div>
<p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。
解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：</p>
<p><img alt="2_3_1_feature_scaling" src="../../assets/images/2_3_1_feature_scaling.png" /></p>
<p>视频里吴恩达老师的方法是令：<span class="arithmatex">\({{x}_{n}}=\frac{{{x}_{n}}-mean}{{max(x)-min(x)}}\)</span>, 其中 <span class="arithmatex">\(mean\)</span>是平均值，<span class="arithmatex">\(max(x)-min(x)\)</span>分别是最大值和最小值。</p>
<p>更通用的是令：<span class="arithmatex">\({{x}_{n}}=\frac{{{x}_{n}}-{mean}}{\sigma}\)</span>，其中 <span class="arithmatex">\(mean\)</span>是平均值，<span class="arithmatex">\(\sigma\)</span>是标准差。</p>
<p>python里的api：</p>
<ul>
<li>
<p><strong>sklearn.preprocessing.StandardScaler()</strong></p>
<ul>
<li>处理之后每列来说<strong>所有数据都聚集在均值0附近标准差差为1</strong></li>
<li><strong>StandardScaler.fit_transform(X)</strong><ul>
<li>X:numpy array格式的数据[n_samples,n_features]</li>
</ul>
</li>
<li>返回值：转换后的形状相同的array</li>
</ul>
</li>
</ul>
<p>部分参考代码：
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="k">def</span> <span class="nf">linear_model1</span><span class="p">():</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span class="c1"># 1.获取数据</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    <span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="c1"># 2. 数据集划分</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>    <span class="c1"># 3. 特征工程-标准化</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>    <span class="n">transfer</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>    <span class="n">x_train</span> <span class="o">=</span> <span class="n">transfer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    <span class="n">x_test</span> <span class="o">=</span> <span class="n">transfer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="4-">4. 梯度下降法 - 学习率</h2>
<div class="admonition info">
<p>参考视频:
4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv</p>
</div>
<p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。</p>
<p><img alt="2_4_1_J_iterations" src="../../assets/images/2_4_1_J_iterations.png" /></p>
<p>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看左上方这样的图表更好。</p>
<p>不正确的学习率，会产生左侧上下两个图像。
<img alt="2_4_2_no_proper_alpha" src="../../assets/images/2_4_2_no_proper_alpha.png" /></p>
<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率 <span class="arithmatex">\(\alpha\)</span> 过小，则达到收敛所需的迭代次数会非常高；如果学习率 <span class="arithmatex">\(\alpha\)</span> 过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：</p>
<p><span class="arithmatex">\(\alpha= 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\)</span></p>
<h2 id="5">5. 特征和多项式回归</h2>
<div class="admonition info">
<p>参考视频:
4 - 5 - Features and Polynomial Regression (8 min).mkv</p>
</div>
<p>如房价预测问题，</p>
<p><img alt="2_5_1_house" src="../../assets/images/2_5_1_house.png" /></p>
<p><span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}\)</span> </p>
<p>当我们真正应用线性回归模型的时候，我们可以创造自己的特征即：
<span class="arithmatex">\({x_{1}}=frontage\)</span>（临街宽度），<span class="arithmatex">\({x_{2}}=depth\)</span>（纵向深度），<span class="arithmatex">\(x=frontage*depth=area\)</span>（面积），
则：<span class="arithmatex">\({h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x\)</span>。</p>
<p><img alt="2_5_2_polynomial_regression" src="../../assets/images/2_5_2_polynomial_regression.png" /></p>
<p>线性回归并不适用于所有数据，有时我们需要其他模型来适应我们的数据，比如一个二次方模型：<span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}\)</span>
 或者三次方模型： <span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}\)</span> </p>
<p>从上面图右侧，可以看出如果我们采用多项式回归模型，在运行梯度下降算法前特征缩放的重要性了。</p>
<p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：</p>
<p><span class="arithmatex">\({{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}\)</span>，从而将模型转化为线性回归模型。</p>
<p>根据函数图形特性，我们还可以使：</p>
<p><span class="arithmatex">\({{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta}_{2}}{{(size)}^{2}}\)</span></p>
<p>或者:</p>
<p><span class="arithmatex">\({{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta }_{2}}\sqrt{size}\)</span></p>
<p><img alt="2_5_3_choice_of_X" src="../../assets/images/2_5_3_choice_of_X.png" /></p>
<h2 id="6">6. 正规方程</h2>
<div class="admonition info">
<p>参考视频:
4 - 6 - Normal Equation (16 min).mkv</p>
</div>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img alt="2_6_1" src="../../assets/images/2_6_1.png" /></p>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：<span class="arithmatex">\(\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0\)</span> 。
 假设我们的训练集特征矩阵为 <span class="arithmatex">\(X\)</span>（包含了 <span class="arithmatex">\({{x}_{0}}=1\)</span>）并且我们的训练集结果为向量 <span class="arithmatex">\(y\)</span>，则利用正规方程解出向量 <span class="arithmatex">\(\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y\)</span> 。</p>
<p>以下面表格数据为例 <span class="arithmatex">\(m=4\)</span>：</p>
<table>
<thead>
<tr>
<th><span class="arithmatex">\(x_0\)</span></th>
<th>Size (feet2)</th>
<th>Number   of bedrooms</th>
<th>Number   of floors</th>
<th>Age of home (years)</th>
<th>Price   ($1000)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2104</td>
<td>5</td>
<td>1</td>
<td>45</td>
<td>460</td>
</tr>
<tr>
<td>1</td>
<td>1416</td>
<td>3</td>
<td>2</td>
<td>40</td>
<td>232</td>
</tr>
<tr>
<td>1</td>
<td>1534</td>
<td>3</td>
<td>2</td>
<td>30</td>
<td>315</td>
</tr>
<tr>
<td>1</td>
<td>852</td>
<td>2</td>
<td>1</td>
<td>36</td>
<td>178</td>
</tr>
</tbody>
</table>
<p><span class="arithmatex">\(X\ \text{=}\begin{bmatrix}1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45\\1 &amp; 1416 &amp; 3 &amp; 2 &amp; 40\\ 1 &amp;1534 &amp; 3 &amp; 2 &amp; 30\\ 1 &amp; 852 &amp; 2 &amp; 1 &amp; 36\end{bmatrix}\)</span>，<span class="arithmatex">\(y\ \text{=}\begin{bmatrix} 460\\232\\315\\178\end{bmatrix}\)</span></p>
<p><span class="arithmatex">\(X维度：(m,n+1),\ y的维度：(m,1)\)</span></p>
<p>这时候求解 <span class="arithmatex">\(\theta\)</span> 只需一步:  <span class="arithmatex">\(\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y\)</span>。</p>
<p>将上面的例子推广到一般情况</p>
<p><img alt="2_6_2" src="../../assets/images/2_6_2.png" /></p>
<p>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。
千万要注意这里的设计矩阵X它的构成，设计完成后，假设函数可以向量化为 <span class="arithmatex">\(h_\theta(x)=X\theta\)</span></p>
<p>梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率<span class="arithmatex">\(\alpha\)</span></td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量<span class="arithmatex">\(n\)</span>大时也能较好适用</td>
<td>需要计算<span class="arithmatex">\({{\left( {{X}^{T}}X \right)}^{-1}}\)</span> 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为<span class="arithmatex">\(O\left( {{n}^{3}} \right)\)</span>，通常来说当<span class="arithmatex">\(n\)</span>小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody>
</table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，<strong>只要特征变量数量小于一万，我通常使用正规方程法，而不使用梯度下降法。</strong></p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<p>正规方程的<strong>python</strong>实现：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a> <span class="k">def</span> <span class="nf">normalEqn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>   <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">)</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span> <span class="c1">#X.T@X等价于X.T.dot(X)</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>   <span class="k">return</span> <span class="n">theta</span>
</code></pre></div>
<p><br>
<br>
<br></p>
<div class="admonition info">
<p><strong>以下是正规方程英文版(有详细推导-自己加上的)，其他内容来源于2006年麻省理工cs 2009机器学习的note。</strong></p>
</div>
<h2 id="7-the-normal-equations">7. The normal equations</h2>
<!--
下面这个css用于控制p标签的两端对齐
-->
<style type="text/css">
p {
    text-align: justify;  /*文本两端对齐*/
}
</style>

<p>Gradient descent gives one way of minimizing <span class="arithmatex">\(J\)</span>. Lets discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize <span class="arithmatex">\(J\)</span> by explicitly taking its derivatives with respect to the <span class="arithmatex">\(θ_j's\)</span>, and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, lets introduce some notation for doing calculus with matrices. </p>
<h3 id="71-matrix-derivatives">7.1 Matrix derivatives</h3>
<p>For a function <span class="arithmatex">\(f\)</span> : <span class="arithmatex">\(\mathbb{R^{m\times n}} \rightarrow \mathbb{R}\)</span> mapping from <span class="arithmatex">\(\text{m-by-n}\)</span> matrices to the real numbers, we define the derivative of f with respect to A to be:</p>
<div class="arithmatex">\[
\nabla _Af(A)=\begin{bmatrix} \frac{\partial f}{\partial A_{11}} &amp; \cdots  &amp; \frac{\partial f}{\partial A_{1n}} \\\\ 
\vdots  &amp; \ddots &amp; \vdots \\\\
\frac{\partial f}{\partial A_{m1}} &amp; \cdots  &amp; \frac{\partial f}{\partial A_{mn}}\end{bmatrix}
\]</div>
<p>Thus, the gradient <span class="arithmatex">\(\nabla _Af(A)\)</span> is itself an <span class="arithmatex">\(\text{m-by-n}\)</span>  matrix, whose <span class="arithmatex">\((i,j)\)</span>-element is <span class="arithmatex">\(\frac{\partial f}{\partial A_{ij}}\)</span>. For example, suppose <span class="arithmatex">\(A =\bigl( \begin{smallmatrix} A_{11}  &amp; A_{12} \\ A_{21} &amp; A_{22} \end{smallmatrix} \bigr)\)</span> is a 2-by-2 matrix, and the function <span class="arithmatex">\(f\)</span>: <span class="arithmatex">\(\mathbb{R^{2\times 2}}\rightarrow \mathbb{R}\)</span> is given by</p>
<div class="arithmatex">\[
f(A)=\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22}
\]</div>
<p>Here, <span class="arithmatex">\(A_{ij}\)</span> denotes the <span class="arithmatex">\((i,j)\)</span> entry of the matrix <span class="arithmatex">\(A\)</span>. We then have</p>
<div class="arithmatex">\[
\nabla _Af(A)=\begin{bmatrix} 
\frac{3}{2} &amp; 10A_{12}\\\\
A_{22} &amp; A_{21}
\end{bmatrix}
\]</div>
<p>&ensp; We also introduce the <strong>trace</strong> opertator, written by "tr.". For an <span class="arithmatex">\(\text{n-by-n}\)</span> (square) matrix A, the trace of A is defined to be the sum of its diagonal entries:</p>
<div class="arithmatex">\[
\operatorname{tr}A=\sum_{i=1}^nA_{ii}
\]</div>
<p>If <span class="arithmatex">\(a\)</span> is a real number (i.e., a 1-by-1 matrix), then <span class="arithmatex">\(\operatorname{tr} a = a\)</span>. (If you haven't seen this "opertator notation" before, you should think of the trace of <span class="arithmatex">\(A\)</span> as <span class="arithmatex">\(\operatorname{tr}(A)\)</span>, or as application of the "trace" function to the matrix <span class="arithmatex">\(A\)</span>. It's more commonly written without the parentheses, however.）
&ensp; The trace opertator has the property that for two matrices <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> such that <span class="arithmatex">\(AB\)</span> is square, we have that <span class="arithmatex">\(\operatorname{tr}(AB)=\operatorname{tr}(BA)\)</span>. <a href="../../math in ML/2. M derivation/#25">证明请查看机器学习中的数学知识中关于迹的交换律的证明。</a> As corollaries of this, we also have, e.g.,</p>
<div class="arithmatex">\[ \operatorname{tr}(ABC)=\operatorname{tr}(CAB)=\operatorname{tr}(BCA)\]</div>
<div class="arithmatex">\[ \operatorname{tr}(ABCD)=\operatorname{tr}(DABC)=\operatorname{tr}(CDAB)=\operatorname{tr}(BCDA)\]</div>
<p>The following properties of the trace operator are also easily verified. Here, <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> are square matrices, and <span class="arithmatex">\(a\)</span> is a real number:</p>
<div class="arithmatex">\[ \operatorname{tr}(A) = \operatorname{tr}(A^T)\]</div>
<div class="arithmatex">\[ \operatorname{tr}(A+B) = \operatorname{tr}(A)+\operatorname{tr}(B)\]</div>
<div class="arithmatex">\[ \operatorname{tr}(aA) = a \operatorname{tr}(A)\]</div>
<p>&ensp; We now state without proof some facts of matrix derivatives (we won’t need some of these until later this quarter). Equation (4) applies only to non-singular square matrices A, where |A| denotes the determinant of A. We have:</p>
<div class="arithmatex">\[ \nabla_A \operatorname{tr}(AB)=B^T\\\\ \tag{1}\]</div>
<div class="arithmatex">\[ \nabla_{A^T}f(A) = \big(\nabla_Af(A)\big)^T \\\\\tag{2}\]</div>
<div class="arithmatex">\[ \nabla_A \operatorname{tr}ABA^TC=CAB+C^TAB^T \\\\\tag{3} \]</div>
<div class="arithmatex">\[\nabla_A|A|=|A|(A^{-1})^T\\\\\tag{4}\]</div>
<hr />
<p>吴恩达老师不给你们证明，我来给你们证明<img alt="😎" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f60e.svg" title=":sunglasses:" /></p>
<p>请在看下面推导之前, 务必先看懂我这个知识库中<strong>机器学习中的数学知识</strong>&ensp;---&ensp;( <strong>一. 矩阵求导本质 &amp;二. 矩阵求导</strong>)。</p>
<p><img alt="2_7_1" src="../../assets/images/2_7_1.png" /></p>
<p>I. <span class="arithmatex">\(\nabla_A \operatorname{tr}(AB)=B^T\)</span></p>
<p><strong>证明:</strong></p>
<p>由 <a href="../../math in ML/2. M derivation/#24">二.矩阵求导中---(6)式的证明</a> 我们可以知道：</p>
<p>对于两个阶数都是 <span class="arithmatex">\(m \times n\)</span> 的矩阵 <span class="arithmatex">\(C_{m \times n}, D_{m \times n}\)</span> 其中<strong>一个矩阵乘以（左乘右乘都可以）另一个矩阵的 <mark>转置</mark> 的迹，本质是 <span class="arithmatex">\(C_{m \times n}, D_{m \times n}\)</span> 两个矩阵对应位置的元素相乘并相加</strong>。</p>
<p>所以这里 <span class="arithmatex">\(\operatorname{tr}(AB)\)</span> 相当于就是 <span class="arithmatex">\(A\)</span> 和 <span class="arithmatex">\(B^T\)</span>  每一个位置对应元素 <mark>相乘并相加</mark> 。
其中, <span class="arithmatex">\(A_{m \times n}, B_{n \times m}\)</span>。</p>
<p>接着, 由 <a href="../../math in ML/1. M derivation essence/#23_1">二.矩阵求导本质---(11)式</a> 我们可以知道:</p>
<p>这里求 <span class="arithmatex">\(\nabla_A \operatorname{tr}(AB)\)</span> 相当于 <span class="arithmatex">\(\operatorname{tr}(AB)\)</span> 按照 <span class="arithmatex">\(A\)</span> 矩阵分布的每个位置元素求偏导。</p>
<p>所以, 综上所述, <span class="arithmatex">\(\nabla_A \operatorname{tr}(AB)=B^T\)</span>。</p>
<p><strong>证毕。</strong></p>
<p>II. <span class="arithmatex">\(\nabla_{A^T}f(A) = \big(\nabla_Af(A)\big)^T\)</span></p>
<p><strong>证明:</strong></p>
<p>由 <a href="../../math in ML/1. M derivation essence/#23_1">二.矩阵求导本质---(11)式</a> 可知:</p>
<div class="arithmatex">\[
\begin{align}
\nabla_{A^T}f(A) &amp;=
\begin{bmatrix} \frac{\partial f}{\partial a_{11}} &amp; \cdots  &amp; \frac{\partial f}{\partial a_{m1}} 
\\ \vdots &amp; \ddots &amp; \vdots 
\\ \frac{\partial f}{\partial x_{1n}} &amp; \cdots &amp; \frac{\partial f}{\partial x_{mn}} \\
\end{bmatrix}\\\\
 &amp; = \begin{bmatrix} \frac{\partial f}{\partial a_{11}} &amp; \cdots  &amp; \frac{\partial f}{\partial a_{1n}} 
\\ \vdots &amp; \ddots &amp; \vdots 
\\ \frac{\partial f}{\partial x_{m1}} &amp; \cdots &amp; \frac{\partial f}{\partial x_{mn}} \\
\end{bmatrix}\\\\
 &amp; = \big(\nabla_Af(A)\big)^T
\end{align}
\]</div>
<p><strong>证毕。</strong></p>
<p>III. <span class="arithmatex">\(\nabla_A \operatorname{tr}ABA^TC=CAB+C^TAB^T\)</span></p>
<p><strong>证明:</strong></p>
<p>首先, 我们要明确这里的 <span class="arithmatex">\(ABA^TC\)</span> 是关于 <span class="arithmatex">\(A\)</span> 矩阵的实值标量函数, 所以, 我们可以令 <span class="arithmatex">\(f(A)=ABA^TC\)</span>。</p>
<div class="admonition note">
<p><strong>注意:</strong> 这里的标记, 由于 <span class="arithmatex">\(A\)</span> 其实是矩阵变元, 应该标记为 <span class="arithmatex">\(f(\pmb A)=\pmb A B \pmb A^TC\)</span>, 所以后面的推导过程,我们严谨一些, 将矩阵变元 <span class="arithmatex">\(A\)</span> 标记为 <span class="arithmatex">\(\pmb A\)</span>。</p>
</div>
<p>仔细想你会发现，对于实值标量函数 <span class="arithmatex">\(f (\pmb{A})\)</span>, <span class="arithmatex">\(\operatorname{tr}\big( f(\pmb A) \big)=f(\pmb A)\)</span> , <span class="arithmatex">\(\mathbb{d}f(\pmb A)=\operatorname{tr}\big( \mathbb{d}f(\pmb A) \big)\)</span></p>
<p>所以有 <span class="arithmatex">\(\mathbb{d}f(\pmb A)=\mathbb{d}\big(\operatorname{tr}f(\pmb A)\big)=\operatorname{tr}\big( \mathbb{d}f(\pmb A) \big)\)</span> 。</p>
<p>由 <a href="../../math in ML/2. M derivation/#32_1">二.矩阵求导本质---(24)式</a> 即:</p>
<div class="arithmatex">\[
\mathbb{d}f(\pmb{X})= \operatorname{tr}\Big(\frac{\partial f(\pmb{X})}{\partial \pmb{X}^T}\mathbb{d}\pmb{X}\Big)
\]</div>
<p>我们可以把一个矩阵变元的实值标量函数的全微分写成上式，我们就找到了矩阵求导的结果, 也即: </p>
<div class="arithmatex">\[
\mathbb{d}f(\pmb{A})= \operatorname{tr}\Big(\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}\mathbb{d}\pmb{A}\Big)\\\\
\tag{II.1}
\]</div>
<p>由我们证明的 II. <span class="arithmatex">\(\nabla_{A^T}f(A) = \big(\nabla_Af(A)\big)^T\)</span>得:</p>
<div class="arithmatex">\[
\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T} = \nabla _{\pmb A^T}f(\pmb A)=\big(\nabla _{\pmb A}f(\pmb A)\big)^T
\]</div>
<p>所以我们要求的:</p>
<div class="arithmatex">\[
\nabla _{\pmb A}f(\pmb A)=\Big(\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}\Big)^T \\\\
\tag{II.2}
\]</div>
<p>最终我们的任务就是转化为求 <span class="arithmatex">\(\mathbb{d}f(\pmb{A})\)</span> 的全微分, 下面开始推导：</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{d}f(\pmb{A}) &amp; = \mathbb{d}\pmb A B \pmb A^TC \\\\
 &amp; = \mathbb{d}\operatorname{tr}(\pmb A B \pmb A^TC)\\\\
 &amp; = \mathbb{d}\operatorname{tr}(C \pmb A B \pmb A^T)\\\\
 &amp; = \operatorname{tr}\mathbb{d}(C \pmb A B \pmb A^T)\\\\
 &amp; = \operatorname{tr}\Big(\mathbb{d}(C \pmb A) B \pmb A^T +  C \pmb A \mathbb{d}(B \pmb A^T)\Big)\\\\
 &amp; = \operatorname{tr}\Big(C(\mathbb{d}\pmb A) B \pmb A^T +  C \pmb A B \mathbb{d}\pmb A^T\Big)\\\\
 &amp; = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big( C \pmb A B \mathbb{d}\pmb A^T\Big)\\\\
 &amp; = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big( C \pmb A B (\mathbb{d}\pmb A)^T\Big)\\\\
 &amp; = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big(\mathbb{d}\pmb A (B^T \pmb A^T C^T )\Big)\\\\
 &amp; = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big( B^T\pmb A^T C^T  \mathbb{d}\pmb A\Big)\\\\
 &amp; = \operatorname{tr}\Big[(B \pmb A^T C + B^T\pmb A^T C^T) \mathbb{d}\pmb A\Big]\\\\
\end{aligned}
\]</div>
<blockquote>
<p>数字是步骤 汉字是每一步依据 <br>
01 -&gt; 02 实值标量函数的性质 <br>
02 -&gt; 03 迹的交换律 <br>
03 -&gt; 04 实值标量函数的性质 <br>
04 -&gt; 05 矩阵微分的乘积法则 <br>
05 -&gt; 06 夹层饼 <br>
06 -&gt; 07 迹的线性法则, 迹的交换律 <br>
07 -&gt; 08 矩阵微分的转置法则 <br>
08 -&gt; 09 转置的迹等于原矩阵的迹 <br>
09 -&gt; 10 迹的交换律 <br>
10 -&gt; 11 迹的线性法则 <br></p>
</blockquote>
<p>结合前面的 <span class="arithmatex">\((II.1)\)</span> 式可得:</p>
<div class="arithmatex">\[
\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}=B \pmb A^T C + B^T\pmb A^T C^T
\]</div>
<p>再结合前面的 <span class="arithmatex">\((II.2)\)</span> 式可得:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla _{\pmb A}f(\pmb A) &amp; = \Big(\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}\Big)^T \\\\
&amp; = (B \pmb A^T C + B^T\pmb A^T C^T)^T\\\\
&amp; = C^T \pmb A B^T + C \pmb A B 
\end{aligned}
\]</div>
<p><strong>证毕。</strong></p>
<p>IV. <span class="arithmatex">\(\nabla_A|A|=|A|(A^{-1})^T\\\\\)</span></p>
<p><strong>证明:</strong></p>
<p>由 <a href="../../math in ML/2. M derivation/#322">矩阵行列式微分 即(25.2.1)式</a>的证明可知:</p>
<div class="arithmatex">\[
\mathbb{d} |\pmb A| = \operatorname{tr}(|\pmb A|\pmb A^{-1}\mathbb{d}\pmb A)
\]</div>
<p>再由 <a href="../../math in ML/2. M derivation/#32_1">二.矩阵求导本质---(24)式</a>可得：</p>
<div class="arithmatex">\[
\frac{\partial |\pmb A|}{\partial \pmb A^T}=|\pmb A|\pmb A^{-1}
\]</div>
<p>因此,</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla _{\pmb A}|\pmb A| &amp; = (\frac{\partial |\pmb A|}{\partial \pmb A^T})^T \\\\
&amp;=(|\pmb A|\pmb A^{-1})^T\\\\
&amp;=|\pmb A|(\pmb A^{-1})^T
\end{aligned}
\]</div>
<p><strong>证毕。</strong></p>
<p>至此, 吴恩达老师省略的证明, 全部证明完毕。下面继续记笔记。</p>
<hr />
<p>To make our martix notation more concrete, let us now explain in detail the meaning of the first of these equations. Suppose we have some fixed matrix <span class="arithmatex">\(B \in \mathbb{R}^{n\times m}\)</span> . We can then define a function <span class="arithmatex">\(f :\mathbb{R}^{m\times n}\rightarrow \mathbb{R}\)</span> according to <span class="arithmatex">\(f(A)=AB\)</span>. Note that this definition makes sense, because if <span class="arithmatex">\(A \in \mathbb{R}^{m\times n}\)</span>, then <span class="arithmatex">\(AB\)</span> is a square matrix, and we can apply the trace operator to it; thus, <span class="arithmatex">\(f\)</span> does indeed map from <span class="arithmatex">\(\mathbb{R}^{m\times n}\)</span> to <span class="arithmatex">\(\mathbb{R}\)</span>. We can then apply our definition of matrix derivatives to find <span class="arithmatex">\(\nabla _Af(A)\)</span>, which will itself by an m-by-n matrix. Equation (1) above states that the <span class="arithmatex">\((i,j)\)</span> entry of this matrix will given by the <span class="arithmatex">\((i,j)\)</span>-entry of <span class="arithmatex">\(B^T\)</span> , or equivalently, by <span class="arithmatex">\(B_{j,i}\)</span> .</p>
<p>&ensp; The proofs of Equation (1-3) are reasonably simply, and are left as an exercise to the reader. Equation (4) can be derived using adjoint representation of the inverse of a martix.</p>
<h3 id="72-least-squares-revisited">7.2 Least squares revisited</h3>
<p>Armed with the tools of matrix derivatives, let us now proceed to find in closed-form the value of <span class="arithmatex">\(\theta\)</span> that minimizes <span class="arithmatex">\(J(\theta)\)</span>. We begin by re-writing <span class="arithmatex">\(J\)</span> in matrix-vectorial notation.</p>
<p>&ensp;Giving a training set, define <strong>the design matrix</strong> <span class="arithmatex">\(X\)</span> to be the m-by-n matrix (actually m-by-n+1, if we include the intercept term) that contains the training examples' input values in its row:</p>
<div class="arithmatex">\[
X = \begin{bmatrix} --- &amp; (x^{(1)})^T &amp; --- \\\\ --- &amp; (x^{(2)})^T &amp;  ---
\\\\ &amp; \vdots &amp;
\\\\ --- &amp; (x^{(m)})^T &amp; ---
\end{bmatrix}
\]</div>
<p>Also, let <span class="arithmatex">\(\vec{y}\)</span> be the m-dimensional vector containing all the target values from the training set:</p>
<div class="arithmatex">\[
\vec{y}=\begin{bmatrix} 
y^{(1)}\\\\
y^{(2)}\\\\
\vdots\\\\
y^{(m)}
\end{bmatrix}
\]</div>
<p>Now, since <span class="arithmatex">\(h_\theta\big(x^{(i)}\big)=(x^{(i)})^T\theta\)</span>, we can easily verify that</p>
<div class="arithmatex">\[
\begin{aligned}
X\theta-\vec{y} &amp;= 
\begin{bmatrix} (x^{(1)})^T\theta \\\\  \vdots  \\\\
(x^{(m)})^T\theta
\end{bmatrix} - \begin{bmatrix}  y^{(1)}\\\\
\vdots\\\\
y^{(m)}
\end{bmatrix}\\\\
&amp; = \begin{bmatrix}
(x^{(1)})^T\theta- y^{(1)} \\\\
  \vdots \\\\
(x^{(m)})^T\theta- y^{(m)} 
\end{bmatrix}
\end{aligned}
\]</div>
<p>Thus, using the fact for a vector <span class="arithmatex">\(z\)</span> , we have that <span class="arithmatex">\(z^Tz=\sum_{i}z_i^2\)</span> .</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{1}{2}(X\theta-\vec{y})^T(X\theta-\vec{y})&amp;=\frac{1}{2}\sum_{i=1}^m\Big(h_\theta(x^{(i)})- y^{(i)} \Big)^2\\\\
&amp;=J(\theta)
\end{aligned}
\]</div>
<p>Finally , to minimize <span class="arithmatex">\(J\)</span> , lets find its derivatives with respect to <span class="arithmatex">\(\theta\)</span> . Combining Equations (2) and (3) , we find that</p>
<div class="arithmatex">\[
\nabla _{A^T}\operatorname{tr}(ABA^TC)=B^TA^TC^T+BA^TC \\\\
\tag{5}
\]</div>
<p>Hence,</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla _\theta J(\theta) &amp; = \nabla _\theta \frac{1}{2}(X\theta-\vec{y})^T(X\theta-\vec{y}) \\\\
&amp; = \frac{1}{2} \nabla _\theta(\theta^TX^TX\theta-\theta^TX^T \vec{y}-\vec{y}^TX \theta+\vec{y}^T \vec{y})\\\\
&amp; = \frac{1}{2} \nabla _\theta \operatorname{tr}(\theta^TX^TX\theta-\theta^TX^T \vec{y}-\vec{y}^TX \theta+\vec{y}^T \vec{y})\\\\
&amp; = \frac{1}{2} \nabla _\theta\Big(\operatorname{tr}(\theta^TX^TX\theta)-2\operatorname{tr}(\vec{y}^TX \theta)\Big)\\\\
&amp; = \frac{1}{2}(X^TX \theta+X^TX \theta-2X^T \vec{y})\\\\
&amp; = X^TX \theta-X^T \vec{y}
\end{aligned}
\]</div>
<p>In the third step, we used the fact that the trace if a real number is just the real number; the fourth step used the fact that <span class="arithmatex">\(\operatorname{tr}(A)=\operatorname{tr}(A^T)\)</span> , and the fifth step used Equation (5) with <span class="arithmatex">\(A^T=\theta\)</span>, <span class="arithmatex">\(B=B^T=X^TX\)</span> , and <span class="arithmatex">\(C=I\)</span>, and Equation (1). To minimize <span class="arithmatex">\(J\)</span> , we set its derivatives to zero, and obtain the normal equations:</p>
<div class="arithmatex">\[
X^TX \theta=X^T \vec{y}
\]</div>
<p>Thus, the value of <span class="arithmatex">\(\theta\)</span> that minimize <span class="arithmatex">\(J(\theta)\)</span> is given in closed form by the equation</p>
<div class="arithmatex">\[
\theta = (X^TX)^{-1}X^T \vec{y}
\]</div>
<h2 id="8">8. 正规方程及不可逆性</h2>
<div class="admonition note">
<p>参考视频: 4 - 7 - Normal Equation Noninvertibility (Optional) (6 min).mkv</p>
</div>
<p>在这段视频中谈谈正规方程 ( <strong>normal equation</strong> )，以及它们的不可逆性。</p>
<p>我们要讲的问题如下：<span class="arithmatex">\(\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y\)</span> 当计算 <span class="arithmatex">\(\theta\)</span>=<code>inv(X'X ) X'y</code> ，那对于矩阵 <span class="arithmatex">\(X'X\)</span> 的结果是不可逆的情况咋办呢?</p>
<p>我们都知道，有些矩阵可逆(<strong>invertible</strong>)，而有些矩阵不可逆(<strong>non-invertible</strong>)。我们称那些不可逆矩阵为奇异(<strong>singular</strong>)或退化(<strong>dgenerate</strong>)矩阵。</p>
<p>首先, 说一下  <strong><span class="arithmatex">\(\pmb X'\pmb X\)</span> 不可逆的原因</strong>。</p>
<ul>
<li>
<ol>
<li><strong>特征值线性相关</strong></li>
</ol>
<p>例如，在预测住房价格时，如果<span class="arithmatex">\({x_{1}}\)</span>是以英尺为尺寸规格计算的房子，<span class="arithmatex">\({x_{2}}\)</span>是以平方米为尺寸规格计算的房子，同时，你也知道1米等于3.28英尺 ( 四舍五入到两位小数 )，这样，你的这两个特征值将始终满足约束：<span class="arithmatex">\({x_{1}}={x_{2}}*{{\left( 3.28 \right)}^{2}}\)</span>。 实际上，如果你用这样的一个线性方程，来展示那两个相关联的特征值，矩阵<span class="arithmatex">\(X'X\)</span>将是不可逆的。</p>
</li>
<li>
<ol>
<li><strong>特征值的数量小于训练集的数量</strong></li>
</ol>
<p>具体地说，在<span class="arithmatex">\(m\)</span>小于或等于n的时候，例如，有<span class="arithmatex">\(m\)</span>等于10个的训练样本也有<span class="arithmatex">\(n\)</span>等于100的特征数量。要找到适合的<span class="arithmatex">\((n +1)\)</span> 维参数矢量<span class="arithmatex">\(\theta\)</span>，这将会变成一个101维的矢量，尝试从10个训练样本中找到满足101个参数的值，这工作可能会让你花上一阵子时间，但这并不总是一个好主意。因为，正如我们所看到你只有10个样本，以适应这100或101个参数，数据还是有些少。<strong>这相当于说是多元方程组中未知数的个数远大于方程的个数。</strong></p>
</li>
</ul>
<p>稍后我们将看到，<strong>如何使用小数据样本以得到这100或101个参数</strong>，<strong>通常，我们会使用</strong>一种叫做<strong>正则化</strong>的线性代数方法，<strong>通过删除某些特征或者是使用某些技术，来解决当<span class="arithmatex">\(m\)</span>比<span class="arithmatex">\(n\)</span>小的时候的问题</strong>。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。</p>
<p>总之当你发现的矩阵<span class="arithmatex">\(X'X\)</span>的结果是奇异矩阵，或者找到的其它矩阵是不可逆的，我会建议你这么做。</p>
<p>首先，<strong>看特征值里是否有一些多余的特征</strong>，像这些<span class="arithmatex">\({x_{1}}\)</span>和<span class="arithmatex">\({x_{2}}\)</span>是<strong>线性相关</strong>的，互为线性函数。同时，当有一些多余的特征时，可以<strong>删除</strong>这两个重复特征里的<strong>其中一个</strong>，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果<strong>特征数量</strong>实在<strong>太多</strong>，我会<strong>用较少的特征</strong>来<strong>反映尽可能多内容</strong>，<strong>否则</strong>我会考虑<strong>使用正规化方法</strong>。</p>
<p>如果矩阵<span class="arithmatex">\(X'X\)</span>是不可逆的，（通常来说，不会出现这种情况），如果在<strong>Octave</strong>里，可以用伪逆函数<code>pinv()</code> 来实现。这种使用不同的线性代数库的方法被称为伪逆。即使<span class="arithmatex">\(X'X\)</span>的结果是不可逆的，但算法执行的流程是正确的。总之，出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，出现不可逆的问题不应该过多的关注<span class="arithmatex">\({X^{T}}X\)</span>是不可逆的。</p>
<h2 id="_1">习题 &amp;&amp; 参考答案</h2>
<h3 id="_2">第一题</h3>
<p>假设m=4个学生上了一节课, 有期中考试和期末考试。你已经收集了他们在两次考试中的分数数据集，如下所示：</p>
<table>
<thead>
<tr>
<th>期中得分</th>
<th>(期中得分)^2</th>
<th>期末得分</th>
</tr>
</thead>
<tbody>
<tr>
<td>89</td>
<td>7921</td>
<td>96</td>
</tr>
<tr>
<td>72</td>
<td>5184</td>
<td>74</td>
</tr>
<tr>
<td>94</td>
<td>8836</td>
<td>87</td>
</tr>
<tr>
<td>69</td>
<td>4761</td>
<td>78</td>
</tr>
</tbody>
</table>
<p>你想用多项式回归来预测一个学生的期中考试成绩。具体地
说, 假设你想拟合一个 <span class="arithmatex">\(h_ \theta (x) = \theta _0+\theta _1x_1++\theta _2x_1\)</span> 的模型, 其中x1是期中得分, x2是（期中得分）^2。此外, 你计划同时使用特征缩放（除以特征的“最大值-最小值”或范围）和均值归一化。</p>
<p>那么标准化后的 <span class="arithmatex">\(x_2^{(4)}\)</span> 特征值是多少？（提示：期中=89，期末=96是训练示例1）</p>
<h3 id="_3">第二题</h3>
<p>用 <span class="arithmatex">\(\alpha=0.3\)</span> 进行15次梯度下降迭代, 每次迭代 <span class="arithmatex">\(j(\theta)\)</span> 后计算。你会发现 <span class="arithmatex">\(j(\theta)\)</span> 的值下降缓慢, 并且在15次迭代后仍在下降。基于此, 以下哪个结论似乎最可信？</p>
<p>A.  <span class="arithmatex">\(\alpha=0.3\)</span> 是学习率的有效选择。</p>
<p>B. 与其使用  <span class="arithmatex">\(\alpha\)</span> 当前值, 不如尝试更小的  <span class="arithmatex">\(\alpha\)</span> 值（比如  <span class="arithmatex">\(\alpha=0.1\)</span> ）</p>
<p>C. 与其使用  <span class="arithmatex">\(\alpha\)</span> 当前值, 不如尝试更大的  <span class="arithmatex">\(\alpha\)</span> 值（比如  <span class="arithmatex">\(\alpha=1.0\)</span> ）</p>
<h3 id="_4">第三题</h3>
<p>假设您有m=14个训练示例, 有n=3个特性（不包括需要另外添加的恒为1的截距项）, 正规方程是 <span class="arithmatex">\(\theta=(X^TX)^{-1}X^Ty\)</span>。对于给定m和n的值, 这个方程中 <span class="arithmatex">\(\theta, X, y\)</span> 的维数分别是多少？</p>
<p>A. <span class="arithmatex">\(X \  14 \times 3, y \  14\times 1, \  \theta 3 \times 3\)</span></p>
<p>B. <span class="arithmatex">\(X \  14 \times 4, y \  14\times 1, \  \theta 4 \times 1\)</span></p>
<p>C. <span class="arithmatex">\(X \  14 \times 3, y \  14\times 1, \  \theta 3 \times 1\)</span></p>
<p>D. <span class="arithmatex">\(X \  14 \times 4, y \  14\times 4, \  \theta 4 \times 4\)</span></p>
<h3 id="_5">第 四 题</h3>
<p>假设您有一个数据集，每个示例有m=1000000个示例和n=200000个特性。你想用多元线性回归来拟合参数 <span class="arithmatex">\(\theta\)</span> 到我们的数据。你更应该用梯度下降还是正规方程？</p>
<p>A. 梯度下降，因为正规方程中 <span class="arithmatex">\(\theta=(X^TX)^{-1}\)</span> 中计算非常慢</p>
<p>B. 正规方程，因为它提供了一种直接求解的有效方法</p>
<p>C. 梯度下降，因为它总是收敛到最优 <span class="arithmatex">\(\theta\)</span></p>
<p>D. 正规方程，因为梯度下降可能无法找到最优 <span class="arithmatex">\(\theta\)</span></p>
<h3 id="_6">第 五 题</h3>
<p>以下哪些是使用特征缩放的原因？</p>
<p>A. 它可以防止梯度下降陷入局部最优</p>
<p>B. 它通过降低梯度下降的每次迭代的计算成本来加速梯度下降</p>
<p>C. 它通过减少迭代次数来获得一个好的解，从而加快了梯度下降的速度</p>
<p>D. 它防止矩阵 <span class="arithmatex">\(X^TX\)</span>（用于正规方程）不可逆（奇异/退化）</p>
<p><br/>
<br/>
<br/>
<br/>
<br/>
<br/></p>
<h3 id="_7">参考答案</h3>
<p>第一题：-0.47</p>
<p><br/>
第二题：C</p>
<p><br/></p>
<p>第三题：B</p>
<p><br/></p>
<p>第四题：A</p>
<p><br/></p>
<p>第五题：C</p>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../1.%20linear%20regression-1%20v/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 一. 单变量线性回归" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              一. 单变量线性回归
            </div>
          </div>
        </a>
      
      
        
        <a href="../../math%20in%20ML/" class="md-footer__link md-footer__link--next" aria-label="下一页: 前言和目录" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              前言和目录
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top"], "search": "../../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c44cc438.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>