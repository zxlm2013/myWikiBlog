
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>二. 多变量线性回归 - 大白的知识库</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="大白的知识库" class="md-header__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            大白的知识库
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              二. 多变量线性回归
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="大白的知识库" class="md-nav__button md-logo" aria-label="大白的知识库" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    大白的知识库
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        前言和目录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../1.%20linear%20regression-1%20v/" class="md-nav__link">
        一. 单变量线性回归
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../matrix%20derivatives/" class="md-nav__link">
        补充知识点I-矩阵求导
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          二. 多变量线性回归
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        二. 多变量线性回归
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1. 多维特征
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2. 多变量梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-1-" class="md-nav__link">
    3. 梯度下降法实用技巧1-特征缩放
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-2-" class="md-nav__link">
    4. 梯度下降法实用技巧2-学习率
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5. 特征和多项式回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    6. 正规方程
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1. 多维特征
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2. 多变量梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-1-" class="md-nav__link">
    3. 梯度下降法实用技巧1-特征缩放
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-2-" class="md-nav__link">
    4. 梯度下降法实用技巧2-学习率
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5. 特征和多项式回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    6. 正规方程
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


  <h1>二. 多变量线性回归</h1>

<h2 id="1">1. 多维特征</h2>
<div class="admonition info">
<p>参考视频:
4 - 1 - Multiple Features (8 min).mkv</p>
</div>
<p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为<span class="arithmatex">\(\left( {x_{1}},{x_{2}},...,{x_{n}} \right)\)</span>。</p>
<table>
<thead>
<tr>
<th align="center">Size (feet2)</th>
<th align="center">Number   of bedrooms</th>
<th align="center">Number   of floors</th>
<th align="center">Age of home (years)</th>
<th align="center">Price   ($1000)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">2104</td>
<td align="center">5</td>
<td align="center">1</td>
<td align="center">45</td>
<td align="center">460</td>
</tr>
<tr>
<td align="center">1416</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">40</td>
<td align="center">232</td>
</tr>
<tr>
<td align="center">1534</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">30</td>
<td align="center">315</td>
</tr>
<tr>
<td align="center">852</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">36</td>
<td align="center">178</td>
</tr>
<tr>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
</tbody>
</table>
<p>增添更多特征后，我们引入一系列新的注释：</p>
<p><span class="arithmatex">\(n\)</span> 代表特征的数量</p>
<p><span class="arithmatex">\({x^{\left( i \right)}}\)</span> 代表第 <span class="arithmatex">\(i\)</span> 个训练实例，是特征矩阵中的第<span class="arithmatex">\(i\)</span>行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>比方说，上图的</p>
<p><span class="arithmatex">\({x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}\)</span>，</p>
<p><span class="arithmatex">\({x}_{j}^{\left( i \right)}\)</span>代表特征矩阵中第 <span class="arithmatex">\(i\)</span> 行的第 <span class="arithmatex">\(j\)</span> 个特征，也就是第 <span class="arithmatex">\(i\)</span> 个训练实例的第 <span class="arithmatex">\(j\)</span> 个特征。</p>
<p>如上图的<span class="arithmatex">\(x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2\)</span>，</p>
<p>支持多变量的假设 <span class="arithmatex">\(h\)</span> 表示为：<span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span>，</p>
<p>这个公式中有<span class="arithmatex">\(n+1\)</span>个参数和<span class="arithmatex">\(n\)</span>个变量，为了使得公式能够简化一些，引入<span class="arithmatex">\(x_{0}=1\)</span>，则公式转化为：<span class="arithmatex">\(h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span></p>
<p><img alt="2_1_1_theta_T_X" src="../../assets/images/2_1_1_theta_T_X.png" /></p>
<p>此时模型中的参数是一个<span class="arithmatex">\(n+1\)</span>维的向量，任何一个训练实例也都是<span class="arithmatex">\(n+1\)</span>维的向量，特征矩阵<span class="arithmatex">\(X\)</span>的维度是 <span class="arithmatex">\(m*(n+1)\)</span>。 因此公式可以简化为：<span class="arithmatex">\(h_{\theta} \left( x \right)={\theta^{T}}X\)</span>，其中上标  <span class="arithmatex">\(T\)</span> 代表矩阵转置。</p>
<p>注意：
从上图可知, <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(X\)</span> 我们是用<strong>列向量</strong>来标记。计算 <span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span> 的时候, 可以将 <span class="arithmatex">\(h\)</span> 表示为 <span class="arithmatex">\(\theta^TX.\)</span></p>
<h2 id="2">2. 多变量梯度下降</h2>
<div class="admonition info">
<p>参考视频:
4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv</p>
</div>
<p>快速回顾我们的记号，并用向量简化。</p>
<p>Hypothesis: <span class="arithmatex">\(h_\theta(x)=\theta_0+\theta_1x+\theta_2x+...+\theta_nx\)</span></p>
<p>简记为： <span class="arithmatex">\(h_\theta(x)=\theta^TX\)</span></p>
<p>Parameters: <span class="arithmatex">\(\theta_0,\theta_1,\theta_2,...\theta_n\)</span></p>
<p>简记为：<span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(n+1\ dimension\ vector\)</span></p>
<p>Cost Function: </p>
<div class="arithmatex">\[
J(\theta_0,\theta_1,\ \theta_2,...\theta_n)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\]</div>
<p>简记为：</p>
<div class="arithmatex">\[
J(\theta)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\]</div>
<p>Gradient descent:
<strong>Repeat {</strong></p>
<p>​                <span class="arithmatex">\({\theta_{j}}:={\theta_{j}}-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,...\theta_n)\)</span></p>
<p>​               <strong>}</strong> (simultaneously update for every j=0, 1, 2, ..., n)
简记为：把上面的<span class="arithmatex">\(J(\theta_0,...\theta_n)\)</span>换成<span class="arithmatex">\(J(\theta)\)</span></p>
<p><img alt="2_2_1_gradient_descent_v2" src="../../assets/images/2_2_1_gradient_descent_v2.png" /></p>
<p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：<span class="arithmatex">\(J\left( {\theta_{0}},{\theta_{1}}...{\theta_{n}} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}\)</span> ，</p>
<p>其中：<span class="arithmatex">\(h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span> ，</p>
<p>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。
多变量线性回归的批量梯度下降算法为：</p>
<p>当<span class="arithmatex">\(n&gt;=1\)</span>时，
<span class="arithmatex">\({{\theta }_{0}}:={{\theta }_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}\)</span></p>
<p><span class="arithmatex">\({{\theta }_{1}}:={{\theta }_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}\)</span></p>
<p><span class="arithmatex">\({{\theta }_{2}}:={{\theta }_{2}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}\)</span></p>
<p>...</p>
<p><span class="arithmatex">\({{\theta }_{n}}:={{\theta }_{n}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{n}^{(i)}\)</span></p>
<p>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。</p>
<p>代码示例：</p>
<p>计算代价函数
<span class="arithmatex">\(J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}\)</span>
其中：<span class="arithmatex">\({h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}\)</span></p>
<p><strong>Python</strong> 代码：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">inner</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(((</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">inner</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</code></pre></div>
<hr>

<p>下面，我们开始推导<span class="arithmatex">\(\frac{\partial}{\partial\theta}J(\theta)\)</span>。这里为了普遍性，还原了 <span class="arithmatex">\(\theta\)</span> 的普遍情况，也即有n个 <span class="arithmatex">\(\theta\)</span> 的情况。</p>
<p>Hypothesis:         <span class="arithmatex">\(h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\)</span></p>
<p>Parameters:         <span class="arithmatex">\(\theta_0,\theta_1,...,\theta_n\)</span></p>
<p>Cost function: </p>
<div class="arithmatex">\[
J(\theta_0,\theta_1,...,\theta_n)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\]</div>
<p>Gradient descent:
Repeat{
$$
\theta_j :=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n)
$$
​        }（simultaneously update for every j=0, 1, 2, ..., n)）</p>
<p>推导： <span class="arithmatex">\(\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n)=?\)</span></p>
<p>Let's first work it for the case if we have only one training example <span class="arithmatex">\((x,y)\)</span>, so that we can neglect the sum in the definition <span class="arithmatex">\(J\)</span>. We have:
$$
\begin{split}
\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n) &amp; =  \frac {1} {2}\frac{\partial}{\partial \theta_j}\Big(h_\theta(x)-y\Big)^2 \\
 &amp; = 2\cdot\frac{1}{2}\cdot(h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(h_\theta(x)-y) \\
 &amp; = (h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n) \\
 &amp; = (h_\theta(x)-y)\cdot x_j
\end{split}
$$</p>
<h2 id="3-1-">3. 梯度下降法实用技巧1-特征缩放</h2>
<div class="admonition info">
<p>参考视频:
4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv</p>
</div>
<p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。
解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：</p>
<p><img alt="2_3_1_feature_scaling" src="../../assets/images/2_3_1_feature_scaling.png" /></p>
<p>视频里吴恩达老师的方法是令：<span class="arithmatex">\({{x}_{n}}=\frac{{{x}_{n}}-mean}{{max(x)-min(x)}}\)</span>, 其中 <span class="arithmatex">\(mean\)</span>是平均值，<span class="arithmatex">\(max(x)-min(x)\)</span>分别是最大值和最小值。</p>
<p>更通用的是令：<span class="arithmatex">\({{x}_{n}}=\frac{{{x}_{n}}-{mean}}{\sigma}\)</span>，其中 <span class="arithmatex">\(mean\)</span>是平均值，<span class="arithmatex">\(\sigma\)</span>是标准差。</p>
<p>python里的api：</p>
<ul>
<li>
<p><strong>sklearn.preprocessing.StandardScaler()</strong></p>
<ul>
<li>处理之后每列来说<strong>所有数据都聚集在均值0附近标准差差为1</strong></li>
<li><strong>StandardScaler.fit_transform(X)</strong><ul>
<li>X:numpy array格式的数据[n_samples,n_features]</li>
</ul>
</li>
<li>返回值：转换后的形状相同的array</li>
</ul>
</li>
</ul>
<p>部分参考代码：
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="k">def</span> <span class="nf">linear_model1</span><span class="p">():</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span class="c1"># 1.获取数据</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    <span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="c1"># 2. 数据集划分</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>    <span class="c1"># 3. 特征工程-标准化</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>    <span class="n">transfer</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>    <span class="n">x_train</span> <span class="o">=</span> <span class="n">transfer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    <span class="n">x_test</span> <span class="o">=</span> <span class="n">transfer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="4-2-">4. 梯度下降法实用技巧2-学习率</h2>
<div class="admonition info">
<p>参考视频:
4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv</p>
</div>
<p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。</p>
<p><img alt="2_4_1_J_iterations" src="../../assets/images/2_4_1_J_iterations.png" /></p>
<p>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看左上方这样的图表更好。</p>
<p>不正确的学习率，会产生左侧上下两个图像。
<img alt="2_4_2_no_proper_alpha" src="../../assets/images/2_4_2_no_proper_alpha.png" /></p>
<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率 <span class="arithmatex">\(\alpha\)</span> 过小，则达到收敛所需的迭代次数会非常高；如果学习率 <span class="arithmatex">\(\alpha\)</span> 过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：</p>
<p><span class="arithmatex">\(\alpha= 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\)</span></p>
<h2 id="5">5. 特征和多项式回归</h2>
<div class="admonition info">
<p>参考视频:
4 - 5 - Features and Polynomial Regression (8 min).mkv</p>
</div>
<p>如房价预测问题，</p>
<p><img alt="2_5_1_house" src="../../assets/images/2_5_1_house.png" /></p>
<p><span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}\)</span> </p>
<p>当我们真正应用线性回归模型的时候，我们可以创造自己的特征即：
<span class="arithmatex">\({x_{1}}=frontage\)</span>（临街宽度），<span class="arithmatex">\({x_{2}}=depth\)</span>（纵向深度），<span class="arithmatex">\(x=frontage*depth=area\)</span>（面积），
则：<span class="arithmatex">\({h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x\)</span>。</p>
<p><img alt="2_5_2_polynomial_regression" src="../../assets/images/2_5_2_polynomial_regression.png" /></p>
<p>线性回归并不适用于所有数据，有时我们需要其他模型来适应我们的数据，比如一个二次方模型：<span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}\)</span>
 或者三次方模型： <span class="arithmatex">\(h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}\)</span> </p>
<p>从上面图右侧，可以看出如果我们采用多项式回归模型，在运行梯度下降算法前特征缩放的重要性了。</p>
<p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：</p>
<p><span class="arithmatex">\({{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}\)</span>，从而将模型转化为线性回归模型。</p>
<p>根据函数图形特性，我们还可以使：</p>
<p><span class="arithmatex">\({{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta}_{2}}{{(size)}^{2}}\)</span></p>
<p>或者:</p>
<p><span class="arithmatex">\({{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta }_{2}}\sqrt{size}\)</span></p>
<p><img alt="2_5_3_choice_of_X" src="../../assets/images/2_5_3_choice_of_X.png" /></p>
<h2 id="6">6. 正规方程</h2>
<div class="admonition info">
<p>参考视频:
4 - 6 - Normal Equation (16 min).mkv</p>
</div>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img alt="2_6_1" src="../../assets/images/2_6_1.png" /></p>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：<span class="arithmatex">\(\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0\)</span> 。
 假设我们的训练集特征矩阵为 <span class="arithmatex">\(X\)</span>（包含了 <span class="arithmatex">\({{x}_{0}}=1\)</span>）并且我们的训练集结果为向量 <span class="arithmatex">\(y\)</span>，则利用正规方程解出向量 <span class="arithmatex">\(\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y\)</span> 。</p>
<p>以下面表格数据为例 <span class="arithmatex">\(m=4\)</span>：</p>
<table>
<thead>
<tr>
<th><span class="arithmatex">\(x_0\)</span></th>
<th>Size (feet2)</th>
<th>Number   of bedrooms</th>
<th>Number   of floors</th>
<th>Age of home (years)</th>
<th>Price   ($1000)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2104</td>
<td>5</td>
<td>1</td>
<td>45</td>
<td>460</td>
</tr>
<tr>
<td>1</td>
<td>1416</td>
<td>3</td>
<td>2</td>
<td>40</td>
<td>232</td>
</tr>
<tr>
<td>1</td>
<td>1534</td>
<td>3</td>
<td>2</td>
<td>30</td>
<td>315</td>
</tr>
<tr>
<td>1</td>
<td>852</td>
<td>2</td>
<td>1</td>
<td>36</td>
<td>178</td>
</tr>
</tbody>
</table>
<p><span class="arithmatex">\(X\ \text{=}\begin{bmatrix}1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45\\1 &amp; 1416 &amp; 3 &amp; 2 &amp; 40\\ 1 &amp;1534 &amp; 3 &amp; 2 &amp; 30\\ 1 &amp; 852 &amp; 2 &amp; 1 &amp; 36\end{bmatrix}\)</span>，<span class="arithmatex">\(y\ \text{=}\begin{bmatrix} 460\\232\\315\\178\end{bmatrix}\)</span></p>
<p><span class="arithmatex">\(X维度：(m,n+1),\ y的维度：(m,1)\)</span></p>
<p>这时候求解 <span class="arithmatex">\(\theta\)</span> 只需一步:  <span class="arithmatex">\(\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y\)</span>。</p>
<p>将上面的例子推广到一般情况</p>
<p><img alt="2_6_2" src="../../assets/images/2_6_2.png" /></p>
<p>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。
千万要注意这里的设计矩阵X它的构成，设计完成后，假设函数可以向量化为 <span class="arithmatex">\(h_\theta(x)=X\theta\)</span></p>
<p>梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率<span class="arithmatex">\(\alpha\)</span></td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量<span class="arithmatex">\(n\)</span>大时也能较好适用</td>
<td>需要计算<span class="arithmatex">\({{\left( {{X}^{T}}X \right)}^{-1}}\)</span> 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为<span class="arithmatex">\(O\left( {{n}^{3}} \right)\)</span>，通常来说当<span class="arithmatex">\(n\)</span>小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody>
</table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，<strong>只要特征变量数量小于一万，我通常使用正规方程法，而不使用梯度下降法。</strong></p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<p>正规方程的<strong>python</strong>实现：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a> <span class="k">def</span> <span class="nf">normalEqn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>   <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">)</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span> <span class="c1">#X.T@X等价于X.T.dot(X)</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>   <span class="k">return</span> <span class="n">theta</span>
</code></pre></div>
<hr>

<p>下面开始推导： <span class="arithmatex">\(\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y\)</span></p>
<p>首先把已知都矩阵/向量化：</p>
<p>Hypothesis:         <span class="arithmatex">\(h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n=X\theta\)</span></p>
<p>Cost function: </p>
<div class="arithmatex">\[
\begin{split}
J(\theta_0,\theta_1,...,\theta_n) &amp; = \frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 \\&amp; = \frac {1} {2m}(X\theta-y)^2
\end{split}
\]</div>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数即：
<span class="arithmatex">\(\frac{\partial}{\partial\theta}J(\theta)=0\)</span></p>
<p>而</p>
<div class="arithmatex">\[
(X\theta-y)^2  = (X\theta-y)^T(X\theta-y)           
\]</div>
<p>所以</p>
<div class="arithmatex">\[
\begin{split}
\frac{\partial}{\partial\theta}J(\theta)=\frac{1}{2m}(\frac{\partial\theta^TX^TX\theta}{\partial\theta}-\frac{\partial\theta^TX^Ty}{\partial\theta}-\frac{\partial{y^T}X\theta}{\partial\theta}+\frac{\partial{y^Ty}}{\partial\theta})=0
\end{split}
\]</div>
<blockquote>
<p>继续做之前，先来推导一下几个矩阵求导公式
这里的A和X与上面毫无关系！！！</p>
</blockquote>
<p><span class="arithmatex">\(A\ \text{=}\begin{bmatrix} a_{11} &amp; a_{12}&amp; ... &amp;a_{1n}\\\
                            a_{21}&amp; a_{22} &amp; ... &amp; a_{2n}\\\
                            ... &amp; ... &amp; ...&amp; ...\\\
                            a_{m1}&amp; a_{m2} &amp; ...&amp; a_{mn}
\end{bmatrix},\  矩阵A是一个m行n列的矩阵\)</span></p>
<p><span class="arithmatex">\(x=(x_1, x_2,...,x_n)^T\)</span>是n行1列的向量</p>
<p>求 <span class="arithmatex">\(\frac{\partial{x}^TA^TAx}{\partial{x}}\)</span>, <span class="arithmatex">\(\frac{\partial{x}^TA^T}{\partial{x}}\)</span>, <span class="arithmatex">\(\frac{\partial{Ax}}{\partial{x}}\)</span></p>
<div class="arithmatex">\[
x^TA^TAx\ \text{=}\begin{bmatrix} a_{11} &amp; a_{12}&amp; ... &amp;a_{1n}\\\
                            a_{21}&amp; a_{22} &amp; ... &amp; a_{2n}\\\
                            ... &amp; ... &amp; ...&amp; ...\\\
                            a_{m1}&amp; a_{m2} &amp; ...&amp; a_{mn}
\end{bmatrix}
\]</div>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../matrix%20derivatives/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 补充知识点I-矩阵求导" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              补充知识点I-矩阵求导
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top"], "search": "../../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c44cc438.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>