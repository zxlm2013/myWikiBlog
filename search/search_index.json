{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u6b22\u8fce\u6765\u5230\u5927\u767d\u7684\u77e5\u8bc6\u5e93 \u4eae\u70b9 \u672c\u7b14\u8bb0\u66f4\u4fa7\u91cd\u516c\u5f0f\u63a8\u5bfc, \u5e95\u5c42\u4ee3\u7801\u5b9e\u73b0(python)\u3002(\u6362\u8a00\u4e4b, \u5176\u4ed6\u90fd\u662f \u6284 \u53c2\u8003 \u7684, \u54c8\u54c8\u54c8\u54c8\u54c8) \u63d2\u66f2 \u539f\u6765\u53ea\u60f3\u7528gitbook\u505a\u4e2a\u7535\u5b50\u4e66\uff0c\u540e\u6765\u53d1\u73b0gitbook\u592a\u9ebb\u70e6\u4e86\u3002\u4e00\u4e0d\u5c0f\u5fc3\u6574\u4e86\u4e00\u4e2a\u4e2a\u4eba\u77e5\u8bc6\u5e93\u3002 \u671f\u95f4\u8fd8\u5c1d\u8bd5\u4f7f\u7528vuepress\u642d\u535a\u5ba2\uff0c\u7ed3\u679c\u53d1\u73b0\u516c\u5f0f\u4e0d\u652f\u6301\u5404\u79cd\u62a5\u9519\uff0c\u5d29\u6e83... \u8fd9\u4e2amkdocs+materials\u56fd\u5185\u5e76\u6ca1\u6709\u5b8c\u6574\u53ef\u9760\u7684\u6559\u7a0b\uff0c\u5b8c\u5168\u9760\u81ea\u5df1\u770b\u5b98\u65b9\u6587\u6863\u6478\u7d22\u641e\u51fa\u6765\u7684\u3002 \u6709\u4e00\u4e9b\u70ab\u9177\u7684\u529f\u80fd\u6211\u600e\u4e48\u4e5f\u641e\u4e0d\u51fa\u6765\uff0c\u8fd8\u4ee5\u4e3a\u662f\u81ea\u5df1\u7684\u95ee\u9898\u3002\u7ed3\u679c\u662f\u8fd9\u4e2a\u4e3b\u9898\u7adf\u7136\u8fd8\u6709\u90e8\u5206\u529f\u80fd\u53ea\u6709\u6350\u8d60\u8005\u624d\u80fd\u7528... \u5728\u7535\u8111\u4e0a\u5199\u516c\u5f0f, \u771f\u7684\u5f88\u75db\u82e6\u3002 \u5c55\u671b \u5b66\u6210\u4e4b\u540e, \u5e0c\u671b\u80fd\u628a\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u5e94\u7528\u5230\u672c\u804c\u5de5\u4f5c\u3002 \u5927\u767d 2022/04/09 15:57:21 \u4e8e\u4e0a\u6d77\u7535\u529b\u5927\u5b66\u6559\u5e08\u516c\u5bd3","title":"\u9996\u9875"},{"location":"#_1","text":"","title":"\u6b22\u8fce\u6765\u5230\u5927\u767d\u7684\u77e5\u8bc6\u5e93"},{"location":"#_2","text":"\u672c\u7b14\u8bb0\u66f4\u4fa7\u91cd\u516c\u5f0f\u63a8\u5bfc, \u5e95\u5c42\u4ee3\u7801\u5b9e\u73b0(python)\u3002(\u6362\u8a00\u4e4b, \u5176\u4ed6\u90fd\u662f \u6284 \u53c2\u8003 \u7684, \u54c8\u54c8\u54c8\u54c8\u54c8)","title":"\u4eae\u70b9"},{"location":"#_3","text":"\u539f\u6765\u53ea\u60f3\u7528gitbook\u505a\u4e2a\u7535\u5b50\u4e66\uff0c\u540e\u6765\u53d1\u73b0gitbook\u592a\u9ebb\u70e6\u4e86\u3002\u4e00\u4e0d\u5c0f\u5fc3\u6574\u4e86\u4e00\u4e2a\u4e2a\u4eba\u77e5\u8bc6\u5e93\u3002 \u671f\u95f4\u8fd8\u5c1d\u8bd5\u4f7f\u7528vuepress\u642d\u535a\u5ba2\uff0c\u7ed3\u679c\u53d1\u73b0\u516c\u5f0f\u4e0d\u652f\u6301\u5404\u79cd\u62a5\u9519\uff0c\u5d29\u6e83... \u8fd9\u4e2amkdocs+materials\u56fd\u5185\u5e76\u6ca1\u6709\u5b8c\u6574\u53ef\u9760\u7684\u6559\u7a0b\uff0c\u5b8c\u5168\u9760\u81ea\u5df1\u770b\u5b98\u65b9\u6587\u6863\u6478\u7d22\u641e\u51fa\u6765\u7684\u3002 \u6709\u4e00\u4e9b\u70ab\u9177\u7684\u529f\u80fd\u6211\u600e\u4e48\u4e5f\u641e\u4e0d\u51fa\u6765\uff0c\u8fd8\u4ee5\u4e3a\u662f\u81ea\u5df1\u7684\u95ee\u9898\u3002\u7ed3\u679c\u662f\u8fd9\u4e2a\u4e3b\u9898\u7adf\u7136\u8fd8\u6709\u90e8\u5206\u529f\u80fd\u53ea\u6709\u6350\u8d60\u8005\u624d\u80fd\u7528... \u5728\u7535\u8111\u4e0a\u5199\u516c\u5f0f, \u771f\u7684\u5f88\u75db\u82e6\u3002","title":"\u63d2\u66f2"},{"location":"#_4","text":"\u5b66\u6210\u4e4b\u540e, \u5e0c\u671b\u80fd\u628a\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u5e94\u7528\u5230\u672c\u804c\u5de5\u4f5c\u3002 \u5927\u767d 2022/04/09 15:57:21 \u4e8e\u4e0a\u6d77\u7535\u529b\u5927\u5b66\u6559\u5e08\u516c\u5bd3","title":"\u5c55\u671b"},{"location":"machine%20learning/","text":"\u53c2\u8003 \u672c\u5957\u7b14\u8bb0\u4e3b\u8981\u53c2\u8003\u4e86\u5434\u6069\u8fbe\u8001\u5e08\u5728\u65af\u5766\u798f\u4ee5\u53ca\u7f51\u6613\u4e91\u8bfe\u5802\u7684\u4e0a\u8bfe\u89c6\u9891\uff0c\u53ca\u8bfe\u4ef6\u8d44\u6599, \u8fd8\u6709\u9ec4\u6d77\u5e7f\u535a\u58eb\u5728github\u4e0a\u7684\u8d44\u6599\u3002 \u673a\u5668\u5b66\u4e60\u4f5c\u4e1aPython\u7248\u672c \u539f\u7248\u4f5c\u4e1a\u662f MATLAB \u7248\u672c\uff08 Octave \uff09\u7684\u3002\u73b0\u5728python\u6709\u597d\u591a\u673a\u5668\u5b66\u4e60\u7684\u5e93\u66f4\u65b9\u4fbf\u56e0\u6b64\uff0c\u6211\u53c2\u8003\u4e86\u9ec4\u6d77\u5e7f\u535a\u58eb\u7684\u4ee3\u7801\uff0c\u5c06\u4f5c\u4e1a\u6539\u6210\u4e86 Python \u7248\u672c\uff0c\u63a8\u8350\u4f7f\u7528 Python 3.6 \u3002 Update \u7531\u4e8e\u8bb0\u7b14\u8bb0\u5b9e\u5728\u5f71\u54cd\u5b66\u4e60\u901f\u5ea6, \u518d\u52a0\u4e0a\u7b14\u8005\u672c\u4eba\u8868\u8fbe\u80fd\u529b\u6349\u9e21, \u6240\u4ee5\u6587\u5b57\u90e8\u5206\u627e\u4e86\u8fc7\u6765\u4eba \u6284 \u53c2\u8003 \u4e86\u4e00\u4e0b\u3002\u4ece\u7b2c\u4e09\u7ae0\u903b\u8f91\u56de\u5f52\u7684\u7b2c\u4e8c\u8282\u5047\u8bbe\u51fd\u6570\u63cf\u8ff0\u5f00\u59cb\u7684\u3002 \u53c2\u8003\u94fe\u63a5: \u5434\u6069\u8fbe(Andrew Ng)\u673a\u5668\u5b66\u4e60\u516c\u5f00\u8bfe\u4e2d\u6587\u7b14\u8bb0 \u5927\u725bgithub\u4e3b\u9875: Scruel Tao \u4e0a\u673a\u4ee3\u7801\u8fd0\u884c\u73af\u5883 OS ubuntu 18.04 Python python 3.6.9 IDE jupyter notebook","title":"\u524d\u8a00\u548c\u76ee\u5f55"},{"location":"machine%20learning/#_1","text":"\u672c\u5957\u7b14\u8bb0\u4e3b\u8981\u53c2\u8003\u4e86\u5434\u6069\u8fbe\u8001\u5e08\u5728\u65af\u5766\u798f\u4ee5\u53ca\u7f51\u6613\u4e91\u8bfe\u5802\u7684\u4e0a\u8bfe\u89c6\u9891\uff0c\u53ca\u8bfe\u4ef6\u8d44\u6599, \u8fd8\u6709\u9ec4\u6d77\u5e7f\u535a\u58eb\u5728github\u4e0a\u7684\u8d44\u6599\u3002 \u673a\u5668\u5b66\u4e60\u4f5c\u4e1aPython\u7248\u672c \u539f\u7248\u4f5c\u4e1a\u662f MATLAB \u7248\u672c\uff08 Octave \uff09\u7684\u3002\u73b0\u5728python\u6709\u597d\u591a\u673a\u5668\u5b66\u4e60\u7684\u5e93\u66f4\u65b9\u4fbf\u56e0\u6b64\uff0c\u6211\u53c2\u8003\u4e86\u9ec4\u6d77\u5e7f\u535a\u58eb\u7684\u4ee3\u7801\uff0c\u5c06\u4f5c\u4e1a\u6539\u6210\u4e86 Python \u7248\u672c\uff0c\u63a8\u8350\u4f7f\u7528 Python 3.6 \u3002","title":"\u53c2\u8003"},{"location":"machine%20learning/#update","text":"\u7531\u4e8e\u8bb0\u7b14\u8bb0\u5b9e\u5728\u5f71\u54cd\u5b66\u4e60\u901f\u5ea6, \u518d\u52a0\u4e0a\u7b14\u8005\u672c\u4eba\u8868\u8fbe\u80fd\u529b\u6349\u9e21, \u6240\u4ee5\u6587\u5b57\u90e8\u5206\u627e\u4e86\u8fc7\u6765\u4eba \u6284 \u53c2\u8003 \u4e86\u4e00\u4e0b\u3002\u4ece\u7b2c\u4e09\u7ae0\u903b\u8f91\u56de\u5f52\u7684\u7b2c\u4e8c\u8282\u5047\u8bbe\u51fd\u6570\u63cf\u8ff0\u5f00\u59cb\u7684\u3002 \u53c2\u8003\u94fe\u63a5: \u5434\u6069\u8fbe(Andrew Ng)\u673a\u5668\u5b66\u4e60\u516c\u5f00\u8bfe\u4e2d\u6587\u7b14\u8bb0 \u5927\u725bgithub\u4e3b\u9875: Scruel Tao","title":"Update"},{"location":"machine%20learning/#_2","text":"OS ubuntu 18.04 Python python 3.6.9 IDE jupyter notebook","title":"\u4e0a\u673a\u4ee3\u7801\u8fd0\u884c\u73af\u5883"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/","text":"1. \u6a21\u578b\u63cf\u8ff0 \u53c2\u8003\u89c6\u9891: 2 - 1 - Model Representation (8 min).mkv \u6211\u4eec\u8fd9\u91cc\u6709\u4e00\u4e2a\u4fc4\u52d2\u5188\u5dde\u6ce2\u7279\u5170\u5e02\u7684 \u4f4f\u623f\u4ef7\u683c\u6570\u636e\u96c6 \uff0c\u6570\u636e\u96c6\u5305\u542b\uff1a\u623f\u5c4b \u5c3a\u5bf8 \uff0c\u623f\u5c4b \u51fa\u552e\u4ef7\u683c ......\u7136\u540e\uff0c\u4f60\u6709\u4e00\u4e2a\u670b\u53cb\u3002\u4ed6\u6709\u4e00\u59571250\u5e73\u7684\u623f\u5b50\uff0c\u4ed6\u9700\u8981\u4f60\u544a\u8bc9\u4ed6\u8fd9\u623f\u5b50\u80fd\u5356\u591a\u5c11\u94b1\u3002\u6211\u4eec\u8be5\u600e\u4e48\u505a\uff1f \u90e8\u5206\u6570\u636e\u96c6\u5982\u4e0b\u8868\u6240\u793a\uff1a Size in \\(feet^2 (x)\\) Price ($) in \\(1000's (y)\\) 2104 460 1416 232 1534 315 852 178 \u2026 \u2026 \u4e0a\u8ff0\u6570\u636e\u96c6\uff0c\u6211\u4eec\u901a\u5e38\u79f0\u4e3a \u8bad\u7ec3\u96c6 \uff08 training set \uff09\u3002 \u4e3a\u4e86 \u65b9\u4fbf\u63cf\u8ff0 \u548c\u540e\u9762 \u516c\u5f0f\u63a8\u5bfc \u5c06\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u5982\u4e0b \u6807\u8bb0\uff08Notion\uff09 \uff1a \\(m\\) \u6837\u672c\u6570\u91cf\uff08number of training examples \uff09 \\(x\\) \u8f93\u5165\u53d8\u91cf/\u7279\u5f81\uff08input of variable/features\uff09 \\(y\\) \u76ee\u6807\u53d8\u91cf/\u8f93\u51fa\u53d8\u91cf\uff08output variable / target variable\uff09 \\((x, y)\\) \u4e00\u4e2a\u6837\u672c\uff08one training example\uff09 \\((x^{(i)}, y^{(i)})\\) \u7b2c \\(i\\) \u4e2a\u6837\u672c \u597d\u4e86\uff0c\u4e0b\u9762\u5148\u6765 \u660e\u786e\u4e00\u4e0b \u6211\u4eec \u5df2\u77e5\u6761\u4ef6 \u548c \u8981\u505a\u7684\u4e8b \uff1a \u6211\u4eec\u5df2\u77e5\u4e86\u8bad\u7ec3\u96c6\uff08m\u7ec4\u6570\u636e\uff1a \\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...(x^{(m)}, y^{(m)}), \u5176\u4e2d(x^{(i)}, y^{(i)})\\) \u4ee3\u8868\u4e86\u7b2c i \u7ec4\u7684\uff08\u623f\u5c4b\u5c3a\u5bf8\uff0c\u623f\u5c4b\u4ef7\u683c\uff09\uff09\uff0c\u6839\u636e\u8fd9\u4e2a\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u8981\u8bad\u7ec3\u51fa\u6211\u4eec\u7684\u6a21\u578b\uff08\u51fd\u6570\uff09\uff0c\u901a\u5e38\u8868\u793a\u4e3ah\uff0c\u5373 hypothesis(\u5047\u8bbe) \u3002\u800c\u8fd9\u4e2a\u51fd\u6570h\u7684\u8f93\u5165\u662f\u623f\u5c4b\u5c3a\u5bf8\uff0c\u8f93\u51fa\u5c31\u662f\u623f\u5c4b\u4ef7\u683c\u3002\u56e0\u6b64\uff0ch \u662f\u4e00\u4e2a\u4ecex \u5230 y \u7684\u51fd\u6570\u6620\u5c04\u3002 \u90a3\u4e48\uff0c\u5bf9\u4e8e\u6211\u4eec\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u63cf\u8ff0 \\(h\\) \uff1f \u4e00\u79cd\u53ef\u80fd\u7684\u8868\u8fbe\u65b9\u5f0f\u4e3a\uff1a $$ h_\\theta \\left( x \\right)=\\theta_{0} + \\theta_{1}x\\tag{1.2.1} $$ \u56e0\u4e3a\u53ea\u542b\u6709\u4e00\u4e2a\u7279\u5f81/\u8f93\u5165\u53d8\u91cf\uff0c\u56e0\u6b64\u8fd9\u6837\u7684\u95ee\u9898\u53eb\u4f5c\u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u3002 Note \u7ebf\u6027\u56de\u5f52\u4e2d \u7ebf\u6027 \u7684\u542b\u4e49\uff1a \u56e0\u53d8\u91cf y \u5bf9\u4e8e \u672a\u77e5\u7684\u56de\u5f52\u7cfb\u6570 \uff08 \\(\\theta_0\\) \uff0c \\(\\theta_1\\) \uff0c.... \\(\\theta_n\\) \uff09 \u662f \u7ebf\u6027 \u7684\u3002 2. \u4ee3\u4ef7\u51fd\u6570 \u53c2\u8003\u89c6\u9891: 2 - 2 - Cost Function (8 min).mkv Training Set Size in \\(feet^2 (x)\\) Price ($) in \\(1000's (y)\\) 2104 460 1416 232 1534 315 852 178 \u2026 \u2026 Hypothesis: \\(h_\\theta \\left( x \\right)=\\theta_{0}+\\theta_{1}x\\) Parameters: \\(\\theta_{0}\\) \uff0c \\(\\theta_{1}\\) \u901a\u8fc7\u4e0a\u4e00\u8282\uff0c\u6211\u4eec\u77e5\u9053\u4e86\uff0c\u6211\u4eec\u8981\u5b8c\u6210\u670b\u53cb\u7684 \u9700\u6c42\uff08\u6839\u636e\u4ed6\u623f\u5b50\u7684\u5927\u5c0f\u9884\u6d4b\u623f\u4ef7 \uff09\uff0c\u8981\u77e5\u9053\u5047\u8bbe\u51fd\u6570 \\(h\\) \uff0c\u6211\u4eec\u5bf9 \\(h\\) \u505a\u51fa\u8fd9\u6837\u4e00\u79cd\u5047\u8bbe\uff1a \\(h_\\theta \\left( x \\right)=\\theta_{0}+\\theta_{1}x\\) \u3002\u901a\u8fc7\u89c2\u5bdf\u8fd9\u4e2a\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u8fd9\u4e2a\u95ee\u9898\u8f6c\u5316\u4e3a\u6c42 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \uff0c\u4ece\u800c\u5f53\u4f60\u670b\u53cb\u628a\u623f\u5b50\u5927\u5c0f\u544a\u8bc9\u4f60\uff0c\u4f60\u5c06\u5176\u4ee3\u5165\u516c\u5f0f\u5373\u53ef\u5f97\u5230\u9884\u6d4b\u7684\u623f\u4ef7\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u5982\u4f55\u9009\u62e9\u5462 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \uff1f \u9996\u5148\uff0c\u6211\u4eec\u5148\u76f4\u89c2\u7406\u89e3 \\(h_\\theta \\left( x \\right)\\) ---\u4e0b\u56fe\u662f \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u53d6\u4e0d\u540c\u503c\u65f6\uff0c \\(h\\) \u7684\u6574\u4f53\u56fe\u50cf\u3002 Note \u4e0a\u9762\u56fe\u50cf\u4e3a\u4e86\u65b9\u4fbf\u7701\u7565\u4e86 \\(\\theta\\) \u4e0b\u6807\uff0c\u5373 \\(h_\\theta(x) = h(x)\\) \uff0c\u540e\u9762\u4e5f\u53ef\u80fd\u4f1a\u8fd9\u6837\u505a\u3002 \u7ed3\u5408\u56fe\u50cf\u63cf\u8ff0\u6211\u4eec\u7684\u4efb\u52a1 \u628a\u8bad\u7ec3\u96c6\u6570\u636e\u7ed8\u5236\u5728\u4e0b\u56fe\u4e2d\uff08\u5e76\u975e\u4e0a\u9762\u7684\u623f\u4ef7\u8bad\u7ec3\u96c6\uff0c\u4ec5\u4e3e\u4f8b\u793a\u610f\uff09 \u6211\u4eec\u8981\u505a\u7684\u5c31\u662f\u7ed8\u5236\u4e00\u6761\u76f4\u7ebf\uff08\u4e0b\u56fe\u84dd\u8272\u7684\u7ebf\uff09\u5c3d\u91cf\u5730\u4e0e\u4e0a\u9762\u90a3\u4e9b\u70b9\u6709\u548c\u597d\u7684\u62df\u5408\uff0c\u8fd9\u6761\u76f4\u7ebf\u5c31\u662f \\(h_\\theta(x)\\) \u3002\u5982\u4f55\u786e\u5b9a\u8fd9\u6761\u76f4\u7ebf\u4e5f\u5c31\u662f\u786e\u5b9a \\(\\theta_0\\) \u548c \\(\\theta_1\\) \u3002 \u6211\u4eec\u7684idea\uff1a Choose \\(\\theta_0, \\theta_1\\) so that \\(h_\\theta(x)\\) is close to \\(y\\) for our training examples \\((x, y)\\) \u5c06\u6211\u4eec\u7684\u4efb\u52a1\u4ee5\u516c\u5f0f\u5316\u6807\u8bb0\uff0c\u5373\uff1a \\[ \\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}\\ \\frac {1} {2m}\\cdot\\sum\\limits_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)^2\\tag{2.1.1} \\] \u7ebf\u6027\u56de\u5f52\u5b9e\u9645\u4e0a\u5c31\u662f\u89e3\u51b3\u4e00\u4e2a\u5173\u4e8e \\(\\theta_0, \\theta_1\\) \u7684\u6700\u5c0f\u5316\u95ee\u9898(minimize)\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u76f4\u7ebf\u4e0e\u90a3\u4e9b\u70b9\u6709\u5f88\u597d\u7684\u62df\u5408\uff0c\u90a3\u6211\u4eec\u628a\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4b\u5f97\u5230\u7684\u503c \\(\\Big(\\) \u5c06x\u4ee3\u5165\u5047\u8bbe\u51fd\u6570\u53ef\u5f97\u5230\uff0c\u5373 \\(h_\\theta(x)\\Big)\\) \u4e0e\u771f\u5b9e\u503cy\u6c42\u4e00\u4e2a\u5dee\u7684\u5e73\u65b9\u3002\u518d\u628a\u8fd9\u4e9b\u5e73\u65b9\u7d2f\u52a0\u3002\u5373: \\[ \\sum\\limits_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)^2\\tag{2.1.2} \\] \u6211\u4eec\u53ea\u8981\u8ba9\u8fd9\u4e2a\u5e73\u65b9\u548c\u6700\u5c0f\u5373\u53ef\u3002 \u6ce8\u610f, \u5173\u4e8e\u5f0f(2.1.1)\u524d\u9762 \\(\\frac{1}{2m}\\) \\(\\frac{1}{m}\\) \u662f\u56e0\u4e3a\u6c42\u548c\u9879\u6709m\u4e2a\uff0c\u8fd9\u91cc\u9664\u4ee5m\uff0c\u662f\u6c42\u5e73\u5747\u503c\u3002 \\(\\frac{1}{2}\\) \u4e3a\u4e86\u540e\u9762\u6c42\u5bfc\u6d88\u53bb \u5176\u5b9e\u8fd9\u4e2a \\(\\frac{1}{2m}\\) \u5bf9\u6211\u4eec\u6c42\u6700\u5c0f\u503c\u6ca1\u6709\u4efb\u4f55\u5f71\u54cd\uff0c\u53ea\u662f\u4e3a\u4e86\u8ba1\u7b97\u65b9\u4fbf\u8fd9\u4e48\u5199\u3002 \u5173\u4e8e\u5f0f(2.1.2)\u7684\u51e0\u4f55\u610f\u4e49 \u6240\u6709\u6570\u636e\u70b9\u4e0e\u62df\u5408\u76f4\u7ebf\u5728y\u8f74\u65b9\u5411\u7684\u622a\u8ddd\u7684\u5e73\u65b9\u548c \u5728\u672c\u4f8b\u5b50\u4e2d\uff0c \\(h_\\theta \\left( x \\right)=\\theta_{0} + \\theta_{1}x\\) \u901a\u5e38\uff0c\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u4ee3\u4ef7\u51fd\u6570(cost function)\uff0c\u5982\u4e0b\uff1a \\[ J(\\theta_0,\\theta_1)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\\tag{2.1.3} \\] \u6211\u4eec\u7684\u6700\u7ec8\u76ee\u6807\u5c31\u8f6c\u5316\u4e3a\uff1a \\[ \\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}J(\\theta_0,\\theta_1) \\] \u8fd9\u91cc\u6211\u4eec\u5b9a\u4e49\u7684\u5f0f(2.1.3)\u635f\u5931\u51fd\u6570, \u4e5f\u88ab\u79f0\u4e3a\u5e73\u65b9\u8bef\u5dee\u51fd\u6570(squared error function)\u3002\u7136\u800c\uff0c\u635f\u5931\u51fd\u6570\u4e0d\u6b62\u8fd9\u4e00\u79cd\u5f62\u5f0f\uff0c\u4f46\u662f\u5728\u56de\u5f52\u95ee\u9898\u4e2d\uff0c\u5e73\u65b9\u8bef\u5dee\u51fd\u6570\u90fd\u662f\u6bd4\u8f83\u5408\u7406\u548c\u6bd4\u8f83\u5e38\u7528\u7684\u9009\u62e9\u3002 3 \u4ee3\u4ef7\u51fd\u6570\u76f4\u89c2\u7406\u89e31 \u53c2\u8003\u89c6\u9891: 2 - 3 - Cost Function - Intuition I (11 min).mkv \u5728\u4e0a\u4e00\u4e2a\u5c0f\u8282\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u4ee3\u4ef7\u51fd\u6570\u7684\u5b9a\u4e49\u3002\u5728\u8fd9\u4e00\u5c0f\u8282\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u83b7\u53d6\u4e00\u4e9b\u76f4\u89c2\u7684\u611f\u53d7\uff0c\u770b\u770b\u4ee3\u4ef7\u51fd\u6570\u5230\u5e95\u662f\u5728\u5e72\u4ec0\u4e48\u3002 Hypothesis: \\(h_\\theta(x)=\\theta_0+\\theta_1x\\) Parameters: \\(\\theta_0,\\ \\theta_1\\) Cost Function: \\(J(\\theta_0,\\theta_1)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\\) Goal: \\(\\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}J(\\theta_0,\\theta_1)\\) \u4e3a\u4e86\u8ba9\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6709\u66f4\u597d\u7684\uff0c\u53ef\u89c6\u5316\u6548\u679c\uff0c\u6211\u4eec\u7b80\u5316\u5047\u8bbe\u51fd\u6570\u4e3a \\(h_\\theta(x)=\\theta_1x\\) \uff0c\u5373\u5047\u8bbe \\(\\theta_0=0\\) \u3002\u7b80\u5316\u540e\uff1a \\(h_\\theta(x)=\\theta_1x\\) \\(J(\\theta_1)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\\) Goal: \\(\\mathop{minimize} \\limits_{\\theta_1}J(\\theta_1)\\) \u5728\u786e\u5b9a\u597d\u4e0d\u540c\u7684 \\(\\theta_1\\) ,\u4e4b\u540e\u753b\u51fa\u5047\u8bbe\u51fd\u6570 \\(h\\) \u548c\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u3002\u672c\u4f8b\u4e2d\uff0c\u5047\u5b9a\u8bad\u7ec3\u96c6\u4e3a \\((1,1), (2,2), (3,3)\\) \u3002 \u5f53 \\(\\theta_1=1\\) \u65f6\uff0c \u6ce8\u610f\u5230\uff0c\u56e0\u4e3a \\(\\theta_1=1\\) \uff0c\u6b64\u65f6 \\(h_\\theta=y\\) ,\u6240\u4ee5 \\(J=0\\) \u3002\u5e76\u5728\u53f3\u8fb9\u56fe\u4e0a\u753b\u51fa\u7b2c\u4e00\u4e2a\u70b9(1,0)\u3002 \u5f53 \\(\\theta_1=0.5\\) \u65f6\uff0c \\((x,y)\\) \u4f9d\u6b21\u53d6(1, 0.5), (2, 1), (3, 1.5) \u8ba1\u7b97 \\(J \\approx 0.58\\) \u3002\u5e76\u5728\u53f3\u8fb9\u56fe\u4e0a\u753b\u51fa\u7b2c\u4e8c\u4e2a\u70b9(0.5,0.58)\u3002 \u5f53 \\(\\theta_1=0\\) \u65f6\uff0c \u540c\u7406\uff0c\u5f53 \\(\\theta_1\\) \u53d6\u4e0d\u540c\u503c\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa \\(J\\) \uff0c\u5e76\u5728\u53f3\u4fa7\u753b\u51fa \\(J\\) \u7684\u5927\u81f4\u56fe\u50cf\u3002 \u603b\u7ed3\uff1a \u6211\u4eec\u901a\u8fc7\u53d6\u4e0d\u540c\u7684 \\(\\theta_1\\) \uff0c\u7ed8\u5236\u51fa\u4e86 \\(J\\) \u3002 So for each value of \\(\\theta_1\\) , we wound up with a diffent value of \\(J(\\theta_1)\\) . And we colud then use this to trace out this plot on the right. Now you remember the optimization objective for our learning algorithm is we want to choose the value of \\(\\theta_1\\) that minimize \\(J(\\theta_1)\\) . This was our objective function for the linear regression. \u73b0\u5728\uff0c\u6211\u4eec\u89c2\u5bdf\u53f3\u4fa7\u90a3\u6761\u66f2\u7ebf\uff0c\u4f1a\u53d1\u73b0\uff0c\u5f53 \\(\\theta_1=1\\) \u65f6\uff0c \\(J(\\theta_1)\\) \u6700\u5c0f\u3002\u518d\u89c2\u5bdf\u5de6\u8fb9\u7684\u62df\u5408\u60c5\u51b5\u4f1a\u53d1\u73b0\uff0c\u8fd9\u786e\u5b9e\u662f\u6700\u597d\u7684\u60c5\u51b5\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u7279\u6b8a\u7684\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u786e\u5b9e\u5b8c\u7f8e\u5730\u62df\u5408\u4e86\u5b83\u3002 And that's why minimizing \\(J(\\theta_1)\\) corresponds to finding a straight line that fits the data well. 4. \u4ee3\u4ef7\u51fd\u6570\u7684\u76f4\u89c2\u7406\u89e3II \u53c2\u8003\u89c6\u9891: 2 - 4 - Cost Function - Intuition II (9 min).mkv \u4ee3\u4ef7\u51fd\u6570\u7684\u6837\u5b50\uff0c\u5219\u53ef\u4ee5\u770b\u51fa\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u5b58\u5728\u4e00\u4e2a\u4f7f\u5f97 \\(J(\\theta_{0}, \\theta_{1})\\) \u6700\u5c0f\u7684\u70b9\u3002 \u901a\u8fc7\u8fd9\u4e9b\u56fe\u5f62\uff0c\u6211\u5e0c\u671b\u4f60\u80fd\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6240\u8868\u8fbe\u7684\u503c\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u5b83\u4eec\u5bf9\u5e94\u7684\u5047\u8bbe\u51fd\u6570\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u4ee5\u53ca\u4ec0\u4e48\u6837\u7684\u5047\u8bbe\u5bf9\u5e94\u7684\u70b9\uff0c\u66f4\u63a5\u8fd1\u4e8e\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u7684\u6700\u5c0f\u503c\u3002 \u5f53\u7136\uff0c\u6211\u4eec\u771f\u6b63\u9700\u8981\u7684\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u5730\u627e\u51fa\u8fd9\u4e9b\u4f7f\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u53d6\u6700\u5c0f\u503c\u7684\u53c2\u6570 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u6765\u3002 \u6211\u4eec\u4e5f\u4e0d\u5e0c\u671b\u7f16\u4e2a\u7a0b\u5e8f\u628a\u8fd9\u4e9b\u70b9\u753b\u51fa\u6765\uff0c\u7136\u540e\u4eba\u5de5\u7684\u65b9\u6cd5\u6765\u8bfb\u51fa\u8fd9\u4e9b\u70b9\u7684\u6570\u503c\uff0c\u8fd9\u5f88\u660e\u663e\u4e0d\u662f\u4e00\u4e2a\u597d\u529e\u6cd5\u3002\u6211\u4eec\u4f1a\u9047\u5230\u66f4\u590d\u6742\u3001\u66f4\u9ad8\u7ef4\u5ea6\u3001\u66f4\u591a\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u800c\u8fd9\u4e9b\u60c5\u51b5\u662f\u5f88\u96be\u753b\u51fa\u56fe\u7684\uff0c\u56e0\u6b64\u66f4\u65e0\u6cd5\u5c06\u5176\u53ef\u89c6\u5316\uff0c\u56e0\u6b64\u6211\u4eec\u771f\u6b63\u9700\u8981\u7684\u662f\u7f16\u5199\u7a0b\u5e8f\u6765\u627e\u51fa\u8fd9\u4e9b\u6700\u5c0f\u5316\u4ee3\u4ef7\u51fd\u6570\u7684 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u7684\u503c\uff0c\u5728\u4e0b\u4e00\u8282\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u4e00\u79cd\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u5730\u627e\u51fa\u80fd\u4f7f\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6700\u5c0f\u5316\u7684\u53c2\u6570 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u7684\u503c\u3002 5. \u68af\u5ea6\u4e0b\u964d \u53c2\u8003\u89c6\u9891: 2 - 5 - Gradient Descent (11 min).mkv \u68af\u5ea6\u4e0b\u964d\u662f\u4e00\u4e2a\u7528\u6765\u6c42\u51fd\u6570\u6700\u5c0f\u503c\u7684\u7b97\u6cd5\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u6c42\u51fa\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_{0}, \\theta_{1})\\) \u7684\u6700\u5c0f\u503c\u3002 \u68af\u5ea6\u4e0b\u964d\u80cc\u540e\u7684\u601d\u60f3\u662f\uff1a\u5f00\u59cb\u65f6\u6211\u4eec\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u53c2\u6570\u7684\u7ec4\u5408 \\(\\left( {\\theta_{0}},{\\theta_{1}},......,{\\theta_{n}} \\right)\\) \uff0c\u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570\uff0c\u7136\u540e\u6211\u4eec\u5bfb\u627e\u4e0b\u4e00\u4e2a\u80fd\u8ba9\u4ee3\u4ef7\u51fd\u6570\u503c\u4e0b\u964d\u6700\u591a\u7684\u53c2\u6570\u7ec4\u5408\u3002\u6211\u4eec\u6301\u7eed\u8fd9\u4e48\u505a\u76f4\u5230\u627e\u5230\u4e00\u4e2a\u5c40\u90e8\u6700\u5c0f\u503c\uff08 local minimum \uff09\uff0c\u56e0\u4e3a\u6211\u4eec\u5e76\u6ca1\u6709\u5c1d\u8bd5\u5b8c\u6240\u6709\u7684\u53c2\u6570\u7ec4\u5408\uff0c\u6240\u4ee5\u4e0d\u80fd\u786e\u5b9a\u6211\u4eec\u5f97\u5230\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u662f\u5426\u4fbf\u662f\u5168\u5c40\u6700\u5c0f\u503c\uff08 global minimum \uff09\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u521d\u59cb\u53c2\u6570\u7ec4\u5408\uff0c\u53ef\u80fd\u4f1a\u627e\u5230\u4e0d\u540c\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002 \u60f3\u8c61\u4e00\u4e0b\u4f60\u6b63\u7ad9\u7acb\u5728\u5c71\u7684\u8fd9\u4e00\u70b9\u4e0a\uff0c\u7ad9\u7acb\u5728\u4f60\u60f3\u8c61\u7684\u516c\u56ed\u8fd9\u5ea7\u7ea2\u8272\u5c71\u4e0a\uff0c\u5728\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u8981\u505a\u7684\u5c31\u662f\u65cb\u8f6c360\u5ea6\uff0c\u770b\u770b\u6211\u4eec\u7684\u5468\u56f4\uff0c\u5e76\u95ee\u81ea\u5df1\u8981\u5728\u67d0\u4e2a\u65b9\u5411\u4e0a\uff0c\u7528\u5c0f\u788e\u6b65\u5c3d\u5feb\u4e0b\u5c71\u3002\u8fd9\u4e9b\u5c0f\u788e\u6b65\u9700\u8981\u671d\u4ec0\u4e48\u65b9\u5411\uff1f\u5982\u679c\u6211\u4eec\u7ad9\u5728\u5c71\u5761\u4e0a\u7684\u8fd9\u4e00\u70b9\uff0c\u4f60\u770b\u4e00\u4e0b\u5468\u56f4\uff0c\u4f60\u4f1a\u53d1\u73b0\u6700\u4f73\u7684\u4e0b\u5c71\u65b9\u5411\uff0c\u4f60\u518d\u770b\u770b\u5468\u56f4\uff0c\u7136\u540e\u518d\u4e00\u6b21\u60f3\u60f3\uff0c\u6211\u5e94\u8be5\u4ece\u4ec0\u4e48\u65b9\u5411\u8fc8\u7740\u5c0f\u788e\u6b65\u4e0b\u5c71\uff1f\u7136\u540e\u4f60\u6309\u7167\u81ea\u5df1\u7684\u5224\u65ad\u53c8\u8fc8\u51fa\u4e00\u6b65\uff0c\u91cd\u590d\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u4ece\u8fd9\u4e2a\u65b0\u7684\u70b9\uff0c\u4f60\u73af\u987e\u56db\u5468\uff0c\u5e76\u51b3\u5b9a\u4ece\u4ec0\u4e48\u65b9\u5411\u5c06\u4f1a\u6700\u5feb\u4e0b\u5c71\uff0c\u7136\u540e\u53c8\u8fc8\u8fdb\u4e86\u4e00\u5c0f\u6b65\uff0c\u5e76\u4f9d\u6b64\u7c7b\u63a8\uff0c\u76f4\u5230\u4f60\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u70b9\u7684\u4f4d\u7f6e\u3002 \u6279\u91cf\u68af\u5ea6\u4e0b\u964d\uff08 batch gradient descent \uff09\u7b97\u6cd5\u7684\u516c\u5f0f\u4e3a\uff1a repeat until convergence { $$ \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1) $$ } \u5176\u4e2d \\(\\alpha\\) \u662f\u5b66\u4e60\u7387\uff08 learning rate \uff09\uff0c\u5b83\u51b3\u5b9a\u4e86\u6211\u4eec\u6cbf\u7740\u80fd\u8ba9\u4ee3\u4ef7\u51fd\u6570\u4e0b\u964d\u7a0b\u5ea6\u6700\u5927\u7684\u65b9\u5411\u5411\u4e0b\u8fc8\u51fa\u7684\u6b65\u5b50\u6709\u591a\u5927\uff0c\u5728\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u4e2d\uff0c\u6211\u4eec\u6bcf\u4e00\u6b21\u90fd\u540c\u65f6\u8ba9\u6240\u6709\u7684\u53c2\u6570\u51cf\u53bb\u5b66\u4e60\u901f\u7387\u4e58\u4ee5\u4ee3\u4ef7\u51fd\u6570\u7684\u5bfc\u6570\u3002 \u5728\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e2d\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u5fae\u5999\u7684\u95ee\u9898\uff0c\u68af\u5ea6\u4e0b\u964d\u4e2d\uff0c\u6211\u4eec\u8981\u66f4\u65b0 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \uff0c\u5f53 \\(j=0\\) \u548c \\(j=1\\) \u65f6\uff0c\u4f1a\u4ea7\u751f\u66f4\u65b0\uff0c\u6240\u4ee5\u4f60\u5c06\u66f4\u65b0 \\(J\\left( {\\theta_{0}} \\right)\\) \u548c \\(J\\left( {\\theta_{1}} \\right)\\) \u3002\u5b9e\u73b0\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u5fae\u5999\u4e4b\u5904\u662f\uff0c\u5728\u8fd9\u4e2a\u8868\u8fbe\u5f0f\u4e2d\uff0c\u5982\u679c\u4f60\u8981\u66f4\u65b0\u8fd9\u4e2a\u7b49\u5f0f\uff0c\u4f60\u9700\u8981\u540c\u65f6\u66f4\u65b0 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \uff0c\u6211\u7684\u610f\u601d\u662f\u5728\u8fd9\u4e2a\u7b49\u5f0f\u4e2d\uff0c\u6211\u4eec\u8981\u8fd9\u6837\u66f4\u65b0\uff1a \\({\\theta_{0}} := {\\theta_{0}}\\) \uff0c\u5e76\u66f4\u65b0 \\({\\theta_{1}}:= {\\theta_{1}}\\) \u3002 \u5b9e\u73b0\u65b9\u6cd5\u662f\uff1a\u4f60\u5e94\u8be5\u8ba1\u7b97\u516c\u5f0f\u53f3\u8fb9\u7684\u90e8\u5206\uff0c\u901a\u8fc7\u90a3\u4e00\u90e8\u5206\u8ba1\u7b97\u51fa \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \u7684\u503c\uff0c\u7136\u540e\u540c\u65f6\u66f4\u65b0 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \u3002 \u8ba9\u6211\u8fdb\u4e00\u6b65\u9610\u8ff0\u8fd9\u4e2a\u8fc7\u7a0b\uff1a \u5728\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e2d\uff0c\u8fd9\u662f\u6b63\u786e\u5b9e\u73b0\u540c\u65f6\u66f4\u65b0\u7684\u65b9\u6cd5\u3002\u6211\u4e0d\u6253\u7b97\u89e3\u91ca\u4e3a\u4ec0\u4e48\u4f60\u9700\u8981\u540c\u65f6\u66f4\u65b0\uff0c\u540c\u65f6\u66f4\u65b0\u662f\u68af\u5ea6\u4e0b\u964d\u4e2d\u7684\u4e00\u79cd\u5e38\u7528\u65b9\u6cd5\u3002\u6211\u4eec\u4e4b\u540e\u4f1a\u8bb2\u5230\uff0c\u540c\u6b65\u66f4\u65b0\u662f\u66f4\u81ea\u7136\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002\u5f53\u4eba\u4eec\u8c08\u5230\u68af\u5ea6\u4e0b\u964d\u65f6\uff0c\u4ed6\u4eec\u7684\u610f\u601d\u5c31\u662f\u540c\u6b65\u66f4\u65b0\u3002\u6ce8\u610f\u4e0a\u9762\u56fe\u5de6\u4e0b\u65b9\u548c\u53f3\u4e0b\u65b9\u7684\u533a\u522b\u3002 \u6ce8\u610f\uff1a \u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u662f\u9700\u8981\u540c\u65f6\u66f4\u65b0 \\({\\theta_{0}}\\) , \\({\\theta_{1}}\\) ,..., \\({\\theta_{n}}\\) \u7684\u3002\u672c\u4f8b\u4e2d\uff0c\u53ea\u6709 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \u3002 \u4e0a\u9762\u56fe\u4e2d\u7528\u4e86 := \u66f4\u65b0 \\(\\theta\\) , \u8fd9\u91cc := \u7b49\u540c\u4e8e\u8ba1\u7b97\u673a\u8bed\u8a00\u4e2d\u7684\u8d4b\u503c\u64cd\u4f5c\u3002 \u5728\u63a5\u4e0b\u6765\u7684\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u8981\u8fdb\u5165\u8fd9\u4e2a\u5fae\u5206\u9879\u7684\u7ec6\u8282\u4e4b\u4e2d\u3002\u6211\u5df2\u7ecf\u5199\u4e86\u51fa\u6765\u4f46\u6ca1\u6709\u771f\u6b63\u5b9a\u4e49\uff0c\u5982\u679c\u4f60\u5df2\u7ecf\u4fee\u8fc7\u5fae\u79ef\u5206\u8bfe\u7a0b\uff0c\u5982\u679c\u4f60\u719f\u6089\u504f\u5bfc\u6570\u548c\u5bfc\u6570\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f\u8fd9\u4e2a\u5fae\u5206\u9879\uff1a \\(\\alpha \\frac{\\partial }{\\partial {{\\theta }_{0}}}J({{\\theta }_{0}},{{\\theta }_{1}})\\) \uff0c \\(\\alpha \\frac{\\partial }{\\partial {{\\theta }_{1}}}J({{\\theta }_{0}},{{\\theta }_{1}})\\) \u3002 6. \u68af\u5ea6\u4e0b\u964d\u7684\u76f4\u89c2\u7406\u89e3 \u53c2\u8003\u89c6\u9891: 2 - 6 - Gradient Descent Intuition (12 min).mkv \u5728\u4e4b\u524d\u7684\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u7ed9\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u4e0a\u5173\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5b9a\u4e49\uff0c\u672c\u6b21\u89c6\u9891\u6211\u4eec\u66f4\u6df1\u5165\u7814\u7a76\u4e00\u4e0b\uff0c\u66f4\u76f4\u89c2\u5730\u611f\u53d7\u4e00\u4e0b\u8fd9\u4e2a\u7b97\u6cd5\u662f\u505a\u4ec0\u4e48\u7684\uff0c\u4ee5\u53ca\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u66f4\u65b0\u8fc7\u7a0b\u6709\u4ec0\u4e48\u610f\u4e49\u3002\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u5982\u4e0b\uff1a \\({\\theta_{j}}:={\\theta_{j}}-\\alpha \\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)\\) \u63cf\u8ff0\uff1a\u5bf9 \\(\\theta\\) \u8d4b\u503c\uff0c\u4f7f\u5f97 \\(J\\left( \\theta \\right)\\) \u6309\u68af\u5ea6\u4e0b\u964d\u6700\u5feb\u65b9\u5411\u8fdb\u884c\uff0c\u4e00\u76f4\u8fed\u4ee3\u4e0b\u53bb\uff0c\u6700\u7ec8\u5f97\u5230\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u5176\u4e2d \\(\\alpha\\) \u662f\u5b66\u4e60\u7387\uff08 learning rate \uff09\uff0c\u5b83\u51b3\u5b9a\u4e86\u6211\u4eec\u6cbf\u7740\u80fd\u8ba9\u4ee3\u4ef7\u51fd\u6570\u4e0b\u964d\u7a0b\u5ea6\u6700\u5927\u7684\u65b9\u5411\u5411\u4e0b\u8fc8\u51fa\u7684\u6b65\u5b50\u6709\u591a\u5927\u3002 \u4ece\u4e0a\u56fe\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u65e0\u8bba \\(\\theta\\) \u8d77\u59cb\u70b9\u4ece\u6b63\u7684\u8fd8\u662f\u8d1f\u7684\u5f00\u59cb\u51fa\u53d1\uff0c\u4ed6\u4eec\u90fd\u4f1a\u5411\u7740 \\(J(\\theta)\\) \u53d8\u5c0f\u7684\u65b9\u5411\u51fa\u53d1\u3002 \u8ba9\u6211\u4eec\u6765\u770b\u770b\u5982\u679c \\(\\alpha\\) \u592a\u5c0f\u6216 \\(\\alpha\\) \u592a\u5927\u4f1a\u51fa\u73b0\u4ec0\u4e48\u60c5\u51b5\uff1a \u5982\u679c \\(\\alpha\\) \u592a\u5c0f\u4e86\uff0c\u5373\u6211\u7684\u5b66\u4e60\u901f\u7387\u592a\u5c0f\uff0c\u7ed3\u679c\u5c31\u662f\u53ea\u80fd\u8fd9\u6837\u50cf\u5c0f\u5b9d\u5b9d\u4e00\u6837\u4e00\u70b9\u70b9\u5730\u632a\u52a8\uff0c\u53bb\u52aa\u529b\u63a5\u8fd1\u6700\u4f4e\u70b9\uff0c\u8fd9\u6837\u5c31\u9700\u8981\u5f88\u591a\u6b65\u624d\u80fd\u5230\u8fbe\u6700\u4f4e\u70b9\uff0c\u6240\u4ee5\u5982\u679c \\(\\alpha\\) \u592a\u5c0f\u7684\u8bdd\uff0c\u53ef\u80fd\u4f1a\u5f88\u6162\uff0c\u56e0\u4e3a\u5b83\u4f1a\u4e00\u70b9\u70b9\u632a\u52a8\uff0c\u5b83\u4f1a\u9700\u8981\u5f88\u591a\u6b65\u624d\u80fd\u5230\u8fbe\u5168\u5c40\u6700\u4f4e\u70b9\u3002 \u5982\u679c \\(\\alpha\\) \u592a\u5927\uff0c\u90a3\u4e48\u68af\u5ea6\u4e0b\u964d\u6cd5\u53ef\u80fd\u4f1a\u8d8a\u8fc7\u6700\u4f4e\u70b9\uff0c\u751a\u81f3\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\uff0c\u4e0b\u4e00\u6b21\u8fed\u4ee3\u53c8\u79fb\u52a8\u4e86\u4e00\u5927\u6b65\uff0c\u8d8a\u8fc7\u4e00\u6b21\uff0c\u53c8\u8d8a\u8fc7\u4e00\u6b21\uff0c\u4e00\u6b21\u6b21\u8d8a\u8fc7\u6700\u4f4e\u70b9\uff0c\u76f4\u5230\u4f60\u53d1\u73b0\u5b9e\u9645\u4e0a\u79bb\u6700\u4f4e\u70b9\u8d8a\u6765\u8d8a\u8fdc\uff0c\u6240\u4ee5\uff0c\u5982\u679c \\(\\alpha\\) \u592a\u5927\uff0c\u5b83\u4f1a\u5bfc\u81f4\u65e0\u6cd5\u6536\u655b\uff0c\u751a\u81f3\u53d1\u6563\u3002 \u5047\u8bbe\u4f60\u5c06 \\({\\theta_{1}}\\) \u521d\u59cb\u5316\u5728\u5c40\u90e8\u6700\u4f4e\u70b9\u3002\u7ed3\u679c\u662f\u5c40\u90e8\u6700\u4f18\u70b9\u7684\u5bfc\u6570\u5c06\u7b49\u4e8e\u96f6\uff0c\u56e0\u4e3a\u5b83\u662f\u90a3\u6761\u5207\u7ebf\u7684\u659c\u7387\u3002\u8fd9\u610f\u5473\u7740\u4f60\u5df2\u7ecf\u5728\u5c40\u90e8\u6700\u4f18\u70b9\uff0c\u5b83\u4f7f\u5f97 \\({\\theta_{1}}\\) \u4e0d\u518d\u6539\u53d8\uff0c\u4e5f\u5c31\u662f\u65b0\u7684 \\({\\theta_{1}}\\) \u7b49\u4e8e\u539f\u6765\u7684 \\({\\theta_{1}}\\) \uff0c\u56e0\u6b64\uff0c\u5982\u679c\u4f60\u7684\u53c2\u6570\u5df2\u7ecf\u5904\u4e8e\u5c40\u90e8\u6700\u4f4e\u70b9\uff0c\u90a3\u4e48\u68af\u5ea6\u4e0b\u964d\u6cd5\u66f4\u65b0\u5176\u5b9e\u4ec0\u4e48\u90fd\u6ca1\u505a\uff0c\u5b83\u4e0d\u4f1a\u6539\u53d8\u53c2\u6570\u7684\u503c\u3002 \u6211\u4eec\u6765\u770b\u4e00\u4e2a\u4f8b\u5b50\uff0c\u8fd9\u662f\u4ee3\u4ef7\u51fd\u6570 \\(J\\left( \\theta \\right)\\) \u3002 \u6211\u60f3\u627e\u5230\u5b83\u7684\u6700\u5c0f\u503c\uff0c\u9996\u5148\u521d\u59cb\u5316\u6211\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u5728\u90a3\u4e2a\u54c1\u7ea2\u8272\u7684\u70b9\u521d\u59cb\u5316\uff0c\u5982\u679c\u6211\u66f4\u65b0\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\uff0c\u4e5f\u8bb8\u5b83\u4f1a\u5e26\u6211\u5230\u8fd9\u4e2a\u70b9\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u70b9\u7684\u5bfc\u6570\u662f\u76f8\u5f53\u9661\u7684\u3002\u73b0\u5728\uff0c\u5728\u8fd9\u4e2a\u7eff\u8272\u7684\u70b9\uff0c\u5982\u679c\u6211\u518d\u66f4\u65b0\u4e00\u6b65\uff0c\u4f60\u4f1a\u53d1\u73b0\u6211\u7684\u5bfc\u6570\uff0c\u4e5f\u5373\u659c\u7387\uff0c\u662f\u6ca1\u90a3\u4e48\u9661\u7684\u3002\u968f\u7740\u6211\u63a5\u8fd1\u6700\u4f4e\u70b9\uff0c\u6211\u7684\u5bfc\u6570\u8d8a\u6765\u8d8a\u63a5\u8fd1\u96f6\uff0c\u6240\u4ee5\uff0c\u68af\u5ea6\u4e0b\u964d\u4e00\u6b65\u540e\uff0c\u65b0\u7684\u5bfc\u6570\u4f1a\u53d8\u5c0f\u4e00\u70b9\u70b9\u3002\u7136\u540e\u6211\u60f3\u518d\u68af\u5ea6\u4e0b\u964d\u4e00\u6b65\uff0c\u5728\u8fd9\u4e2a\u7eff\u70b9\uff0c\u6211\u81ea\u7136\u4f1a\u7528\u4e00\u4e2a\u7a0d\u5fae\u8ddf\u521a\u624d\u5728\u90a3\u4e2a\u54c1\u7ea2\u70b9\u65f6\u6bd4\uff0c\u518d\u5c0f\u4e00\u70b9\u7684\u4e00\u6b65\uff0c\u5230\u4e86\u65b0\u7684\u7ea2\u8272\u70b9\uff0c\u66f4\u63a5\u8fd1\u5168\u5c40\u6700\u4f4e\u70b9\u4e86\uff0c\u56e0\u6b64\u8fd9\u70b9\u7684\u5bfc\u6570\u4f1a\u6bd4\u5728\u7eff\u70b9\u65f6\u66f4\u5c0f\u3002\u6240\u4ee5\uff0c\u6211\u518d\u8fdb\u884c\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\u65f6\uff0c\u6211\u7684\u5bfc\u6570\u9879\u662f\u66f4\u5c0f\u7684\uff0c \\({\\theta_{1}}\\) \u66f4\u65b0\u7684\u5e45\u5ea6\u5c31\u4f1a\u66f4\u5c0f\u3002\u6240\u4ee5 \u968f\u7740\u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u8fd0\u884c\uff0c\u4f60\u79fb\u52a8\u7684\u5e45\u5ea6\u4f1a\u81ea\u52a8\u53d8\u5f97\u8d8a\u6765\u8d8a\u5c0f\uff0c\u76f4\u5230\u6700\u7ec8\u79fb\u52a8\u5e45\u5ea6\u975e\u5e38\u5c0f\uff0c\u4f60\u4f1a\u53d1\u73b0\u5df2\u7ecf\u6536\u655b\u5230\u5c40\u90e8\u6781\u5c0f\u503c \u3002 \u56de\u987e\u4e00\u4e0b\uff0c\u5728\u68af\u5ea6\u4e0b\u964d\u6cd5\u4e2d\uff0c\u5f53\u6211\u4eec\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u70b9\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u4f1a\u81ea\u52a8\u91c7\u53d6\u66f4\u5c0f\u7684\u5e45\u5ea6\uff0c\u8fd9\u662f\u56e0\u4e3a\u5f53\u6211\u4eec\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u70b9\u65f6\uff0c\u5f88\u663e\u7136\u5728\u5c40\u90e8\u6700\u4f4e\u65f6\u5bfc\u6570\u7b49\u4e8e\u96f6\uff0c\u6240\u4ee5\u5f53\u6211\u4eec\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u65f6\uff0c\u5bfc\u6570\u503c\u4f1a\u81ea\u52a8\u53d8\u5f97\u8d8a\u6765\u8d8a\u5c0f\uff0c\u6240\u4ee5\u68af\u5ea6\u4e0b\u964d\u5c06\u81ea\u52a8\u91c7\u53d6\u8f83\u5c0f\u7684\u5e45\u5ea6\uff0c\u8fd9\u5c31\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u505a\u6cd5\u3002\u6240\u4ee5\u5b9e\u9645\u4e0a\u6ca1\u6709\u5fc5\u8981\u518d\u53e6\u5916\u51cf\u5c0f \\(a\\) \u3002 \u8fd9\u5c31\u662f\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u7528\u5b83\u6765\u6700\u5c0f\u5316\u4efb\u4f55\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \uff0c\u4e0d\u53ea\u662f\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u3002 7. \u68af\u5ea6\u4e0b\u964d\u5e94\u7528\u81f3\u7ebf\u6027\u56de\u5f52 \u53c2\u8003\u89c6\u9891: 2 - 7 - GradientDescentForLinearRegression (6 min).mkv \u8fd9\u662f\u6211\u4eec\u4e4b\u524d\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u73b0\u5728\u5c1d\u8bd5\u5c06\u68af\u5ea6\u4e0b\u964d\u6cd5\u5e94\u7528\u5230\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002 \u6211\u4eec\u7684Goal\uff1a \\(\\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}J(\\theta_0,\\theta_1)\\) \u4ed4\u7ec6\u89c2\u5bdf\uff0c\u6211\u4eec\u4f1a\u53d1\u73b0\u89e3\u51b3\u8fd9\u4e2a\u76ee\u6807\u7684\u5173\u952e\u5c31\u662f\u8fd9\u4e2a\u5bfc\u6570\u9879 \\(\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)\\) \u3002 \\(\\frac{\\partial }{\\partial {{\\theta }_{j}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{\\partial }{\\partial {{\\theta }_{j}}}\\frac{1}{2m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}^{2}}\\) \u600e\u4e48\u4ece\u4e0a\u5f0f\u53d8\u5230\u4e0b\u9762\u4e24\u4e2a\u5f0f\u5b50\uff0c\u8bf7\u770b\u4e0b\u4e00\u7ae0\u8282\uff0c\u6709\u5177\u4f53\u63a8\u5bfc\u3002 \\(j=0\\) \u65f6\uff1a \\(\\frac{\\partial }{\\partial {{\\theta }_{0}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}}\\) \\(j=1\\) \u65f6\uff1a \\(\\frac{\\partial }{\\partial {{\\theta }_{1}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}\\) \u5219\u7b97\u6cd5\u6539\u5199\u6210\uff1a Repeat { \u200b \\({\\theta_{0}}:={\\theta_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{ \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}\\) \u200b \\({\\theta_{1}}:={\\theta_{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}\\) \u200b } \u91cd\u8981\u7ed3\u8bba\uff1a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u91cc\u7684\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u603b\u662f\u5f13\u72b6\u51fd\u6570\uff0c\u53c8\u79f0\u51f8\u51fd\u6570( convex function )\u3002\u5982\u4e0b\u56fe\uff1a \u8fd9\u4e2a\u51fd\u6570\u6ca1\u6709\u5c40\u90e8\u6700\u4f18(local optima)\uff0c\u53ea\u6709\u5168\u5c40\u6700\u4f18(global optimum)\u3002\u5f53\u6211\u4eec\u7528\u68af\u5ea6\u4e0b\u964d\u53bb\u8ba1\u7b97\u7684\u65f6\u5019\uff0c\u4ed6\u603b\u80fd\u591f\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u3002 \u6211\u4eec\u521a\u521a\u4f7f\u7528\u7684\u7b97\u6cd5\uff0c\u6709\u65f6\u4e5f\u79f0\u4e3a\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u3002\u5b9e\u9645\u4e0a\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u901a\u5e38\u4e0d\u592a\u4f1a\u7ed9\u7b97\u6cd5\u8d77\u540d\u5b57\uff0c\u4f46\u8fd9\u4e2a\u540d\u5b57\u201d \u6279\u91cf\u68af\u5ea6\u4e0b\u964d \u201d\uff0c\u6307\u7684\u662f\u5728\u68af\u5ea6\u4e0b\u964d\u7684\u6bcf\u4e00\u6b65\u4e2d\uff0c\u6211\u4eec\u90fd\u7528\u5230\u4e86\u6240\u6709\u7684\u8bad\u7ec3\u6837\u672c\u3002\u5728\u5e94\u7528\u68af\u5ea6\u4e0b\u964d\uff0c\u5e76\u8ba1\u7b97\u504f\u5bfc\u6570\u65f6\uff0c\u6211\u4eec\u90fd\u9700\u8981\u8ba1\u7b97 \\(\\sum\\limits_{i=1}^{m}\\Big(h_\\theta(x)-y\\Big)\\) \u3002\u56e0\u6b64\uff0c\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u6cd5\u8fd9\u4e2a\u540d\u5b57\u8bf4\u660e\u4e86\u6211\u4eec\u9700\u8981\u8003\u8651 \u6240\u6709 \u8fd9\u4e00\"\u6279\"\u8bad\u7ec3\u6837\u672c\uff0c\u800c\u4e8b\u5b9e\u4e0a\uff0c\u6709\u65f6\u4e5f\u6709\u5176\u4ed6\u7c7b\u578b\u7684\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u4e0d\u662f\u8fd9\u79cd\"\u6279\u91cf\"\u578b\u7684\uff0c\u4e0d\u8003\u8651\u6574\u4e2a\u7684\u8bad\u7ec3\u96c6\uff0c\u800c\u662f\u6bcf\u6b21\u53ea\u5173\u6ce8\u8bad\u7ec3\u96c6\u4e2d\u7684\u4e00\u4e9b\u5c0f\u7684\u5b50\u96c6\u3002\u5728\u540e\u9762\u7684\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e5f\u5c06\u4ecb\u7ecd\u8fd9\u4e9b\u65b9\u6cd5\u3002 \u5982\u679c\u4f60\u4e4b\u524d\u5b66\u8fc7\u7ebf\u6027\u4ee3\u6570\uff0c\u6709\u4e9b\u540c\u5b66\u4e4b\u524d\u53ef\u80fd\u5df2\u7ecf\u5b66\u8fc7\u9ad8\u7b49\u7ebf\u6027\u4ee3\u6570\uff0c\u4f60\u5e94\u8be5\u77e5\u9053\u6709\u4e00\u79cd\u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6700\u5c0f\u503c\u7684\u6570\u503c\u89e3\u6cd5\uff0c\u4e0d\u9700\u8981\u68af\u5ea6\u4e0b\u964d\u8fd9\u79cd\u8fed\u4ee3\u7b97\u6cd5\u3002\u5728\u540e\u9762\u7684\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e5f\u4f1a\u8c08\u5230\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u591a\u6b65\u68af\u5ea6\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u89e3\u51fa\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u7684\u6700\u5c0f\u503c\uff0c\u8fd9\u662f\u53e6\u4e00\u79cd\u79f0\u4e3a\u6b63\u89c4\u65b9\u7a0b( normal equations )\u7684\u65b9\u6cd5\u3002\u5b9e\u9645\u4e0a\u5728\u6570\u636e\u91cf\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u6bd4\u6b63\u89c4\u65b9\u7a0b\u8981\u66f4\u9002\u7528\u4e00\u4e9b\u3002 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u638c\u63e1\u4e86\u68af\u5ea6\u4e0b\u964d\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u6211\u4eec\u8fd8\u5c06\u5728\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e2d\u5927\u91cf\u5730\u4f7f\u7528\u5b83\u3002\u6240\u4ee5\uff0c\u795d\u8d3a\u81ea\u5df1 \u6210\u529f\u5b66\u4f1a\u4e86\u7b2c\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002 \u5148\u5199\u5230\u8fd9\u91cc\uff0c\u540e\u9762\u6253\u7b97\u9644\u4e0a\u4e60\u9898\u548c\u5b9e\u64cdpython\u4ee3\u7801\u3002 \u4e60\u9898 && \u53c2\u8003\u7b54\u6848 \u7b2c\u4e00\u9898 \u57fa\u4e8e\u4e00\u4e2a\u5b66\u751f\u5728\u5927\u5b66\u4e00\u5e74\u7ea7\u7684\u8868\u73b0\uff0c\u9884\u6d4b\u4ed6\u5728\u5927\u5b66\u4e8c\u5e74\u7ea7\u8868\u73b0\u3002 \u4ee4x\u7b49\u4e8e\u5b66\u751f\u5728\u5927\u5b66\u7b2c\u4e00\u5e74\u5f97\u5230\u7684\u201cA\u201d\u7684\u4e2a\u6570\uff08\u5305\u62ecA-\uff0cA\u548cA+\u6210\u7ee9\uff09\u5b66\u751f\u5728\u5927\u5b66\u7b2c\u4e00\u5e74\u5f97\u5230\u7684\u6210\u7ee9\u3002\u9884\u6d4by\u7684\u503c\uff1a\u7b2c\u4e8c\u5e74\u83b7\u5f97\u7684\u201cA\u201d\u7ea7\u7684\u6570\u91cf \u8fd9\u91cc\u6bcf\u4e00\u884c\u662f\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u3002\u5728\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u6211\u4eec\u7684\u5047\u8bbe \\(h_\\theta(x)=\\theta_0+\\theta_1x\\) \uff0c\u5e76\u4e14\u6211\u4eec\u4f7f\u7528 m \u6765\u8868\u793a\u8bad\u7ec3\u793a\u4f8b\u7684\u6570\u91cf\u3002 x y 3 2 1 2 0 1 4 3 \u5bf9\u4e8e\u4e0a\u9762\u7ed9\u51fa\u7684\u8bad\u7ec3\u96c6 \uff08\u6ce8\u610f\uff0c\u6b64\u8bad\u7ec3\u96c6\u4e5f\u53ef\u4ee5\u5728\u672c\u6d4b\u9a8c\u7684\u5176\u4ed6\u95ee\u9898\u4e2d\u5f15\u7528\uff09\uff0c m \u7684\u503c\u662f\u591a\u5c11 \uff1f \u7b2c\u4e8c\u9898 \u5047\u8bbe\u6211\u4eec\u4f7f\u7528\u7b2c\u4e00\u9898\u4e2d\u7684\u8bad\u7ec3\u96c6\u3002\u5e76\u4e14\uff0c\u6211\u4eec\u4ee3\u4ef7\u51fd\u6570\u7684\u5b9a\u4e49\u662f \\(J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}\\big(h_\\theta(x^{(i)})-y^{(i)}\\big)^2\\) \u6c42 \\(J(0, 1)\\) ? \u7b2c\u4e09\u9898 \u4ee4\u95ee\u98981\u4e2d\uff0c\u7ebf\u6027\u56de\u5f52\u5047\u8bbe\u7684 \\(\\theta_0=-1, \\theta_1=2\\) , \u6c42 \\(h_\\theta(6)\\) ? \u7b2c\u56db\u9898 \u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u4e0e \\(\\theta_0, \\theta_1\\) \u7684\u5173\u7cfb\u5982\u4e0b\u56fe\u6240\u793a\u3002\u56fe\u4e2d\u4e2d\u7ed9\u51fa\u4e86\u76f8\u540c\u4ee3\u4ef7\u51fd\u6570\u7684\u7b49\u9ad8\u7ebf\u56fe\u3002\u6839\u636e\u56fe\u793a\uff0c\u9009\u62e9\u6b63\u786e\u7684\u9009\u9879\uff08\u9009\u51fa\u6240\u6709\u6b63\u786e\u9879\uff09 A. \u4eceB\u70b9\u5f00\u59cb\uff0c\u5b66\u4e60\u7387\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u6700\u7ec8\u5e2e\u52a9\u6211\u4eec\u5230\u8fbe\u6216\u8005\u63a5\u8fd1A\u70b9\uff0c\u5373\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u5728A\u70b9\u6709\u6700\u5c0f\u503c B. \u70b9P\uff08\u56fe2\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff09\u5bf9\u5e94\u4e8e\u56fe1\u7684\u70b9C C. \u4eceB\u70b9\u5f00\u59cb\uff0c\u5b66\u4e60\u7387\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u6700\u7ec8\u5e2e\u52a9\u6211\u4eec\u5230\u8fbe\u6216\u8005\u63a5\u8fd1C\u70b9\uff0c\u5373\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u5728C\u70b9\u6709\u6700\u5c0f\u503c D. \u4eceB\u70b9\u5f00\u59cb\uff0c\u5b66\u4e60\u7387\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u6700\u7ec8\u5e2e\u52a9\u6211\u4eec\u5230\u8fbe\u6216\u8005\u63a5\u8fd1A\u70b9\uff0c\u5373\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u5728A\u70b9\u6709\u6700\u5927\u503c E. \u70b9P\uff08\u56fe2\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff09\u5bf9\u5e94\u4e8e\u56fe1\u7684\u70b9A \u7b2c\u4e94\u9898 \u5047\u8bbe\u5bf9\u4e8e\u67d0\u4e2a\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff08\u6bd4\u5982\u9884\u6d4b\u623f\u4ef7\uff09\uff0c\u6211\u4eec\u6709\u4e00\u4e9b\u8bad\u7ec3\u96c6\uff0c\u5bf9\u4e8e\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u80fd\u591f\u627e\u5230\u4e00\u4e9b \\(\\theta_0, \\theta_1\\) \uff0c\u4f7f\u5f97 \\(J(\\theta_0, \\theta_1)=0\\) \u3002\u4ee5\u4e0b\u54ea\u9879\u9648\u8ff0\u662f\u6b63\u786e\u7684\uff1f\uff08\u9009\u51fa\u6240\u6709\u6b63\u786e\u9879\uff09 A. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5fc5\u987b\u6709 \\(\\theta_0=0, \\theta_1=0\\) \uff0c\u8fd9\u6837\u624d\u80fd\u4f7f \\(J(\\theta_0, \\theta_1) = 0\\) B. \u5bf9\u4e8e\u6ee1\u8db3 \\(J(\\theta_0, \\theta_1) = 0\\) \u7684 \\(\\theta_0, \\theta_1\\) \u7684\u503c\uff0c\u5176\u5bf9\u4e8e\u6bcf\u4e2a\u8bad\u7ec3\u4f8b\u5b50 \\((x^{(i)},y^{(i)})\\) \uff0c\u90fd\u6709 \\(h_\\theta(x^{(i)})=y^{(i)}\\) C. \u8fd9\u662f\u4e0d\u53ef\u80fd\u7684\uff1a\u901a\u8fc7 \\(J(\\theta_0, \\theta_1) = 0\\) \u7684\u5b9a\u4e49\uff0c\u4e0d\u53ef\u80fd\u5b58\u5728 \\(\\theta_0, \\theta_1\\) \u4f7f\u5f97 \\(J(\\theta_0, \\theta_1) = 0\\) D. \u5373\u4f7f\u5bf9\u4e8e\u6211\u4eec\u8fd8\u6ca1\u6709\u770b\u5230\u7684\u65b0\u4f8b\u5b50\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u5b8c\u7f8e\u5730\u9884\u6d4by\u7684\u503c\uff08\u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u5b8c\u7f8e\u5730\u9884\u6d4b\u6211\u4eec\u5c1a\u672a\u89c1\u8fc7\u7684\u65b0\u623f\u7684\u4ef7\u683c\uff09 \u53c2\u8003\u7b54\u6848 \u7b2c\u4e00\u9898\uff1a4 \u7b2c\u4e8c\u9898\uff1a0.5 \u7531\u5df2\u77e5\u6c42 \\(J(0, 1)\\) , \u53ef\u5f97\uff1a \\(\\theta_0=0, \\theta_0=1\\) \u6240\u4ee5 \\(h_\\theta(x)=0+1\\cdot x=x\\) \u6700\u540e, \u5c06\u4e0a\u8ff0\u7ed3\u679c\u548c\u8bad\u7ec3\u96c6\u6570\u636e\u4ee3\u5165\u53ef\u5f97\uff1a \\[ \\begin{split} J(0, 1)=\\frac{1}{2*4}[ & (3-2)^2 + (1-2)^2+ \\\\\\\\ & (0-1)^2+(4-3)^2]=0.5 \\end{split} \\] \u7b2c\u4e09\u9898\uff1a11 \u5df2\u77e5, \\(h_\\theta(x)=\\theta_0+\\theta_1x\\) , \\(\\theta_0=-1, \\theta_1=2\\) \u6240\u4ee5\u5c06 \\(\\theta_0, \\theta_1\\) \u4ee3\u5165\u53ef\u5f97 \\(h_\\theta(x)=-1+2x\\) \u8ba9\u6211\u4eec\u6c42 \\(h_\\theta(6)\\) \u6700\u540e, \\(h_\\theta(6)=-1+2*6=11\\) \u7b2c\u56db\u9898\uff1aAE \u7b2c\u4e94\u9898\uff1aB \u4e0a\u673a\u7ec3\u4e60 p { text-align: justify; /*\u6587\u672c\u4e24\u7aef\u5bf9\u9f50*/ } In this part of exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and popularations from the cities. \u2002 you would like to use this data to help you select which city to expand to next. \u2002 The file ex1data.txt contains the dataset for our linear regression problem. The first column is the popularations of a city and the second column is the profits of a food truck in that city. A negative value for profits indicates a loss. 1\u3001Plotting the Data Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only properties to plot (profit and popularation). (Many other problems that you will encounter in real life are multi-dimensional and can't be plotted on 2-d plot). \u4ee3\u7801\u662f\u5728jupyter notebook\u4e0a\u6267\u884c\uff01\uff01\uff01 \u8bf7\u5c06\u6bcf\u4e2a\u4ee3\u7801\u5757\u6309\u987a\u5e8f\u62f7\u8d1d\u5230cell\u4e2d\u6267\u884c\u3002 \u4ee3\u7801\u57571 \u5bfc\u5165\u5fc5\u8981\u7684\u5e93\u548c\u6570\u636e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 0. \u5bfc\u5165\u9700\u8981\u7684\u5e93 import numpy as np import pandas as pd import matplotlib.pyplot as plt # 1. \u5bfc\u5165\u6570\u636e path = 'ex1data1.txt' # 1.1 \u4f7f\u7528pandas\u4e2d\u7684read_csv \u63a5\u6536\u6570\u636e ''' \u6ce8\u610f: header=None: \u662f\u6307\u6211\u4eec\u8bfb\u53d6\u7684\u539f\u59cb\u6587\u4ef6\u6570\u636e\u6ca1\u6709\u5217\u7d22\u5f15 names: \u6307\u5b9a\u65b0\u5217\u540d ''' data = pd . read_csv ( path , header = None , names = [ 'Population' , 'Profit' ]) # 1.2 \u5c55\u793a\u524d\u4e94\u884c\u6570\u636e data . head () \u8fd4\u56de\u7ed3\u679c: Population Profit 0 6.1101 17.5920 1 5.5277 9.1302 2 8.5186 13.6620 3 7.0032 11.8540 4 5.8598 6.8233 \u7528matplotlib\u753b\u56fe 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 1. \u521b\u5efa\u753b\u5e03 plt . figure ( figsize = ( 12 , 8 ), dpi = 100 ) # 2. \u7ed8\u5236\u6563\u70b9\u56fe plt . scatter ( data [ \"Population\" ], data [ \"Profit\" ], color = \"r\" , label = \"Training data\" ) # 2.1 \u6dfb\u52a0\u63cf\u8ff0\u4fe1\u606f plt . xlabel ( \"Population of City in 10,000s\" ) plt . ylabel ( \"Profit in $10,000s\" ) # 2.2 \u6dfb\u52a0\u7f51\u683c\u663e\u793a plt . grid ( True , linestyle = \"--\" , alpha = 0.5 ) # 2.3 \u6dfb\u52a0\u56fe\u4f8b plt . legend ( loc = 7 ) # 3.\u663e\u793a\u56fe\u50cf plt . show () \u8f93\u51fa\u56fe\u50cf\uff1a 2\u3001Gradient Descent In this part you will fit linear regression parameters \\(\\theta\\) to our dataset using gradient descent. 2.1 Update Equations The objective of linear regression is to minimize the cost function \\[ J(\\theta)=\\sum_{i=1}^m{h_ \\theta(x^{(i)}-y^{(i)})}^2 \\] where the hypothesis \\(h_\\theta(x)\\) is given by the linear model \\[ h_\\theta(x) = \\theta^Tx=\\theta_0+\\theta_1x_1 \\] \u2002 Recall that the parameters of your model are the \\(\\theta_j\\) values. These are the values you will adjust to minimize cost \\(J(\\theta)\\) . One way to do this is to use the batch descent algorithm. In batch gradient descent, each iteration performs the update \\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m{h_ \\theta(x^{(i)}-y^{(i)})x_j^{(i)}} \\\\\\\\ (simultaneously\\ update\\ \\theta_j\\ for\\ all\\ j ) \\] \u2002 With each step of gradient descent, your parameters \\(\\theta_j\\) come closer to the optomal values that will achieve that the lowest cost \\(J(\\theta)\\) Implementation Note To take into accout the intercept term \\((\\theta_0)\\) , we add an additional first column to X and set it to all ones. This allows us to treat \\(\\theta_0\\) as simply another 'feature'. 2.2 Implementation In \u4ee3\u7801\u57571, we have already set up the data for linear regression. In the following lines, we add another dimension to our data to accommodate the \\(\\theta_0\\) intercept term. We also initialize the initial parameters to 0 and the learning rate alpha to 0.01. \u4ee3\u7801\u57572.1 \u8bbe\u7f6eX\u548cy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # \u8fd9\u4e00\u6b65\u76f8\u5f53\u4e8e\u8bbe\u7f6e\u6240\u6709 x_0=1 data . insert ( 0 , 'Ones' , 1 ) # \u53ef\u4ee5\u5148\u770b\u4e00\u4e0b\u76ee\u524ddata.shape # data.shape ## \u5148\u83b7\u53d6data\u603b\u5217\u6570 cols = data . shape [ 1 ] # \u4ecedata\u4e2d\u53d6\u5230X\u548cy # X : training data # y : target variable X = data . iloc [:, 0 : cols - 1 ] # X\u53d6\u6240\u6709\u884c, \u53bb\u6700\u540e\u4e00\u5217 y = data . iloc [:, cols - 1 ] # y\u53d6\u6240\u6709\u884c, \u6700\u540e\u4e00\u5217 # \u6ce8\u610f\u8fd9\u91cc\u7684X, y\u4ecd\u7136\u5305\u542b\u884c\u7d22\u5f15\u548c\u5217\u7d22\u5f15\u503c \u8f93\u51fa\u5e76\u89c2\u5bdf\u4e0b X (\u8bad\u7ec3\u96c6) and y (\u76ee\u6807\u53d8\u91cf)\u662f\u5426\u6b63\u786e\u3002 \u8f93\u5165 [1]\uff1a X . head () \u8f93\u51fa [1]\uff1a Ones Population 0 1 6.1101 1 1 5.5277 2 1 8.5186 3 1 7.0032 4 1 5.8598 \u8f93\u5165 [2]\uff1a y . head () \u8f93\u51fa [2]\uff1a 0 17.5920 1 9.1302 2 13.6620 3 11.8540 4 6.8233 Name : Profit , dtype : float64 \u4ee3\u7801\u57572.2 \u5c06X\u548cy\u8f6c\u6210ndarray \u5e76\u8bbe\u7f6e\u5b66\u4e60\u7387\u548c\u8fed\u4ee3\u6b21\u6570 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # \u4ecedataframe \u53d6\u51favalue X = X . value y = y . value # \u53ef\u4ee5\u68c0\u9a8c\u4e00\u4e0b X\u548cy\u7684shape # X = X.shape # Y = y.shape # \u5c06X\u548cy\u8f6c\u5316\u6210numpy array \u52a0\u5feb\u8ba1\u7b97\u901f\u5ea6 X = np . array ( X ) # reshape\u7684\u4f5c\u7528\u662f\u5c06y\u8f6c\u6210\u4e0d\u7ba1\u884c, \u5217\u6570\u4e3a1\u7684\u77e9\u9635 y = np . array ( y ) # \u8bbe\u7f6e\u8fed\u4ee3\u6b21\u6570 iterations = 1500 # \u8bbe\u7f6e\u5b66\u4e60\u7387 alpha = 0.01 # \u521d\u59cb\u5316theta theta = np . zeros ([ 2 , ]) # \u8bbe\u7f6e\u5b8c\u6210\u540e\uff0c\u770b\u4e0bX y theta\u7684\u7ef4\u5ea6 X . shape , y . shape , theta . shape \u8f93\u51fa (( 97 , 2 ), ( 97 , 1 ), ( 2 , 1 )) \u91cd\u91cd\u91cd\u8981\u63d0\u793a!!! \u7b14\u8005\u5728\u8fd9\u91cc\u8e29\u4e86\u4e00\u4e2a\u5927\u5751, \u4e0a\u9762\u4e00\u4e2a\u4ee3\u7801\u57572.2 \u7684\u7b2c 19 \u884c\u975e\u5e38\u91cd\u8981\u3002\u56e0\u4e3atheta \u662f\u4e00\u4e2a\u5411\u91cf, \u6240\u4ee5\u7ef4\u5ea6\u53ea\u80fd\u8bbe\u7f6e\u4e3a[2,], \u800c\u4e0d\u662f[2,1]\u3002numpy\u91cc[2,]\u548c[2,1] \u4e0d\u662f\u4e00\u56de\u4e8b\u3002 2.3 Computing the \\(J(\\theta)\\) As you perform gradient descent to learn minimize the cost function \\(J(\\theta)\\) , it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate \\(J(\\theta)\\) so you can check the convergence of your gradient descent implementation. \u2002 As you are doing this, remember that the variables X and y are not scalar values, but matrices whose rows represent the examples from the training set. \u2002 Once you have completed the function, you should expect to see a cost of 32.07 ( \\(\\theta\\) initialized to zeros). \u4ee3\u7801\u57572.3 \u5b9a\u4e49\u4ee3\u4ef7\u51fd\u6570\u5e76\u8ba1\u7b97\u521d\u59cb\u503c 1 2 3 4 5 6 7 8 9 10 11 12 # \u5b9a\u4e49\u4ee3\u4ef7\u51fd\u6570 def computeCost ( X , y , theta ): m = X . shape [ 0 ] # print((np.dot(X,theta)-y).shape) inner = np . dot (( np . dot ( X , theta ) - y ) . T , ( np . dot ( X , theta ) - y )) return np . sum ( inner ) / ( 2 * m ) computeCost ( X , y , theta ) # \u8ba1\u7b97\u4e0d\u8fed\u4ee3\u4e4b\u524dJ\u7684\u521d\u59cb\u503c computeCost ( X , y , theta ) \u8f93\u51fa 32.072733877455676 2.4 Gradient descent \u2002 As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the \\(J(\\theta)\\) is parameterized by the vector \\(\\theta\\) not X and y. That is, we minimize the value of \\(J(\\theta)\\) by changing the values of the vector \\(\\theta\\) , not by changing X or y. Refer to the equations in this handout and to the video lectures if you are uncertain. \u2002 A good way to verify that gradient descent is working correctly is to look at the value of \\(J(\\theta)\\) and check that it is decreasing with each step. Assuming you have implemented gradient descent and computeCost correctly, your value of \\(J(\\theta)\\) should never increase, you should converge to a steady value by the end of the algorithm. \u5b9a\u4e49 \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 \u753b\u56fe 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # \u68af\u5ea6\u4e0b\u964d # \u5b9a\u4e49\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 def gradientDescent ( X , y , theta , alpha , iterations ): m = X . shape [ 0 ] # m: \u6837\u672c\u7684\u603b\u4e2a\u6570 n = len ( theta ) # n: theta\u7684\u603b\u4e2a\u6570 # \u7528\u4e00\u4e2a\u5411\u91cf\u6765\u8bb0\u5f55\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u6240\u6709\u7684cost\u503c cost = np . zeros ( iterations ) for i in range ( iterations ): cost [ i ] = computeCost ( X , y , theta ) # theta\u662f\u51e0\u4e2a\u5c31\u8981\u66f4\u65b0\u51e0\u4e2a for j in range ( n ): theta [ j ] = theta [ j ] - alpha * \\ ( 1 / m ) * np . sum (( np . dot ( X , theta ) - y ) * X [:, j ]) return theta , cost # \u4fdd\u8bc1\u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0ctheta\u4e0d\u53d7\u524d\u9762\u5f71\u54cd\uff0c\u6545\u91cd\u65b0\u521d\u59cb\u5316theta\u3002 theta = np . zeros ([ 2 , ]) # \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u5206\u522b\u63a5\u6536\u66f4\u65b0\u540e\u7684theta\u503c\u548c\u6bcf\u4e00\u6b65\u8fed\u4ee3\u7684cost\u503c theta_hat , costs = gradientDescent ( X , y , theta , alpha , iterations ) # \u6253\u5370\u68af\u5ea6\u5904\u7406\u540e\u7684\u9884\u6d4b\u51fd\u6570\u7684\u53c2\u6570theta print ( theta_hat ) # \u7528matplotlib\u753b\u56fe # 1. \u521b\u5efa\u753b\u5e03 plt . figure ( figsize = ( 12 , 8 ), dpi = 100 ) # 2. \u7ed8\u5236\u56fe\u50cf # \u7ed8\u5236\u8bad\u7ec3\u96c6\u6570\u636e plt . scatter ( data [ \"Population\" ], data [ \"Profit\" ], color = \"r\" , label = \"Training data\" ) # \u7ed8\u5236\u62df\u5408\u540e\u7684\u6570\u636e x_hat = np . linspace ( data . Population . min (), data . Population . max (), 100 ) y_hat = theta_hat [ 0 ] + theta_hat [ 1 ] * x_hat plt . plot ( x_hat , y_hat , color = 'b' , label = \"Linear regression\" ) # 2.1 \u6dfb\u52a0\u63cf\u8ff0\u4fe1\u606f plt . xlabel ( \"Population of City in 10,000s\" ) plt . ylabel ( \"Profit in $10,000s\" ) # 2.2 \u6dfb\u52a0\u7f51\u683c\u663e\u793a plt . grid ( True , linestyle = \"--\" , alpha = 0.5 ) # 2.3 \u6dfb\u52a0\u56fe\u4f8b plt . legend ( loc = 7 ) # 3.\u663e\u793a\u56fe\u50cf plt . show () \u8f93\u51fa\u7ed3\u679c array ([ - 3.63606348 , 1.16698916 ]) \u539f\u7ec3\u4e60\u4e2d\uff0c\u9700\u8981\u753b( \\(\\theta\\) , J ) \u7684\u4e09\u7ef4\u56fe \u4ee5\u53ca \u7b49\u9ad8\u7ebf\u56fe, \u4ed6\u4eec\u662f\u7528matlab/octave \u5b9e\u73b0\u7684\u3002\u7b14\u8005python\u592a\u83dc\u4e86\uff0c\u5c31\u4e0d\u5f04\u4e86\u8d39\u65f6\u95f4\u3002\u6700\u540e\u753b\u4e00\u4e2a\u8fed\u4ee3\u6b21\u6570\u548ccost\u7684\u6298\u7ebf\u56fe\u6536\u5c3e\u672c\u7ae0\u8282\u3002 \u753b\u51facost\u5173\u4e8e\u8fed\u4ee3\u6b21\u6570\u7684\u56fe\u50cf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1. \u521b\u5efa\u753b\u5e03 plt . figure ( figsize = ( 12 , 8 ), dpi = 100 ) # 2. \u7ed8\u5236\u6298\u7ebf\u56fe plt . plot ( np . arange ( iterations ), costs , 'r' ) # 2.1 \u6dfb\u52a0\u63cf\u8ff0\u4fe1\u606f plt . xlabel ( \"Iters\" ) plt . ylabel ( \"Cost\" ) # 2.2 \u6dfb\u52a0\u7f51\u683c\u663e\u793a plt . grid ( True , linestyle = \"--\" , alpha = 0.5 ) # 3.\u663e\u793a\u56fe\u50cf plt . show ()","title":"\u4e00. \u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#1","text":"\u53c2\u8003\u89c6\u9891: 2 - 1 - Model Representation (8 min).mkv \u6211\u4eec\u8fd9\u91cc\u6709\u4e00\u4e2a\u4fc4\u52d2\u5188\u5dde\u6ce2\u7279\u5170\u5e02\u7684 \u4f4f\u623f\u4ef7\u683c\u6570\u636e\u96c6 \uff0c\u6570\u636e\u96c6\u5305\u542b\uff1a\u623f\u5c4b \u5c3a\u5bf8 \uff0c\u623f\u5c4b \u51fa\u552e\u4ef7\u683c ......\u7136\u540e\uff0c\u4f60\u6709\u4e00\u4e2a\u670b\u53cb\u3002\u4ed6\u6709\u4e00\u59571250\u5e73\u7684\u623f\u5b50\uff0c\u4ed6\u9700\u8981\u4f60\u544a\u8bc9\u4ed6\u8fd9\u623f\u5b50\u80fd\u5356\u591a\u5c11\u94b1\u3002\u6211\u4eec\u8be5\u600e\u4e48\u505a\uff1f \u90e8\u5206\u6570\u636e\u96c6\u5982\u4e0b\u8868\u6240\u793a\uff1a Size in \\(feet^2 (x)\\) Price ($) in \\(1000's (y)\\) 2104 460 1416 232 1534 315 852 178 \u2026 \u2026 \u4e0a\u8ff0\u6570\u636e\u96c6\uff0c\u6211\u4eec\u901a\u5e38\u79f0\u4e3a \u8bad\u7ec3\u96c6 \uff08 training set \uff09\u3002 \u4e3a\u4e86 \u65b9\u4fbf\u63cf\u8ff0 \u548c\u540e\u9762 \u516c\u5f0f\u63a8\u5bfc \u5c06\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u5982\u4e0b \u6807\u8bb0\uff08Notion\uff09 \uff1a \\(m\\) \u6837\u672c\u6570\u91cf\uff08number of training examples \uff09 \\(x\\) \u8f93\u5165\u53d8\u91cf/\u7279\u5f81\uff08input of variable/features\uff09 \\(y\\) \u76ee\u6807\u53d8\u91cf/\u8f93\u51fa\u53d8\u91cf\uff08output variable / target variable\uff09 \\((x, y)\\) \u4e00\u4e2a\u6837\u672c\uff08one training example\uff09 \\((x^{(i)}, y^{(i)})\\) \u7b2c \\(i\\) \u4e2a\u6837\u672c \u597d\u4e86\uff0c\u4e0b\u9762\u5148\u6765 \u660e\u786e\u4e00\u4e0b \u6211\u4eec \u5df2\u77e5\u6761\u4ef6 \u548c \u8981\u505a\u7684\u4e8b \uff1a \u6211\u4eec\u5df2\u77e5\u4e86\u8bad\u7ec3\u96c6\uff08m\u7ec4\u6570\u636e\uff1a \\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...(x^{(m)}, y^{(m)}), \u5176\u4e2d(x^{(i)}, y^{(i)})\\) \u4ee3\u8868\u4e86\u7b2c i \u7ec4\u7684\uff08\u623f\u5c4b\u5c3a\u5bf8\uff0c\u623f\u5c4b\u4ef7\u683c\uff09\uff09\uff0c\u6839\u636e\u8fd9\u4e2a\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u8981\u8bad\u7ec3\u51fa\u6211\u4eec\u7684\u6a21\u578b\uff08\u51fd\u6570\uff09\uff0c\u901a\u5e38\u8868\u793a\u4e3ah\uff0c\u5373 hypothesis(\u5047\u8bbe) \u3002\u800c\u8fd9\u4e2a\u51fd\u6570h\u7684\u8f93\u5165\u662f\u623f\u5c4b\u5c3a\u5bf8\uff0c\u8f93\u51fa\u5c31\u662f\u623f\u5c4b\u4ef7\u683c\u3002\u56e0\u6b64\uff0ch \u662f\u4e00\u4e2a\u4ecex \u5230 y \u7684\u51fd\u6570\u6620\u5c04\u3002 \u90a3\u4e48\uff0c\u5bf9\u4e8e\u6211\u4eec\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u63cf\u8ff0 \\(h\\) \uff1f \u4e00\u79cd\u53ef\u80fd\u7684\u8868\u8fbe\u65b9\u5f0f\u4e3a\uff1a $$ h_\\theta \\left( x \\right)=\\theta_{0} + \\theta_{1}x\\tag{1.2.1} $$ \u56e0\u4e3a\u53ea\u542b\u6709\u4e00\u4e2a\u7279\u5f81/\u8f93\u5165\u53d8\u91cf\uff0c\u56e0\u6b64\u8fd9\u6837\u7684\u95ee\u9898\u53eb\u4f5c\u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u3002 Note \u7ebf\u6027\u56de\u5f52\u4e2d \u7ebf\u6027 \u7684\u542b\u4e49\uff1a \u56e0\u53d8\u91cf y \u5bf9\u4e8e \u672a\u77e5\u7684\u56de\u5f52\u7cfb\u6570 \uff08 \\(\\theta_0\\) \uff0c \\(\\theta_1\\) \uff0c.... \\(\\theta_n\\) \uff09 \u662f \u7ebf\u6027 \u7684\u3002","title":"1. \u6a21\u578b\u63cf\u8ff0"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#2","text":"\u53c2\u8003\u89c6\u9891: 2 - 2 - Cost Function (8 min).mkv Training Set Size in \\(feet^2 (x)\\) Price ($) in \\(1000's (y)\\) 2104 460 1416 232 1534 315 852 178 \u2026 \u2026 Hypothesis: \\(h_\\theta \\left( x \\right)=\\theta_{0}+\\theta_{1}x\\) Parameters: \\(\\theta_{0}\\) \uff0c \\(\\theta_{1}\\) \u901a\u8fc7\u4e0a\u4e00\u8282\uff0c\u6211\u4eec\u77e5\u9053\u4e86\uff0c\u6211\u4eec\u8981\u5b8c\u6210\u670b\u53cb\u7684 \u9700\u6c42\uff08\u6839\u636e\u4ed6\u623f\u5b50\u7684\u5927\u5c0f\u9884\u6d4b\u623f\u4ef7 \uff09\uff0c\u8981\u77e5\u9053\u5047\u8bbe\u51fd\u6570 \\(h\\) \uff0c\u6211\u4eec\u5bf9 \\(h\\) \u505a\u51fa\u8fd9\u6837\u4e00\u79cd\u5047\u8bbe\uff1a \\(h_\\theta \\left( x \\right)=\\theta_{0}+\\theta_{1}x\\) \u3002\u901a\u8fc7\u89c2\u5bdf\u8fd9\u4e2a\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u8fd9\u4e2a\u95ee\u9898\u8f6c\u5316\u4e3a\u6c42 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \uff0c\u4ece\u800c\u5f53\u4f60\u670b\u53cb\u628a\u623f\u5b50\u5927\u5c0f\u544a\u8bc9\u4f60\uff0c\u4f60\u5c06\u5176\u4ee3\u5165\u516c\u5f0f\u5373\u53ef\u5f97\u5230\u9884\u6d4b\u7684\u623f\u4ef7\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u5982\u4f55\u9009\u62e9\u5462 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \uff1f \u9996\u5148\uff0c\u6211\u4eec\u5148\u76f4\u89c2\u7406\u89e3 \\(h_\\theta \\left( x \\right)\\) ---\u4e0b\u56fe\u662f \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u53d6\u4e0d\u540c\u503c\u65f6\uff0c \\(h\\) \u7684\u6574\u4f53\u56fe\u50cf\u3002 Note \u4e0a\u9762\u56fe\u50cf\u4e3a\u4e86\u65b9\u4fbf\u7701\u7565\u4e86 \\(\\theta\\) \u4e0b\u6807\uff0c\u5373 \\(h_\\theta(x) = h(x)\\) \uff0c\u540e\u9762\u4e5f\u53ef\u80fd\u4f1a\u8fd9\u6837\u505a\u3002 \u7ed3\u5408\u56fe\u50cf\u63cf\u8ff0\u6211\u4eec\u7684\u4efb\u52a1 \u628a\u8bad\u7ec3\u96c6\u6570\u636e\u7ed8\u5236\u5728\u4e0b\u56fe\u4e2d\uff08\u5e76\u975e\u4e0a\u9762\u7684\u623f\u4ef7\u8bad\u7ec3\u96c6\uff0c\u4ec5\u4e3e\u4f8b\u793a\u610f\uff09 \u6211\u4eec\u8981\u505a\u7684\u5c31\u662f\u7ed8\u5236\u4e00\u6761\u76f4\u7ebf\uff08\u4e0b\u56fe\u84dd\u8272\u7684\u7ebf\uff09\u5c3d\u91cf\u5730\u4e0e\u4e0a\u9762\u90a3\u4e9b\u70b9\u6709\u548c\u597d\u7684\u62df\u5408\uff0c\u8fd9\u6761\u76f4\u7ebf\u5c31\u662f \\(h_\\theta(x)\\) \u3002\u5982\u4f55\u786e\u5b9a\u8fd9\u6761\u76f4\u7ebf\u4e5f\u5c31\u662f\u786e\u5b9a \\(\\theta_0\\) \u548c \\(\\theta_1\\) \u3002 \u6211\u4eec\u7684idea\uff1a Choose \\(\\theta_0, \\theta_1\\) so that \\(h_\\theta(x)\\) is close to \\(y\\) for our training examples \\((x, y)\\) \u5c06\u6211\u4eec\u7684\u4efb\u52a1\u4ee5\u516c\u5f0f\u5316\u6807\u8bb0\uff0c\u5373\uff1a \\[ \\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}\\ \\frac {1} {2m}\\cdot\\sum\\limits_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)^2\\tag{2.1.1} \\] \u7ebf\u6027\u56de\u5f52\u5b9e\u9645\u4e0a\u5c31\u662f\u89e3\u51b3\u4e00\u4e2a\u5173\u4e8e \\(\\theta_0, \\theta_1\\) \u7684\u6700\u5c0f\u5316\u95ee\u9898(minimize)\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u76f4\u7ebf\u4e0e\u90a3\u4e9b\u70b9\u6709\u5f88\u597d\u7684\u62df\u5408\uff0c\u90a3\u6211\u4eec\u628a\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4b\u5f97\u5230\u7684\u503c \\(\\Big(\\) \u5c06x\u4ee3\u5165\u5047\u8bbe\u51fd\u6570\u53ef\u5f97\u5230\uff0c\u5373 \\(h_\\theta(x)\\Big)\\) \u4e0e\u771f\u5b9e\u503cy\u6c42\u4e00\u4e2a\u5dee\u7684\u5e73\u65b9\u3002\u518d\u628a\u8fd9\u4e9b\u5e73\u65b9\u7d2f\u52a0\u3002\u5373: \\[ \\sum\\limits_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)^2\\tag{2.1.2} \\] \u6211\u4eec\u53ea\u8981\u8ba9\u8fd9\u4e2a\u5e73\u65b9\u548c\u6700\u5c0f\u5373\u53ef\u3002 \u6ce8\u610f, \u5173\u4e8e\u5f0f(2.1.1)\u524d\u9762 \\(\\frac{1}{2m}\\) \\(\\frac{1}{m}\\) \u662f\u56e0\u4e3a\u6c42\u548c\u9879\u6709m\u4e2a\uff0c\u8fd9\u91cc\u9664\u4ee5m\uff0c\u662f\u6c42\u5e73\u5747\u503c\u3002 \\(\\frac{1}{2}\\) \u4e3a\u4e86\u540e\u9762\u6c42\u5bfc\u6d88\u53bb \u5176\u5b9e\u8fd9\u4e2a \\(\\frac{1}{2m}\\) \u5bf9\u6211\u4eec\u6c42\u6700\u5c0f\u503c\u6ca1\u6709\u4efb\u4f55\u5f71\u54cd\uff0c\u53ea\u662f\u4e3a\u4e86\u8ba1\u7b97\u65b9\u4fbf\u8fd9\u4e48\u5199\u3002 \u5173\u4e8e\u5f0f(2.1.2)\u7684\u51e0\u4f55\u610f\u4e49 \u6240\u6709\u6570\u636e\u70b9\u4e0e\u62df\u5408\u76f4\u7ebf\u5728y\u8f74\u65b9\u5411\u7684\u622a\u8ddd\u7684\u5e73\u65b9\u548c \u5728\u672c\u4f8b\u5b50\u4e2d\uff0c \\(h_\\theta \\left( x \\right)=\\theta_{0} + \\theta_{1}x\\) \u901a\u5e38\uff0c\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u4ee3\u4ef7\u51fd\u6570(cost function)\uff0c\u5982\u4e0b\uff1a \\[ J(\\theta_0,\\theta_1)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\\tag{2.1.3} \\] \u6211\u4eec\u7684\u6700\u7ec8\u76ee\u6807\u5c31\u8f6c\u5316\u4e3a\uff1a \\[ \\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}J(\\theta_0,\\theta_1) \\] \u8fd9\u91cc\u6211\u4eec\u5b9a\u4e49\u7684\u5f0f(2.1.3)\u635f\u5931\u51fd\u6570, \u4e5f\u88ab\u79f0\u4e3a\u5e73\u65b9\u8bef\u5dee\u51fd\u6570(squared error function)\u3002\u7136\u800c\uff0c\u635f\u5931\u51fd\u6570\u4e0d\u6b62\u8fd9\u4e00\u79cd\u5f62\u5f0f\uff0c\u4f46\u662f\u5728\u56de\u5f52\u95ee\u9898\u4e2d\uff0c\u5e73\u65b9\u8bef\u5dee\u51fd\u6570\u90fd\u662f\u6bd4\u8f83\u5408\u7406\u548c\u6bd4\u8f83\u5e38\u7528\u7684\u9009\u62e9\u3002","title":"2. \u4ee3\u4ef7\u51fd\u6570"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#3-1","text":"\u53c2\u8003\u89c6\u9891: 2 - 3 - Cost Function - Intuition I (11 min).mkv \u5728\u4e0a\u4e00\u4e2a\u5c0f\u8282\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u4ee3\u4ef7\u51fd\u6570\u7684\u5b9a\u4e49\u3002\u5728\u8fd9\u4e00\u5c0f\u8282\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u83b7\u53d6\u4e00\u4e9b\u76f4\u89c2\u7684\u611f\u53d7\uff0c\u770b\u770b\u4ee3\u4ef7\u51fd\u6570\u5230\u5e95\u662f\u5728\u5e72\u4ec0\u4e48\u3002 Hypothesis: \\(h_\\theta(x)=\\theta_0+\\theta_1x\\) Parameters: \\(\\theta_0,\\ \\theta_1\\) Cost Function: \\(J(\\theta_0,\\theta_1)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\\) Goal: \\(\\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}J(\\theta_0,\\theta_1)\\) \u4e3a\u4e86\u8ba9\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6709\u66f4\u597d\u7684\uff0c\u53ef\u89c6\u5316\u6548\u679c\uff0c\u6211\u4eec\u7b80\u5316\u5047\u8bbe\u51fd\u6570\u4e3a \\(h_\\theta(x)=\\theta_1x\\) \uff0c\u5373\u5047\u8bbe \\(\\theta_0=0\\) \u3002\u7b80\u5316\u540e\uff1a \\(h_\\theta(x)=\\theta_1x\\) \\(J(\\theta_1)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\\) Goal: \\(\\mathop{minimize} \\limits_{\\theta_1}J(\\theta_1)\\) \u5728\u786e\u5b9a\u597d\u4e0d\u540c\u7684 \\(\\theta_1\\) ,\u4e4b\u540e\u753b\u51fa\u5047\u8bbe\u51fd\u6570 \\(h\\) \u548c\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u3002\u672c\u4f8b\u4e2d\uff0c\u5047\u5b9a\u8bad\u7ec3\u96c6\u4e3a \\((1,1), (2,2), (3,3)\\) \u3002 \u5f53 \\(\\theta_1=1\\) \u65f6\uff0c \u6ce8\u610f\u5230\uff0c\u56e0\u4e3a \\(\\theta_1=1\\) \uff0c\u6b64\u65f6 \\(h_\\theta=y\\) ,\u6240\u4ee5 \\(J=0\\) \u3002\u5e76\u5728\u53f3\u8fb9\u56fe\u4e0a\u753b\u51fa\u7b2c\u4e00\u4e2a\u70b9(1,0)\u3002 \u5f53 \\(\\theta_1=0.5\\) \u65f6\uff0c \\((x,y)\\) \u4f9d\u6b21\u53d6(1, 0.5), (2, 1), (3, 1.5) \u8ba1\u7b97 \\(J \\approx 0.58\\) \u3002\u5e76\u5728\u53f3\u8fb9\u56fe\u4e0a\u753b\u51fa\u7b2c\u4e8c\u4e2a\u70b9(0.5,0.58)\u3002 \u5f53 \\(\\theta_1=0\\) \u65f6\uff0c \u540c\u7406\uff0c\u5f53 \\(\\theta_1\\) \u53d6\u4e0d\u540c\u503c\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa \\(J\\) \uff0c\u5e76\u5728\u53f3\u4fa7\u753b\u51fa \\(J\\) \u7684\u5927\u81f4\u56fe\u50cf\u3002 \u603b\u7ed3\uff1a \u6211\u4eec\u901a\u8fc7\u53d6\u4e0d\u540c\u7684 \\(\\theta_1\\) \uff0c\u7ed8\u5236\u51fa\u4e86 \\(J\\) \u3002 So for each value of \\(\\theta_1\\) , we wound up with a diffent value of \\(J(\\theta_1)\\) . And we colud then use this to trace out this plot on the right. Now you remember the optimization objective for our learning algorithm is we want to choose the value of \\(\\theta_1\\) that minimize \\(J(\\theta_1)\\) . This was our objective function for the linear regression. \u73b0\u5728\uff0c\u6211\u4eec\u89c2\u5bdf\u53f3\u4fa7\u90a3\u6761\u66f2\u7ebf\uff0c\u4f1a\u53d1\u73b0\uff0c\u5f53 \\(\\theta_1=1\\) \u65f6\uff0c \\(J(\\theta_1)\\) \u6700\u5c0f\u3002\u518d\u89c2\u5bdf\u5de6\u8fb9\u7684\u62df\u5408\u60c5\u51b5\u4f1a\u53d1\u73b0\uff0c\u8fd9\u786e\u5b9e\u662f\u6700\u597d\u7684\u60c5\u51b5\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u7279\u6b8a\u7684\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u786e\u5b9e\u5b8c\u7f8e\u5730\u62df\u5408\u4e86\u5b83\u3002 And that's why minimizing \\(J(\\theta_1)\\) corresponds to finding a straight line that fits the data well.","title":"3 \u4ee3\u4ef7\u51fd\u6570\u76f4\u89c2\u7406\u89e31"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#4-ii","text":"\u53c2\u8003\u89c6\u9891: 2 - 4 - Cost Function - Intuition II (9 min).mkv \u4ee3\u4ef7\u51fd\u6570\u7684\u6837\u5b50\uff0c\u5219\u53ef\u4ee5\u770b\u51fa\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u5b58\u5728\u4e00\u4e2a\u4f7f\u5f97 \\(J(\\theta_{0}, \\theta_{1})\\) \u6700\u5c0f\u7684\u70b9\u3002 \u901a\u8fc7\u8fd9\u4e9b\u56fe\u5f62\uff0c\u6211\u5e0c\u671b\u4f60\u80fd\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6240\u8868\u8fbe\u7684\u503c\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u5b83\u4eec\u5bf9\u5e94\u7684\u5047\u8bbe\u51fd\u6570\u662f\u4ec0\u4e48\u6837\u7684\uff0c\u4ee5\u53ca\u4ec0\u4e48\u6837\u7684\u5047\u8bbe\u5bf9\u5e94\u7684\u70b9\uff0c\u66f4\u63a5\u8fd1\u4e8e\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u7684\u6700\u5c0f\u503c\u3002 \u5f53\u7136\uff0c\u6211\u4eec\u771f\u6b63\u9700\u8981\u7684\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u5730\u627e\u51fa\u8fd9\u4e9b\u4f7f\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u53d6\u6700\u5c0f\u503c\u7684\u53c2\u6570 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u6765\u3002 \u6211\u4eec\u4e5f\u4e0d\u5e0c\u671b\u7f16\u4e2a\u7a0b\u5e8f\u628a\u8fd9\u4e9b\u70b9\u753b\u51fa\u6765\uff0c\u7136\u540e\u4eba\u5de5\u7684\u65b9\u6cd5\u6765\u8bfb\u51fa\u8fd9\u4e9b\u70b9\u7684\u6570\u503c\uff0c\u8fd9\u5f88\u660e\u663e\u4e0d\u662f\u4e00\u4e2a\u597d\u529e\u6cd5\u3002\u6211\u4eec\u4f1a\u9047\u5230\u66f4\u590d\u6742\u3001\u66f4\u9ad8\u7ef4\u5ea6\u3001\u66f4\u591a\u53c2\u6570\u7684\u60c5\u51b5\uff0c\u800c\u8fd9\u4e9b\u60c5\u51b5\u662f\u5f88\u96be\u753b\u51fa\u56fe\u7684\uff0c\u56e0\u6b64\u66f4\u65e0\u6cd5\u5c06\u5176\u53ef\u89c6\u5316\uff0c\u56e0\u6b64\u6211\u4eec\u771f\u6b63\u9700\u8981\u7684\u662f\u7f16\u5199\u7a0b\u5e8f\u6765\u627e\u51fa\u8fd9\u4e9b\u6700\u5c0f\u5316\u4ee3\u4ef7\u51fd\u6570\u7684 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u7684\u503c\uff0c\u5728\u4e0b\u4e00\u8282\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u4e00\u79cd\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u5730\u627e\u51fa\u80fd\u4f7f\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6700\u5c0f\u5316\u7684\u53c2\u6570 \\(\\theta_{0}\\) \u548c \\(\\theta_{1}\\) \u7684\u503c\u3002","title":"4. \u4ee3\u4ef7\u51fd\u6570\u7684\u76f4\u89c2\u7406\u89e3II"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#5","text":"\u53c2\u8003\u89c6\u9891: 2 - 5 - Gradient Descent (11 min).mkv \u68af\u5ea6\u4e0b\u964d\u662f\u4e00\u4e2a\u7528\u6765\u6c42\u51fd\u6570\u6700\u5c0f\u503c\u7684\u7b97\u6cd5\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u6c42\u51fa\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_{0}, \\theta_{1})\\) \u7684\u6700\u5c0f\u503c\u3002 \u68af\u5ea6\u4e0b\u964d\u80cc\u540e\u7684\u601d\u60f3\u662f\uff1a\u5f00\u59cb\u65f6\u6211\u4eec\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u53c2\u6570\u7684\u7ec4\u5408 \\(\\left( {\\theta_{0}},{\\theta_{1}},......,{\\theta_{n}} \\right)\\) \uff0c\u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570\uff0c\u7136\u540e\u6211\u4eec\u5bfb\u627e\u4e0b\u4e00\u4e2a\u80fd\u8ba9\u4ee3\u4ef7\u51fd\u6570\u503c\u4e0b\u964d\u6700\u591a\u7684\u53c2\u6570\u7ec4\u5408\u3002\u6211\u4eec\u6301\u7eed\u8fd9\u4e48\u505a\u76f4\u5230\u627e\u5230\u4e00\u4e2a\u5c40\u90e8\u6700\u5c0f\u503c\uff08 local minimum \uff09\uff0c\u56e0\u4e3a\u6211\u4eec\u5e76\u6ca1\u6709\u5c1d\u8bd5\u5b8c\u6240\u6709\u7684\u53c2\u6570\u7ec4\u5408\uff0c\u6240\u4ee5\u4e0d\u80fd\u786e\u5b9a\u6211\u4eec\u5f97\u5230\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u662f\u5426\u4fbf\u662f\u5168\u5c40\u6700\u5c0f\u503c\uff08 global minimum \uff09\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u521d\u59cb\u53c2\u6570\u7ec4\u5408\uff0c\u53ef\u80fd\u4f1a\u627e\u5230\u4e0d\u540c\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002 \u60f3\u8c61\u4e00\u4e0b\u4f60\u6b63\u7ad9\u7acb\u5728\u5c71\u7684\u8fd9\u4e00\u70b9\u4e0a\uff0c\u7ad9\u7acb\u5728\u4f60\u60f3\u8c61\u7684\u516c\u56ed\u8fd9\u5ea7\u7ea2\u8272\u5c71\u4e0a\uff0c\u5728\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u8981\u505a\u7684\u5c31\u662f\u65cb\u8f6c360\u5ea6\uff0c\u770b\u770b\u6211\u4eec\u7684\u5468\u56f4\uff0c\u5e76\u95ee\u81ea\u5df1\u8981\u5728\u67d0\u4e2a\u65b9\u5411\u4e0a\uff0c\u7528\u5c0f\u788e\u6b65\u5c3d\u5feb\u4e0b\u5c71\u3002\u8fd9\u4e9b\u5c0f\u788e\u6b65\u9700\u8981\u671d\u4ec0\u4e48\u65b9\u5411\uff1f\u5982\u679c\u6211\u4eec\u7ad9\u5728\u5c71\u5761\u4e0a\u7684\u8fd9\u4e00\u70b9\uff0c\u4f60\u770b\u4e00\u4e0b\u5468\u56f4\uff0c\u4f60\u4f1a\u53d1\u73b0\u6700\u4f73\u7684\u4e0b\u5c71\u65b9\u5411\uff0c\u4f60\u518d\u770b\u770b\u5468\u56f4\uff0c\u7136\u540e\u518d\u4e00\u6b21\u60f3\u60f3\uff0c\u6211\u5e94\u8be5\u4ece\u4ec0\u4e48\u65b9\u5411\u8fc8\u7740\u5c0f\u788e\u6b65\u4e0b\u5c71\uff1f\u7136\u540e\u4f60\u6309\u7167\u81ea\u5df1\u7684\u5224\u65ad\u53c8\u8fc8\u51fa\u4e00\u6b65\uff0c\u91cd\u590d\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u4ece\u8fd9\u4e2a\u65b0\u7684\u70b9\uff0c\u4f60\u73af\u987e\u56db\u5468\uff0c\u5e76\u51b3\u5b9a\u4ece\u4ec0\u4e48\u65b9\u5411\u5c06\u4f1a\u6700\u5feb\u4e0b\u5c71\uff0c\u7136\u540e\u53c8\u8fc8\u8fdb\u4e86\u4e00\u5c0f\u6b65\uff0c\u5e76\u4f9d\u6b64\u7c7b\u63a8\uff0c\u76f4\u5230\u4f60\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u70b9\u7684\u4f4d\u7f6e\u3002 \u6279\u91cf\u68af\u5ea6\u4e0b\u964d\uff08 batch gradient descent \uff09\u7b97\u6cd5\u7684\u516c\u5f0f\u4e3a\uff1a repeat until convergence { $$ \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1) $$ } \u5176\u4e2d \\(\\alpha\\) \u662f\u5b66\u4e60\u7387\uff08 learning rate \uff09\uff0c\u5b83\u51b3\u5b9a\u4e86\u6211\u4eec\u6cbf\u7740\u80fd\u8ba9\u4ee3\u4ef7\u51fd\u6570\u4e0b\u964d\u7a0b\u5ea6\u6700\u5927\u7684\u65b9\u5411\u5411\u4e0b\u8fc8\u51fa\u7684\u6b65\u5b50\u6709\u591a\u5927\uff0c\u5728\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u4e2d\uff0c\u6211\u4eec\u6bcf\u4e00\u6b21\u90fd\u540c\u65f6\u8ba9\u6240\u6709\u7684\u53c2\u6570\u51cf\u53bb\u5b66\u4e60\u901f\u7387\u4e58\u4ee5\u4ee3\u4ef7\u51fd\u6570\u7684\u5bfc\u6570\u3002 \u5728\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e2d\uff0c\u8fd8\u6709\u4e00\u4e2a\u66f4\u5fae\u5999\u7684\u95ee\u9898\uff0c\u68af\u5ea6\u4e0b\u964d\u4e2d\uff0c\u6211\u4eec\u8981\u66f4\u65b0 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \uff0c\u5f53 \\(j=0\\) \u548c \\(j=1\\) \u65f6\uff0c\u4f1a\u4ea7\u751f\u66f4\u65b0\uff0c\u6240\u4ee5\u4f60\u5c06\u66f4\u65b0 \\(J\\left( {\\theta_{0}} \\right)\\) \u548c \\(J\\left( {\\theta_{1}} \\right)\\) \u3002\u5b9e\u73b0\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u5fae\u5999\u4e4b\u5904\u662f\uff0c\u5728\u8fd9\u4e2a\u8868\u8fbe\u5f0f\u4e2d\uff0c\u5982\u679c\u4f60\u8981\u66f4\u65b0\u8fd9\u4e2a\u7b49\u5f0f\uff0c\u4f60\u9700\u8981\u540c\u65f6\u66f4\u65b0 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \uff0c\u6211\u7684\u610f\u601d\u662f\u5728\u8fd9\u4e2a\u7b49\u5f0f\u4e2d\uff0c\u6211\u4eec\u8981\u8fd9\u6837\u66f4\u65b0\uff1a \\({\\theta_{0}} := {\\theta_{0}}\\) \uff0c\u5e76\u66f4\u65b0 \\({\\theta_{1}}:= {\\theta_{1}}\\) \u3002 \u5b9e\u73b0\u65b9\u6cd5\u662f\uff1a\u4f60\u5e94\u8be5\u8ba1\u7b97\u516c\u5f0f\u53f3\u8fb9\u7684\u90e8\u5206\uff0c\u901a\u8fc7\u90a3\u4e00\u90e8\u5206\u8ba1\u7b97\u51fa \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \u7684\u503c\uff0c\u7136\u540e\u540c\u65f6\u66f4\u65b0 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \u3002 \u8ba9\u6211\u8fdb\u4e00\u6b65\u9610\u8ff0\u8fd9\u4e2a\u8fc7\u7a0b\uff1a \u5728\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e2d\uff0c\u8fd9\u662f\u6b63\u786e\u5b9e\u73b0\u540c\u65f6\u66f4\u65b0\u7684\u65b9\u6cd5\u3002\u6211\u4e0d\u6253\u7b97\u89e3\u91ca\u4e3a\u4ec0\u4e48\u4f60\u9700\u8981\u540c\u65f6\u66f4\u65b0\uff0c\u540c\u65f6\u66f4\u65b0\u662f\u68af\u5ea6\u4e0b\u964d\u4e2d\u7684\u4e00\u79cd\u5e38\u7528\u65b9\u6cd5\u3002\u6211\u4eec\u4e4b\u540e\u4f1a\u8bb2\u5230\uff0c\u540c\u6b65\u66f4\u65b0\u662f\u66f4\u81ea\u7136\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002\u5f53\u4eba\u4eec\u8c08\u5230\u68af\u5ea6\u4e0b\u964d\u65f6\uff0c\u4ed6\u4eec\u7684\u610f\u601d\u5c31\u662f\u540c\u6b65\u66f4\u65b0\u3002\u6ce8\u610f\u4e0a\u9762\u56fe\u5de6\u4e0b\u65b9\u548c\u53f3\u4e0b\u65b9\u7684\u533a\u522b\u3002 \u6ce8\u610f\uff1a \u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u662f\u9700\u8981\u540c\u65f6\u66f4\u65b0 \\({\\theta_{0}}\\) , \\({\\theta_{1}}\\) ,..., \\({\\theta_{n}}\\) \u7684\u3002\u672c\u4f8b\u4e2d\uff0c\u53ea\u6709 \\({\\theta_{0}}\\) \u548c \\({\\theta_{1}}\\) \u3002 \u4e0a\u9762\u56fe\u4e2d\u7528\u4e86 := \u66f4\u65b0 \\(\\theta\\) , \u8fd9\u91cc := \u7b49\u540c\u4e8e\u8ba1\u7b97\u673a\u8bed\u8a00\u4e2d\u7684\u8d4b\u503c\u64cd\u4f5c\u3002 \u5728\u63a5\u4e0b\u6765\u7684\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u8981\u8fdb\u5165\u8fd9\u4e2a\u5fae\u5206\u9879\u7684\u7ec6\u8282\u4e4b\u4e2d\u3002\u6211\u5df2\u7ecf\u5199\u4e86\u51fa\u6765\u4f46\u6ca1\u6709\u771f\u6b63\u5b9a\u4e49\uff0c\u5982\u679c\u4f60\u5df2\u7ecf\u4fee\u8fc7\u5fae\u79ef\u5206\u8bfe\u7a0b\uff0c\u5982\u679c\u4f60\u719f\u6089\u504f\u5bfc\u6570\u548c\u5bfc\u6570\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f\u8fd9\u4e2a\u5fae\u5206\u9879\uff1a \\(\\alpha \\frac{\\partial }{\\partial {{\\theta }_{0}}}J({{\\theta }_{0}},{{\\theta }_{1}})\\) \uff0c \\(\\alpha \\frac{\\partial }{\\partial {{\\theta }_{1}}}J({{\\theta }_{0}},{{\\theta }_{1}})\\) \u3002","title":"5. \u68af\u5ea6\u4e0b\u964d"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#6","text":"\u53c2\u8003\u89c6\u9891: 2 - 6 - Gradient Descent Intuition (12 min).mkv \u5728\u4e4b\u524d\u7684\u89c6\u9891\u4e2d\uff0c\u6211\u4eec\u7ed9\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u4e0a\u5173\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5b9a\u4e49\uff0c\u672c\u6b21\u89c6\u9891\u6211\u4eec\u66f4\u6df1\u5165\u7814\u7a76\u4e00\u4e0b\uff0c\u66f4\u76f4\u89c2\u5730\u611f\u53d7\u4e00\u4e0b\u8fd9\u4e2a\u7b97\u6cd5\u662f\u505a\u4ec0\u4e48\u7684\uff0c\u4ee5\u53ca\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u66f4\u65b0\u8fc7\u7a0b\u6709\u4ec0\u4e48\u610f\u4e49\u3002\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u5982\u4e0b\uff1a \\({\\theta_{j}}:={\\theta_{j}}-\\alpha \\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)\\) \u63cf\u8ff0\uff1a\u5bf9 \\(\\theta\\) \u8d4b\u503c\uff0c\u4f7f\u5f97 \\(J\\left( \\theta \\right)\\) \u6309\u68af\u5ea6\u4e0b\u964d\u6700\u5feb\u65b9\u5411\u8fdb\u884c\uff0c\u4e00\u76f4\u8fed\u4ee3\u4e0b\u53bb\uff0c\u6700\u7ec8\u5f97\u5230\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u5176\u4e2d \\(\\alpha\\) \u662f\u5b66\u4e60\u7387\uff08 learning rate \uff09\uff0c\u5b83\u51b3\u5b9a\u4e86\u6211\u4eec\u6cbf\u7740\u80fd\u8ba9\u4ee3\u4ef7\u51fd\u6570\u4e0b\u964d\u7a0b\u5ea6\u6700\u5927\u7684\u65b9\u5411\u5411\u4e0b\u8fc8\u51fa\u7684\u6b65\u5b50\u6709\u591a\u5927\u3002 \u4ece\u4e0a\u56fe\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u65e0\u8bba \\(\\theta\\) \u8d77\u59cb\u70b9\u4ece\u6b63\u7684\u8fd8\u662f\u8d1f\u7684\u5f00\u59cb\u51fa\u53d1\uff0c\u4ed6\u4eec\u90fd\u4f1a\u5411\u7740 \\(J(\\theta)\\) \u53d8\u5c0f\u7684\u65b9\u5411\u51fa\u53d1\u3002 \u8ba9\u6211\u4eec\u6765\u770b\u770b\u5982\u679c \\(\\alpha\\) \u592a\u5c0f\u6216 \\(\\alpha\\) \u592a\u5927\u4f1a\u51fa\u73b0\u4ec0\u4e48\u60c5\u51b5\uff1a \u5982\u679c \\(\\alpha\\) \u592a\u5c0f\u4e86\uff0c\u5373\u6211\u7684\u5b66\u4e60\u901f\u7387\u592a\u5c0f\uff0c\u7ed3\u679c\u5c31\u662f\u53ea\u80fd\u8fd9\u6837\u50cf\u5c0f\u5b9d\u5b9d\u4e00\u6837\u4e00\u70b9\u70b9\u5730\u632a\u52a8\uff0c\u53bb\u52aa\u529b\u63a5\u8fd1\u6700\u4f4e\u70b9\uff0c\u8fd9\u6837\u5c31\u9700\u8981\u5f88\u591a\u6b65\u624d\u80fd\u5230\u8fbe\u6700\u4f4e\u70b9\uff0c\u6240\u4ee5\u5982\u679c \\(\\alpha\\) \u592a\u5c0f\u7684\u8bdd\uff0c\u53ef\u80fd\u4f1a\u5f88\u6162\uff0c\u56e0\u4e3a\u5b83\u4f1a\u4e00\u70b9\u70b9\u632a\u52a8\uff0c\u5b83\u4f1a\u9700\u8981\u5f88\u591a\u6b65\u624d\u80fd\u5230\u8fbe\u5168\u5c40\u6700\u4f4e\u70b9\u3002 \u5982\u679c \\(\\alpha\\) \u592a\u5927\uff0c\u90a3\u4e48\u68af\u5ea6\u4e0b\u964d\u6cd5\u53ef\u80fd\u4f1a\u8d8a\u8fc7\u6700\u4f4e\u70b9\uff0c\u751a\u81f3\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\uff0c\u4e0b\u4e00\u6b21\u8fed\u4ee3\u53c8\u79fb\u52a8\u4e86\u4e00\u5927\u6b65\uff0c\u8d8a\u8fc7\u4e00\u6b21\uff0c\u53c8\u8d8a\u8fc7\u4e00\u6b21\uff0c\u4e00\u6b21\u6b21\u8d8a\u8fc7\u6700\u4f4e\u70b9\uff0c\u76f4\u5230\u4f60\u53d1\u73b0\u5b9e\u9645\u4e0a\u79bb\u6700\u4f4e\u70b9\u8d8a\u6765\u8d8a\u8fdc\uff0c\u6240\u4ee5\uff0c\u5982\u679c \\(\\alpha\\) \u592a\u5927\uff0c\u5b83\u4f1a\u5bfc\u81f4\u65e0\u6cd5\u6536\u655b\uff0c\u751a\u81f3\u53d1\u6563\u3002 \u5047\u8bbe\u4f60\u5c06 \\({\\theta_{1}}\\) \u521d\u59cb\u5316\u5728\u5c40\u90e8\u6700\u4f4e\u70b9\u3002\u7ed3\u679c\u662f\u5c40\u90e8\u6700\u4f18\u70b9\u7684\u5bfc\u6570\u5c06\u7b49\u4e8e\u96f6\uff0c\u56e0\u4e3a\u5b83\u662f\u90a3\u6761\u5207\u7ebf\u7684\u659c\u7387\u3002\u8fd9\u610f\u5473\u7740\u4f60\u5df2\u7ecf\u5728\u5c40\u90e8\u6700\u4f18\u70b9\uff0c\u5b83\u4f7f\u5f97 \\({\\theta_{1}}\\) \u4e0d\u518d\u6539\u53d8\uff0c\u4e5f\u5c31\u662f\u65b0\u7684 \\({\\theta_{1}}\\) \u7b49\u4e8e\u539f\u6765\u7684 \\({\\theta_{1}}\\) \uff0c\u56e0\u6b64\uff0c\u5982\u679c\u4f60\u7684\u53c2\u6570\u5df2\u7ecf\u5904\u4e8e\u5c40\u90e8\u6700\u4f4e\u70b9\uff0c\u90a3\u4e48\u68af\u5ea6\u4e0b\u964d\u6cd5\u66f4\u65b0\u5176\u5b9e\u4ec0\u4e48\u90fd\u6ca1\u505a\uff0c\u5b83\u4e0d\u4f1a\u6539\u53d8\u53c2\u6570\u7684\u503c\u3002 \u6211\u4eec\u6765\u770b\u4e00\u4e2a\u4f8b\u5b50\uff0c\u8fd9\u662f\u4ee3\u4ef7\u51fd\u6570 \\(J\\left( \\theta \\right)\\) \u3002 \u6211\u60f3\u627e\u5230\u5b83\u7684\u6700\u5c0f\u503c\uff0c\u9996\u5148\u521d\u59cb\u5316\u6211\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u5728\u90a3\u4e2a\u54c1\u7ea2\u8272\u7684\u70b9\u521d\u59cb\u5316\uff0c\u5982\u679c\u6211\u66f4\u65b0\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\uff0c\u4e5f\u8bb8\u5b83\u4f1a\u5e26\u6211\u5230\u8fd9\u4e2a\u70b9\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u70b9\u7684\u5bfc\u6570\u662f\u76f8\u5f53\u9661\u7684\u3002\u73b0\u5728\uff0c\u5728\u8fd9\u4e2a\u7eff\u8272\u7684\u70b9\uff0c\u5982\u679c\u6211\u518d\u66f4\u65b0\u4e00\u6b65\uff0c\u4f60\u4f1a\u53d1\u73b0\u6211\u7684\u5bfc\u6570\uff0c\u4e5f\u5373\u659c\u7387\uff0c\u662f\u6ca1\u90a3\u4e48\u9661\u7684\u3002\u968f\u7740\u6211\u63a5\u8fd1\u6700\u4f4e\u70b9\uff0c\u6211\u7684\u5bfc\u6570\u8d8a\u6765\u8d8a\u63a5\u8fd1\u96f6\uff0c\u6240\u4ee5\uff0c\u68af\u5ea6\u4e0b\u964d\u4e00\u6b65\u540e\uff0c\u65b0\u7684\u5bfc\u6570\u4f1a\u53d8\u5c0f\u4e00\u70b9\u70b9\u3002\u7136\u540e\u6211\u60f3\u518d\u68af\u5ea6\u4e0b\u964d\u4e00\u6b65\uff0c\u5728\u8fd9\u4e2a\u7eff\u70b9\uff0c\u6211\u81ea\u7136\u4f1a\u7528\u4e00\u4e2a\u7a0d\u5fae\u8ddf\u521a\u624d\u5728\u90a3\u4e2a\u54c1\u7ea2\u70b9\u65f6\u6bd4\uff0c\u518d\u5c0f\u4e00\u70b9\u7684\u4e00\u6b65\uff0c\u5230\u4e86\u65b0\u7684\u7ea2\u8272\u70b9\uff0c\u66f4\u63a5\u8fd1\u5168\u5c40\u6700\u4f4e\u70b9\u4e86\uff0c\u56e0\u6b64\u8fd9\u70b9\u7684\u5bfc\u6570\u4f1a\u6bd4\u5728\u7eff\u70b9\u65f6\u66f4\u5c0f\u3002\u6240\u4ee5\uff0c\u6211\u518d\u8fdb\u884c\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\u65f6\uff0c\u6211\u7684\u5bfc\u6570\u9879\u662f\u66f4\u5c0f\u7684\uff0c \\({\\theta_{1}}\\) \u66f4\u65b0\u7684\u5e45\u5ea6\u5c31\u4f1a\u66f4\u5c0f\u3002\u6240\u4ee5 \u968f\u7740\u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u8fd0\u884c\uff0c\u4f60\u79fb\u52a8\u7684\u5e45\u5ea6\u4f1a\u81ea\u52a8\u53d8\u5f97\u8d8a\u6765\u8d8a\u5c0f\uff0c\u76f4\u5230\u6700\u7ec8\u79fb\u52a8\u5e45\u5ea6\u975e\u5e38\u5c0f\uff0c\u4f60\u4f1a\u53d1\u73b0\u5df2\u7ecf\u6536\u655b\u5230\u5c40\u90e8\u6781\u5c0f\u503c \u3002 \u56de\u987e\u4e00\u4e0b\uff0c\u5728\u68af\u5ea6\u4e0b\u964d\u6cd5\u4e2d\uff0c\u5f53\u6211\u4eec\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u70b9\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u4f1a\u81ea\u52a8\u91c7\u53d6\u66f4\u5c0f\u7684\u5e45\u5ea6\uff0c\u8fd9\u662f\u56e0\u4e3a\u5f53\u6211\u4eec\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u70b9\u65f6\uff0c\u5f88\u663e\u7136\u5728\u5c40\u90e8\u6700\u4f4e\u65f6\u5bfc\u6570\u7b49\u4e8e\u96f6\uff0c\u6240\u4ee5\u5f53\u6211\u4eec\u63a5\u8fd1\u5c40\u90e8\u6700\u4f4e\u65f6\uff0c\u5bfc\u6570\u503c\u4f1a\u81ea\u52a8\u53d8\u5f97\u8d8a\u6765\u8d8a\u5c0f\uff0c\u6240\u4ee5\u68af\u5ea6\u4e0b\u964d\u5c06\u81ea\u52a8\u91c7\u53d6\u8f83\u5c0f\u7684\u5e45\u5ea6\uff0c\u8fd9\u5c31\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u505a\u6cd5\u3002\u6240\u4ee5\u5b9e\u9645\u4e0a\u6ca1\u6709\u5fc5\u8981\u518d\u53e6\u5916\u51cf\u5c0f \\(a\\) \u3002 \u8fd9\u5c31\u662f\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u7528\u5b83\u6765\u6700\u5c0f\u5316\u4efb\u4f55\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \uff0c\u4e0d\u53ea\u662f\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u3002","title":"6. \u68af\u5ea6\u4e0b\u964d\u7684\u76f4\u89c2\u7406\u89e3"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#7","text":"\u53c2\u8003\u89c6\u9891: 2 - 7 - GradientDescentForLinearRegression (6 min).mkv \u8fd9\u662f\u6211\u4eec\u4e4b\u524d\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u73b0\u5728\u5c1d\u8bd5\u5c06\u68af\u5ea6\u4e0b\u964d\u6cd5\u5e94\u7528\u5230\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002 \u6211\u4eec\u7684Goal\uff1a \\(\\mathop{minimize} \\limits_{\\theta_0,\\ \\theta_1}J(\\theta_0,\\theta_1)\\) \u4ed4\u7ec6\u89c2\u5bdf\uff0c\u6211\u4eec\u4f1a\u53d1\u73b0\u89e3\u51b3\u8fd9\u4e2a\u76ee\u6807\u7684\u5173\u952e\u5c31\u662f\u8fd9\u4e2a\u5bfc\u6570\u9879 \\(\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)\\) \u3002 \\(\\frac{\\partial }{\\partial {{\\theta }_{j}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{\\partial }{\\partial {{\\theta }_{j}}}\\frac{1}{2m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}^{2}}\\) \u600e\u4e48\u4ece\u4e0a\u5f0f\u53d8\u5230\u4e0b\u9762\u4e24\u4e2a\u5f0f\u5b50\uff0c\u8bf7\u770b\u4e0b\u4e00\u7ae0\u8282\uff0c\u6709\u5177\u4f53\u63a8\u5bfc\u3002 \\(j=0\\) \u65f6\uff1a \\(\\frac{\\partial }{\\partial {{\\theta }_{0}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}}\\) \\(j=1\\) \u65f6\uff1a \\(\\frac{\\partial }{\\partial {{\\theta }_{1}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}\\) \u5219\u7b97\u6cd5\u6539\u5199\u6210\uff1a Repeat { \u200b \\({\\theta_{0}}:={\\theta_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{ \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}\\) \u200b \\({\\theta_{1}}:={\\theta_{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}\\) \u200b } \u91cd\u8981\u7ed3\u8bba\uff1a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u91cc\u7684\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u603b\u662f\u5f13\u72b6\u51fd\u6570\uff0c\u53c8\u79f0\u51f8\u51fd\u6570( convex function )\u3002\u5982\u4e0b\u56fe\uff1a \u8fd9\u4e2a\u51fd\u6570\u6ca1\u6709\u5c40\u90e8\u6700\u4f18(local optima)\uff0c\u53ea\u6709\u5168\u5c40\u6700\u4f18(global optimum)\u3002\u5f53\u6211\u4eec\u7528\u68af\u5ea6\u4e0b\u964d\u53bb\u8ba1\u7b97\u7684\u65f6\u5019\uff0c\u4ed6\u603b\u80fd\u591f\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u3002 \u6211\u4eec\u521a\u521a\u4f7f\u7528\u7684\u7b97\u6cd5\uff0c\u6709\u65f6\u4e5f\u79f0\u4e3a\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u3002\u5b9e\u9645\u4e0a\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u901a\u5e38\u4e0d\u592a\u4f1a\u7ed9\u7b97\u6cd5\u8d77\u540d\u5b57\uff0c\u4f46\u8fd9\u4e2a\u540d\u5b57\u201d \u6279\u91cf\u68af\u5ea6\u4e0b\u964d \u201d\uff0c\u6307\u7684\u662f\u5728\u68af\u5ea6\u4e0b\u964d\u7684\u6bcf\u4e00\u6b65\u4e2d\uff0c\u6211\u4eec\u90fd\u7528\u5230\u4e86\u6240\u6709\u7684\u8bad\u7ec3\u6837\u672c\u3002\u5728\u5e94\u7528\u68af\u5ea6\u4e0b\u964d\uff0c\u5e76\u8ba1\u7b97\u504f\u5bfc\u6570\u65f6\uff0c\u6211\u4eec\u90fd\u9700\u8981\u8ba1\u7b97 \\(\\sum\\limits_{i=1}^{m}\\Big(h_\\theta(x)-y\\Big)\\) \u3002\u56e0\u6b64\uff0c\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u6cd5\u8fd9\u4e2a\u540d\u5b57\u8bf4\u660e\u4e86\u6211\u4eec\u9700\u8981\u8003\u8651 \u6240\u6709 \u8fd9\u4e00\"\u6279\"\u8bad\u7ec3\u6837\u672c\uff0c\u800c\u4e8b\u5b9e\u4e0a\uff0c\u6709\u65f6\u4e5f\u6709\u5176\u4ed6\u7c7b\u578b\u7684\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u4e0d\u662f\u8fd9\u79cd\"\u6279\u91cf\"\u578b\u7684\uff0c\u4e0d\u8003\u8651\u6574\u4e2a\u7684\u8bad\u7ec3\u96c6\uff0c\u800c\u662f\u6bcf\u6b21\u53ea\u5173\u6ce8\u8bad\u7ec3\u96c6\u4e2d\u7684\u4e00\u4e9b\u5c0f\u7684\u5b50\u96c6\u3002\u5728\u540e\u9762\u7684\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e5f\u5c06\u4ecb\u7ecd\u8fd9\u4e9b\u65b9\u6cd5\u3002 \u5982\u679c\u4f60\u4e4b\u524d\u5b66\u8fc7\u7ebf\u6027\u4ee3\u6570\uff0c\u6709\u4e9b\u540c\u5b66\u4e4b\u524d\u53ef\u80fd\u5df2\u7ecf\u5b66\u8fc7\u9ad8\u7b49\u7ebf\u6027\u4ee3\u6570\uff0c\u4f60\u5e94\u8be5\u77e5\u9053\u6709\u4e00\u79cd\u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u6700\u5c0f\u503c\u7684\u6570\u503c\u89e3\u6cd5\uff0c\u4e0d\u9700\u8981\u68af\u5ea6\u4e0b\u964d\u8fd9\u79cd\u8fed\u4ee3\u7b97\u6cd5\u3002\u5728\u540e\u9762\u7684\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e5f\u4f1a\u8c08\u5230\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u591a\u6b65\u68af\u5ea6\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u89e3\u51fa\u4ee3\u4ef7\u51fd\u6570 \\(J\\) \u7684\u6700\u5c0f\u503c\uff0c\u8fd9\u662f\u53e6\u4e00\u79cd\u79f0\u4e3a\u6b63\u89c4\u65b9\u7a0b( normal equations )\u7684\u65b9\u6cd5\u3002\u5b9e\u9645\u4e0a\u5728\u6570\u636e\u91cf\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u6bd4\u6b63\u89c4\u65b9\u7a0b\u8981\u66f4\u9002\u7528\u4e00\u4e9b\u3002 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u638c\u63e1\u4e86\u68af\u5ea6\u4e0b\u964d\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u6211\u4eec\u8fd8\u5c06\u5728\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e2d\u5927\u91cf\u5730\u4f7f\u7528\u5b83\u3002\u6240\u4ee5\uff0c\u795d\u8d3a\u81ea\u5df1 \u6210\u529f\u5b66\u4f1a\u4e86\u7b2c\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002 \u5148\u5199\u5230\u8fd9\u91cc\uff0c\u540e\u9762\u6253\u7b97\u9644\u4e0a\u4e60\u9898\u548c\u5b9e\u64cdpython\u4ee3\u7801\u3002","title":"7. \u68af\u5ea6\u4e0b\u964d\u5e94\u7528\u81f3\u7ebf\u6027\u56de\u5f52"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_1","text":"","title":"\u4e60\u9898 &amp;&amp; \u53c2\u8003\u7b54\u6848"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_2","text":"\u57fa\u4e8e\u4e00\u4e2a\u5b66\u751f\u5728\u5927\u5b66\u4e00\u5e74\u7ea7\u7684\u8868\u73b0\uff0c\u9884\u6d4b\u4ed6\u5728\u5927\u5b66\u4e8c\u5e74\u7ea7\u8868\u73b0\u3002 \u4ee4x\u7b49\u4e8e\u5b66\u751f\u5728\u5927\u5b66\u7b2c\u4e00\u5e74\u5f97\u5230\u7684\u201cA\u201d\u7684\u4e2a\u6570\uff08\u5305\u62ecA-\uff0cA\u548cA+\u6210\u7ee9\uff09\u5b66\u751f\u5728\u5927\u5b66\u7b2c\u4e00\u5e74\u5f97\u5230\u7684\u6210\u7ee9\u3002\u9884\u6d4by\u7684\u503c\uff1a\u7b2c\u4e8c\u5e74\u83b7\u5f97\u7684\u201cA\u201d\u7ea7\u7684\u6570\u91cf \u8fd9\u91cc\u6bcf\u4e00\u884c\u662f\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u3002\u5728\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u6211\u4eec\u7684\u5047\u8bbe \\(h_\\theta(x)=\\theta_0+\\theta_1x\\) \uff0c\u5e76\u4e14\u6211\u4eec\u4f7f\u7528 m \u6765\u8868\u793a\u8bad\u7ec3\u793a\u4f8b\u7684\u6570\u91cf\u3002 x y 3 2 1 2 0 1 4 3 \u5bf9\u4e8e\u4e0a\u9762\u7ed9\u51fa\u7684\u8bad\u7ec3\u96c6 \uff08\u6ce8\u610f\uff0c\u6b64\u8bad\u7ec3\u96c6\u4e5f\u53ef\u4ee5\u5728\u672c\u6d4b\u9a8c\u7684\u5176\u4ed6\u95ee\u9898\u4e2d\u5f15\u7528\uff09\uff0c m \u7684\u503c\u662f\u591a\u5c11 \uff1f","title":"\u7b2c\u4e00\u9898"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_3","text":"\u5047\u8bbe\u6211\u4eec\u4f7f\u7528\u7b2c\u4e00\u9898\u4e2d\u7684\u8bad\u7ec3\u96c6\u3002\u5e76\u4e14\uff0c\u6211\u4eec\u4ee3\u4ef7\u51fd\u6570\u7684\u5b9a\u4e49\u662f \\(J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}\\big(h_\\theta(x^{(i)})-y^{(i)}\\big)^2\\) \u6c42 \\(J(0, 1)\\) ?","title":"\u7b2c\u4e8c\u9898"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_4","text":"\u4ee4\u95ee\u98981\u4e2d\uff0c\u7ebf\u6027\u56de\u5f52\u5047\u8bbe\u7684 \\(\\theta_0=-1, \\theta_1=2\\) , \u6c42 \\(h_\\theta(6)\\) ?","title":"\u7b2c\u4e09\u9898"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_5","text":"\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u4e0e \\(\\theta_0, \\theta_1\\) \u7684\u5173\u7cfb\u5982\u4e0b\u56fe\u6240\u793a\u3002\u56fe\u4e2d\u4e2d\u7ed9\u51fa\u4e86\u76f8\u540c\u4ee3\u4ef7\u51fd\u6570\u7684\u7b49\u9ad8\u7ebf\u56fe\u3002\u6839\u636e\u56fe\u793a\uff0c\u9009\u62e9\u6b63\u786e\u7684\u9009\u9879\uff08\u9009\u51fa\u6240\u6709\u6b63\u786e\u9879\uff09 A. \u4eceB\u70b9\u5f00\u59cb\uff0c\u5b66\u4e60\u7387\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u6700\u7ec8\u5e2e\u52a9\u6211\u4eec\u5230\u8fbe\u6216\u8005\u63a5\u8fd1A\u70b9\uff0c\u5373\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u5728A\u70b9\u6709\u6700\u5c0f\u503c B. \u70b9P\uff08\u56fe2\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff09\u5bf9\u5e94\u4e8e\u56fe1\u7684\u70b9C C. \u4eceB\u70b9\u5f00\u59cb\uff0c\u5b66\u4e60\u7387\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u6700\u7ec8\u5e2e\u52a9\u6211\u4eec\u5230\u8fbe\u6216\u8005\u63a5\u8fd1C\u70b9\uff0c\u5373\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u5728C\u70b9\u6709\u6700\u5c0f\u503c D. \u4eceB\u70b9\u5f00\u59cb\uff0c\u5b66\u4e60\u7387\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u6700\u7ec8\u5e2e\u52a9\u6211\u4eec\u5230\u8fbe\u6216\u8005\u63a5\u8fd1A\u70b9\uff0c\u5373\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta_0, \\theta_1)\\) \u5728A\u70b9\u6709\u6700\u5927\u503c E. \u70b9P\uff08\u56fe2\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff09\u5bf9\u5e94\u4e8e\u56fe1\u7684\u70b9A","title":"\u7b2c\u56db\u9898"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_6","text":"\u5047\u8bbe\u5bf9\u4e8e\u67d0\u4e2a\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff08\u6bd4\u5982\u9884\u6d4b\u623f\u4ef7\uff09\uff0c\u6211\u4eec\u6709\u4e00\u4e9b\u8bad\u7ec3\u96c6\uff0c\u5bf9\u4e8e\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u80fd\u591f\u627e\u5230\u4e00\u4e9b \\(\\theta_0, \\theta_1\\) \uff0c\u4f7f\u5f97 \\(J(\\theta_0, \\theta_1)=0\\) \u3002\u4ee5\u4e0b\u54ea\u9879\u9648\u8ff0\u662f\u6b63\u786e\u7684\uff1f\uff08\u9009\u51fa\u6240\u6709\u6b63\u786e\u9879\uff09 A. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5fc5\u987b\u6709 \\(\\theta_0=0, \\theta_1=0\\) \uff0c\u8fd9\u6837\u624d\u80fd\u4f7f \\(J(\\theta_0, \\theta_1) = 0\\) B. \u5bf9\u4e8e\u6ee1\u8db3 \\(J(\\theta_0, \\theta_1) = 0\\) \u7684 \\(\\theta_0, \\theta_1\\) \u7684\u503c\uff0c\u5176\u5bf9\u4e8e\u6bcf\u4e2a\u8bad\u7ec3\u4f8b\u5b50 \\((x^{(i)},y^{(i)})\\) \uff0c\u90fd\u6709 \\(h_\\theta(x^{(i)})=y^{(i)}\\) C. \u8fd9\u662f\u4e0d\u53ef\u80fd\u7684\uff1a\u901a\u8fc7 \\(J(\\theta_0, \\theta_1) = 0\\) \u7684\u5b9a\u4e49\uff0c\u4e0d\u53ef\u80fd\u5b58\u5728 \\(\\theta_0, \\theta_1\\) \u4f7f\u5f97 \\(J(\\theta_0, \\theta_1) = 0\\) D. \u5373\u4f7f\u5bf9\u4e8e\u6211\u4eec\u8fd8\u6ca1\u6709\u770b\u5230\u7684\u65b0\u4f8b\u5b50\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u5b8c\u7f8e\u5730\u9884\u6d4by\u7684\u503c\uff08\u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u5b8c\u7f8e\u5730\u9884\u6d4b\u6211\u4eec\u5c1a\u672a\u89c1\u8fc7\u7684\u65b0\u623f\u7684\u4ef7\u683c\uff09","title":"\u7b2c\u4e94\u9898"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_7","text":"\u7b2c\u4e00\u9898\uff1a4 \u7b2c\u4e8c\u9898\uff1a0.5 \u7531\u5df2\u77e5\u6c42 \\(J(0, 1)\\) , \u53ef\u5f97\uff1a \\(\\theta_0=0, \\theta_0=1\\) \u6240\u4ee5 \\(h_\\theta(x)=0+1\\cdot x=x\\) \u6700\u540e, \u5c06\u4e0a\u8ff0\u7ed3\u679c\u548c\u8bad\u7ec3\u96c6\u6570\u636e\u4ee3\u5165\u53ef\u5f97\uff1a \\[ \\begin{split} J(0, 1)=\\frac{1}{2*4}[ & (3-2)^2 + (1-2)^2+ \\\\\\\\ & (0-1)^2+(4-3)^2]=0.5 \\end{split} \\] \u7b2c\u4e09\u9898\uff1a11 \u5df2\u77e5, \\(h_\\theta(x)=\\theta_0+\\theta_1x\\) , \\(\\theta_0=-1, \\theta_1=2\\) \u6240\u4ee5\u5c06 \\(\\theta_0, \\theta_1\\) \u4ee3\u5165\u53ef\u5f97 \\(h_\\theta(x)=-1+2x\\) \u8ba9\u6211\u4eec\u6c42 \\(h_\\theta(6)\\) \u6700\u540e, \\(h_\\theta(6)=-1+2*6=11\\) \u7b2c\u56db\u9898\uff1aAE \u7b2c\u4e94\u9898\uff1aB","title":"\u53c2\u8003\u7b54\u6848"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#_8","text":"p { text-align: justify; /*\u6587\u672c\u4e24\u7aef\u5bf9\u9f50*/ } In this part of exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and popularations from the cities. \u2002 you would like to use this data to help you select which city to expand to next. \u2002 The file ex1data.txt contains the dataset for our linear regression problem. The first column is the popularations of a city and the second column is the profits of a food truck in that city. A negative value for profits indicates a loss.","title":"\u4e0a\u673a\u7ec3\u4e60"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#1plotting-the-data","text":"Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only properties to plot (profit and popularation). (Many other problems that you will encounter in real life are multi-dimensional and can't be plotted on 2-d plot). \u4ee3\u7801\u662f\u5728jupyter notebook\u4e0a\u6267\u884c\uff01\uff01\uff01 \u8bf7\u5c06\u6bcf\u4e2a\u4ee3\u7801\u5757\u6309\u987a\u5e8f\u62f7\u8d1d\u5230cell\u4e2d\u6267\u884c\u3002 \u4ee3\u7801\u57571 \u5bfc\u5165\u5fc5\u8981\u7684\u5e93\u548c\u6570\u636e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 0. \u5bfc\u5165\u9700\u8981\u7684\u5e93 import numpy as np import pandas as pd import matplotlib.pyplot as plt # 1. \u5bfc\u5165\u6570\u636e path = 'ex1data1.txt' # 1.1 \u4f7f\u7528pandas\u4e2d\u7684read_csv \u63a5\u6536\u6570\u636e ''' \u6ce8\u610f: header=None: \u662f\u6307\u6211\u4eec\u8bfb\u53d6\u7684\u539f\u59cb\u6587\u4ef6\u6570\u636e\u6ca1\u6709\u5217\u7d22\u5f15 names: \u6307\u5b9a\u65b0\u5217\u540d ''' data = pd . read_csv ( path , header = None , names = [ 'Population' , 'Profit' ]) # 1.2 \u5c55\u793a\u524d\u4e94\u884c\u6570\u636e data . head () \u8fd4\u56de\u7ed3\u679c: Population Profit 0 6.1101 17.5920 1 5.5277 9.1302 2 8.5186 13.6620 3 7.0032 11.8540 4 5.8598 6.8233 \u7528matplotlib\u753b\u56fe 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 1. \u521b\u5efa\u753b\u5e03 plt . figure ( figsize = ( 12 , 8 ), dpi = 100 ) # 2. \u7ed8\u5236\u6563\u70b9\u56fe plt . scatter ( data [ \"Population\" ], data [ \"Profit\" ], color = \"r\" , label = \"Training data\" ) # 2.1 \u6dfb\u52a0\u63cf\u8ff0\u4fe1\u606f plt . xlabel ( \"Population of City in 10,000s\" ) plt . ylabel ( \"Profit in $10,000s\" ) # 2.2 \u6dfb\u52a0\u7f51\u683c\u663e\u793a plt . grid ( True , linestyle = \"--\" , alpha = 0.5 ) # 2.3 \u6dfb\u52a0\u56fe\u4f8b plt . legend ( loc = 7 ) # 3.\u663e\u793a\u56fe\u50cf plt . show () \u8f93\u51fa\u56fe\u50cf\uff1a","title":"1\u3001Plotting the Data"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#2gradient-descent","text":"In this part you will fit linear regression parameters \\(\\theta\\) to our dataset using gradient descent.","title":"2\u3001Gradient Descent"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#21-update-equations","text":"The objective of linear regression is to minimize the cost function \\[ J(\\theta)=\\sum_{i=1}^m{h_ \\theta(x^{(i)}-y^{(i)})}^2 \\] where the hypothesis \\(h_\\theta(x)\\) is given by the linear model \\[ h_\\theta(x) = \\theta^Tx=\\theta_0+\\theta_1x_1 \\] \u2002 Recall that the parameters of your model are the \\(\\theta_j\\) values. These are the values you will adjust to minimize cost \\(J(\\theta)\\) . One way to do this is to use the batch descent algorithm. In batch gradient descent, each iteration performs the update \\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m{h_ \\theta(x^{(i)}-y^{(i)})x_j^{(i)}} \\\\\\\\ (simultaneously\\ update\\ \\theta_j\\ for\\ all\\ j ) \\] \u2002 With each step of gradient descent, your parameters \\(\\theta_j\\) come closer to the optomal values that will achieve that the lowest cost \\(J(\\theta)\\) Implementation Note To take into accout the intercept term \\((\\theta_0)\\) , we add an additional first column to X and set it to all ones. This allows us to treat \\(\\theta_0\\) as simply another 'feature'.","title":"2.1 Update Equations"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#22-implementation","text":"In \u4ee3\u7801\u57571, we have already set up the data for linear regression. In the following lines, we add another dimension to our data to accommodate the \\(\\theta_0\\) intercept term. We also initialize the initial parameters to 0 and the learning rate alpha to 0.01. \u4ee3\u7801\u57572.1 \u8bbe\u7f6eX\u548cy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # \u8fd9\u4e00\u6b65\u76f8\u5f53\u4e8e\u8bbe\u7f6e\u6240\u6709 x_0=1 data . insert ( 0 , 'Ones' , 1 ) # \u53ef\u4ee5\u5148\u770b\u4e00\u4e0b\u76ee\u524ddata.shape # data.shape ## \u5148\u83b7\u53d6data\u603b\u5217\u6570 cols = data . shape [ 1 ] # \u4ecedata\u4e2d\u53d6\u5230X\u548cy # X : training data # y : target variable X = data . iloc [:, 0 : cols - 1 ] # X\u53d6\u6240\u6709\u884c, \u53bb\u6700\u540e\u4e00\u5217 y = data . iloc [:, cols - 1 ] # y\u53d6\u6240\u6709\u884c, \u6700\u540e\u4e00\u5217 # \u6ce8\u610f\u8fd9\u91cc\u7684X, y\u4ecd\u7136\u5305\u542b\u884c\u7d22\u5f15\u548c\u5217\u7d22\u5f15\u503c \u8f93\u51fa\u5e76\u89c2\u5bdf\u4e0b X (\u8bad\u7ec3\u96c6) and y (\u76ee\u6807\u53d8\u91cf)\u662f\u5426\u6b63\u786e\u3002 \u8f93\u5165 [1]\uff1a X . head () \u8f93\u51fa [1]\uff1a Ones Population 0 1 6.1101 1 1 5.5277 2 1 8.5186 3 1 7.0032 4 1 5.8598 \u8f93\u5165 [2]\uff1a y . head () \u8f93\u51fa [2]\uff1a 0 17.5920 1 9.1302 2 13.6620 3 11.8540 4 6.8233 Name : Profit , dtype : float64 \u4ee3\u7801\u57572.2 \u5c06X\u548cy\u8f6c\u6210ndarray \u5e76\u8bbe\u7f6e\u5b66\u4e60\u7387\u548c\u8fed\u4ee3\u6b21\u6570 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # \u4ecedataframe \u53d6\u51favalue X = X . value y = y . value # \u53ef\u4ee5\u68c0\u9a8c\u4e00\u4e0b X\u548cy\u7684shape # X = X.shape # Y = y.shape # \u5c06X\u548cy\u8f6c\u5316\u6210numpy array \u52a0\u5feb\u8ba1\u7b97\u901f\u5ea6 X = np . array ( X ) # reshape\u7684\u4f5c\u7528\u662f\u5c06y\u8f6c\u6210\u4e0d\u7ba1\u884c, \u5217\u6570\u4e3a1\u7684\u77e9\u9635 y = np . array ( y ) # \u8bbe\u7f6e\u8fed\u4ee3\u6b21\u6570 iterations = 1500 # \u8bbe\u7f6e\u5b66\u4e60\u7387 alpha = 0.01 # \u521d\u59cb\u5316theta theta = np . zeros ([ 2 , ]) # \u8bbe\u7f6e\u5b8c\u6210\u540e\uff0c\u770b\u4e0bX y theta\u7684\u7ef4\u5ea6 X . shape , y . shape , theta . shape \u8f93\u51fa (( 97 , 2 ), ( 97 , 1 ), ( 2 , 1 )) \u91cd\u91cd\u91cd\u8981\u63d0\u793a!!! \u7b14\u8005\u5728\u8fd9\u91cc\u8e29\u4e86\u4e00\u4e2a\u5927\u5751, \u4e0a\u9762\u4e00\u4e2a\u4ee3\u7801\u57572.2 \u7684\u7b2c 19 \u884c\u975e\u5e38\u91cd\u8981\u3002\u56e0\u4e3atheta \u662f\u4e00\u4e2a\u5411\u91cf, \u6240\u4ee5\u7ef4\u5ea6\u53ea\u80fd\u8bbe\u7f6e\u4e3a[2,], \u800c\u4e0d\u662f[2,1]\u3002numpy\u91cc[2,]\u548c[2,1] \u4e0d\u662f\u4e00\u56de\u4e8b\u3002","title":"2.2 Implementation"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#23-computing-the-jtheta","text":"As you perform gradient descent to learn minimize the cost function \\(J(\\theta)\\) , it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate \\(J(\\theta)\\) so you can check the convergence of your gradient descent implementation. \u2002 As you are doing this, remember that the variables X and y are not scalar values, but matrices whose rows represent the examples from the training set. \u2002 Once you have completed the function, you should expect to see a cost of 32.07 ( \\(\\theta\\) initialized to zeros). \u4ee3\u7801\u57572.3 \u5b9a\u4e49\u4ee3\u4ef7\u51fd\u6570\u5e76\u8ba1\u7b97\u521d\u59cb\u503c 1 2 3 4 5 6 7 8 9 10 11 12 # \u5b9a\u4e49\u4ee3\u4ef7\u51fd\u6570 def computeCost ( X , y , theta ): m = X . shape [ 0 ] # print((np.dot(X,theta)-y).shape) inner = np . dot (( np . dot ( X , theta ) - y ) . T , ( np . dot ( X , theta ) - y )) return np . sum ( inner ) / ( 2 * m ) computeCost ( X , y , theta ) # \u8ba1\u7b97\u4e0d\u8fed\u4ee3\u4e4b\u524dJ\u7684\u521d\u59cb\u503c computeCost ( X , y , theta ) \u8f93\u51fa 32.072733877455676","title":"2.3 Computing the \\(J(\\theta)\\)"},{"location":"machine%20learning/1.%20linear%20regression-1%20v/#24-gradient-descent","text":"As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the \\(J(\\theta)\\) is parameterized by the vector \\(\\theta\\) not X and y. That is, we minimize the value of \\(J(\\theta)\\) by changing the values of the vector \\(\\theta\\) , not by changing X or y. Refer to the equations in this handout and to the video lectures if you are uncertain. \u2002 A good way to verify that gradient descent is working correctly is to look at the value of \\(J(\\theta)\\) and check that it is decreasing with each step. Assuming you have implemented gradient descent and computeCost correctly, your value of \\(J(\\theta)\\) should never increase, you should converge to a steady value by the end of the algorithm. \u5b9a\u4e49 \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 \u753b\u56fe 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # \u68af\u5ea6\u4e0b\u964d # \u5b9a\u4e49\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 def gradientDescent ( X , y , theta , alpha , iterations ): m = X . shape [ 0 ] # m: \u6837\u672c\u7684\u603b\u4e2a\u6570 n = len ( theta ) # n: theta\u7684\u603b\u4e2a\u6570 # \u7528\u4e00\u4e2a\u5411\u91cf\u6765\u8bb0\u5f55\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u6240\u6709\u7684cost\u503c cost = np . zeros ( iterations ) for i in range ( iterations ): cost [ i ] = computeCost ( X , y , theta ) # theta\u662f\u51e0\u4e2a\u5c31\u8981\u66f4\u65b0\u51e0\u4e2a for j in range ( n ): theta [ j ] = theta [ j ] - alpha * \\ ( 1 / m ) * np . sum (( np . dot ( X , theta ) - y ) * X [:, j ]) return theta , cost # \u4fdd\u8bc1\u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0ctheta\u4e0d\u53d7\u524d\u9762\u5f71\u54cd\uff0c\u6545\u91cd\u65b0\u521d\u59cb\u5316theta\u3002 theta = np . zeros ([ 2 , ]) # \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u5206\u522b\u63a5\u6536\u66f4\u65b0\u540e\u7684theta\u503c\u548c\u6bcf\u4e00\u6b65\u8fed\u4ee3\u7684cost\u503c theta_hat , costs = gradientDescent ( X , y , theta , alpha , iterations ) # \u6253\u5370\u68af\u5ea6\u5904\u7406\u540e\u7684\u9884\u6d4b\u51fd\u6570\u7684\u53c2\u6570theta print ( theta_hat ) # \u7528matplotlib\u753b\u56fe # 1. \u521b\u5efa\u753b\u5e03 plt . figure ( figsize = ( 12 , 8 ), dpi = 100 ) # 2. \u7ed8\u5236\u56fe\u50cf # \u7ed8\u5236\u8bad\u7ec3\u96c6\u6570\u636e plt . scatter ( data [ \"Population\" ], data [ \"Profit\" ], color = \"r\" , label = \"Training data\" ) # \u7ed8\u5236\u62df\u5408\u540e\u7684\u6570\u636e x_hat = np . linspace ( data . Population . min (), data . Population . max (), 100 ) y_hat = theta_hat [ 0 ] + theta_hat [ 1 ] * x_hat plt . plot ( x_hat , y_hat , color = 'b' , label = \"Linear regression\" ) # 2.1 \u6dfb\u52a0\u63cf\u8ff0\u4fe1\u606f plt . xlabel ( \"Population of City in 10,000s\" ) plt . ylabel ( \"Profit in $10,000s\" ) # 2.2 \u6dfb\u52a0\u7f51\u683c\u663e\u793a plt . grid ( True , linestyle = \"--\" , alpha = 0.5 ) # 2.3 \u6dfb\u52a0\u56fe\u4f8b plt . legend ( loc = 7 ) # 3.\u663e\u793a\u56fe\u50cf plt . show () \u8f93\u51fa\u7ed3\u679c array ([ - 3.63606348 , 1.16698916 ]) \u539f\u7ec3\u4e60\u4e2d\uff0c\u9700\u8981\u753b( \\(\\theta\\) , J ) \u7684\u4e09\u7ef4\u56fe \u4ee5\u53ca \u7b49\u9ad8\u7ebf\u56fe, \u4ed6\u4eec\u662f\u7528matlab/octave \u5b9e\u73b0\u7684\u3002\u7b14\u8005python\u592a\u83dc\u4e86\uff0c\u5c31\u4e0d\u5f04\u4e86\u8d39\u65f6\u95f4\u3002\u6700\u540e\u753b\u4e00\u4e2a\u8fed\u4ee3\u6b21\u6570\u548ccost\u7684\u6298\u7ebf\u56fe\u6536\u5c3e\u672c\u7ae0\u8282\u3002 \u753b\u51facost\u5173\u4e8e\u8fed\u4ee3\u6b21\u6570\u7684\u56fe\u50cf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1. \u521b\u5efa\u753b\u5e03 plt . figure ( figsize = ( 12 , 8 ), dpi = 100 ) # 2. \u7ed8\u5236\u6298\u7ebf\u56fe plt . plot ( np . arange ( iterations ), costs , 'r' ) # 2.1 \u6dfb\u52a0\u63cf\u8ff0\u4fe1\u606f plt . xlabel ( \"Iters\" ) plt . ylabel ( \"Cost\" ) # 2.2 \u6dfb\u52a0\u7f51\u683c\u663e\u793a plt . grid ( True , linestyle = \"--\" , alpha = 0.5 ) # 3.\u663e\u793a\u56fe\u50cf plt . show ()","title":"2.4 Gradient descent"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/","text":"1. \u591a\u7ef4\u7279\u5f81 \u53c2\u8003\u89c6\u9891: 4 - 1 - Multiple Features (8 min).mkv \u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u5355\u53d8\u91cf/\u7279\u5f81\u7684\u56de\u5f52\u6a21\u578b\uff0c\u73b0\u5728\u6211\u4eec\u5bf9\u623f\u4ef7\u6a21\u578b\u589e\u52a0\u66f4\u591a\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u623f\u95f4\u6570\u697c\u5c42\u7b49\uff0c\u6784\u6210\u4e00\u4e2a\u542b\u6709\u591a\u4e2a\u53d8\u91cf\u7684\u6a21\u578b\uff0c\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u4e3a \\(\\left( {x_{1}},{x_{2}},...,{x_{n}} \\right)\\) \u3002 Size (feet2) Number of bedrooms Number of floors Age of home (years) Price ($1000) 2104 5 1 45 460 1416 3 2 40 232 1534 3 2 30 315 852 2 1 36 178 \u2026 \u2026 \u2026 \u2026 \u2026 \u589e\u6dfb\u66f4\u591a\u7279\u5f81\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e00\u7cfb\u5217\u65b0\u7684\u6ce8\u91ca\uff1a \\(n\\) \u4ee3\u8868\u7279\u5f81\u7684\u6570\u91cf \\({x^{\\left( i \\right)}}\\) \u4ee3\u8868\u7b2c \\(i\\) \u4e2a\u8bad\u7ec3\u5b9e\u4f8b\uff0c\u662f\u7279\u5f81\u77e9\u9635\u4e2d\u7684\u7b2c \\(i\\) \u884c\uff0c\u662f\u4e00\u4e2a \u5411\u91cf \uff08 vector \uff09\u3002 \u6bd4\u65b9\u8bf4\uff0c\u4e0a\u56fe\u7684 \\({x}^{(2)}\\text{=}\\begin{bmatrix} 1416\\\\\\ 3\\\\\\ 2\\\\\\ 40 \\end{bmatrix}\\) \uff0c \\({x}_{j}^{\\left( i \\right)}\\) \u4ee3\u8868\u7279\u5f81\u77e9\u9635\u4e2d\u7b2c \\(i\\) \u884c\u7684\u7b2c \\(j\\) \u4e2a\u7279\u5f81\uff0c\u4e5f\u5c31\u662f\u7b2c \\(i\\) \u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u7b2c \\(j\\) \u4e2a\u7279\u5f81\u3002 \u5982\u4e0a\u56fe\u7684 \\(x_{2}^{\\left( 2 \\right)}=3,x_{3}^{\\left( 2 \\right)}=2\\) \uff0c \u652f\u6301\u591a\u53d8\u91cf\u7684\u5047\u8bbe \\(h\\) \u8868\u793a\u4e3a\uff1a \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \uff0c \u8fd9\u4e2a\u516c\u5f0f\u4e2d\u6709 \\(n+1\\) \u4e2a\u53c2\u6570\u548c \\(n\\) \u4e2a\u53d8\u91cf\uff0c\u4e3a\u4e86\u4f7f\u5f97\u516c\u5f0f\u80fd\u591f\u7b80\u5316\u4e00\u4e9b\uff0c\u5f15\u5165 \\(x_{0}=1\\) \uff0c\u5219\u516c\u5f0f\u8f6c\u5316\u4e3a\uff1a \\(h_{\\theta} \\left( x \\right)={\\theta_{0}}{x_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \u6b64\u65f6\u6a21\u578b\u4e2d\u7684\u53c2\u6570\u662f\u4e00\u4e2a \\(n+1\\) \u7ef4\u7684\u5411\u91cf\uff0c\u4efb\u4f55\u4e00\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e5f\u90fd\u662f \\(n+1\\) \u7ef4\u7684\u5411\u91cf\uff0c\u7279\u5f81\u77e9\u9635 \\(X\\) \u7684\u7ef4\u5ea6\u662f \\(m*(n+1)\\) \u3002 \u56e0\u6b64\u516c\u5f0f\u53ef\u4ee5\u7b80\u5316\u4e3a\uff1a \\(h_{\\theta} \\left( x \\right)={\\theta^{T}}X\\) \uff0c\u5176\u4e2d\u4e0a\u6807 \\(T\\) \u4ee3\u8868\u77e9\u9635\u8f6c\u7f6e\u3002 \u6ce8\u610f\uff1a \u4ece\u4e0a\u56fe\u53ef\u77e5, \\(\\theta\\) , \\(X\\) \u6211\u4eec\u662f\u7528 \u5217\u5411\u91cf \u6765\u6807\u8bb0\u3002\u8ba1\u7b97 \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \u7684\u65f6\u5019, \u53ef\u4ee5\u5c06 \\(h\\) \u8868\u793a\u4e3a \\(\\theta^TX.\\) 2. \u591a\u53d8\u91cf\u68af\u5ea6\u4e0b\u964d \u53c2\u8003\u89c6\u9891: 4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv \u5feb\u901f\u56de\u987e\u6211\u4eec\u7684\u8bb0\u53f7\uff0c\u5e76\u7528\u5411\u91cf\u7b80\u5316\u3002 Hypothesis: \\(h_\\theta(x)=\\theta_0+\\theta_1x+\\theta_2x+...+\\theta_nx\\) \u7b80\u8bb0\u4e3a\uff1a \\(h_\\theta(x)=\\theta^TX\\) Parameters: \\(\\theta_0,\\theta_1,\\theta_2,...\\theta_n\\) \u7b80\u8bb0\u4e3a\uff1a \\(\\theta\\) , \\(n+1\\ dimension\\ vector\\) Cost Function: \\[ J(\\theta_0,\\theta_1,\\ \\theta_2,...\\theta_n)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 \\] \u7b80\u8bb0\u4e3a\uff1a \\[ J(\\theta)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 \\] Gradient descent: Repeat { \u200b \\({\\theta_{j}}:={\\theta_{j}}-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,...\\theta_n)\\) \u200b } (simultaneously update for every j=0, 1, 2, ..., n) \u7b80\u8bb0\u4e3a\uff1a\u628a\u4e0a\u9762\u7684 \\(J(\\theta_0,...\\theta_n)\\) \u6362\u6210 \\(J(\\theta)\\) \u4e0e\u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u7c7b\u4f3c\uff0c\u5728\u591a\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u6211\u4eec\u4e5f\u6784\u5efa\u4e00\u4e2a\u4ee3\u4ef7\u51fd\u6570\uff0c\u5219\u8fd9\u4e2a\u4ee3\u4ef7\u51fd\u6570\u662f\u6240\u6709\u5efa\u6a21\u8bef\u5dee\u7684\u5e73\u65b9\u548c\uff0c\u5373\uff1a \\(J\\left( {\\theta_{0}},{\\theta_{1}}...{\\theta_{n}} \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( h_{\\theta} \\left({x}^{\\left( i \\right)} \\right)-{y}^{\\left( i \\right)} \\right)}^{2}}}\\) \uff0c \u5176\u4e2d\uff1a \\(h_{\\theta}\\left( x \\right)=\\theta^{T}X={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \uff0c \u6211\u4eec\u7684\u76ee\u6807\u548c\u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u4e2d\u4e00\u6837\uff0c\u662f\u8981\u627e\u51fa\u4f7f\u5f97\u4ee3\u4ef7\u51fd\u6570\u6700\u5c0f\u7684\u4e00\u7cfb\u5217\u53c2\u6570\u3002 \u591a\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u7684\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e3a\uff1a \u5f53 \\(n>=1\\) \u65f6\uff0c \\({{\\theta }_{0}}:={{\\theta }_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}\\) \\({{\\theta }_{1}}:={{\\theta }_{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}\\) \\({{\\theta }_{2}}:={{\\theta }_{2}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}\\) ... \\({{\\theta }_{n}}:={{\\theta }_{n}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{n}^{(i)}\\) \u6211\u4eec\u5f00\u59cb\u968f\u673a\u9009\u62e9\u4e00\u7cfb\u5217\u7684\u53c2\u6570\u503c\uff0c\u8ba1\u7b97\u6240\u6709\u7684\u9884\u6d4b\u7ed3\u679c\u540e\uff0c\u518d\u7ed9\u6240\u6709\u7684\u53c2\u6570\u4e00\u4e2a\u65b0\u7684\u503c\uff0c\u5982\u6b64\u5faa\u73af\u76f4\u5230\u6536\u655b\u3002 \u4ee3\u7801\u793a\u4f8b\uff1a \u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570 \\(J\\left( \\theta \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {h_{\\theta}}\\left( {x^{(i)}} \\right)-{y^{(i)}} \\right)}^{2}}}\\) \u5176\u4e2d\uff1a \\({h_{\\theta}}\\left( x \\right)={\\theta^{T}}X={\\theta_{0}}{x_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) Python \u4ee3\u7801\uff1a def computeCost ( X , y , theta ): inner = np . power ((( X * theta . T ) - y ), 2 ) return np . sum ( inner ) / ( 2 * len ( X )) \u4e0b\u9762\uff0c\u6211\u4eec\u5f00\u59cb\u63a8\u5bfc \\(\\frac{\\partial}{\\partial\\theta}J(\\theta)\\) \u3002\u8fd9\u91cc\u4e3a\u4e86\u666e\u904d\u6027\uff0c\u8fd8\u539f\u4e86 \\(\\theta\\) \u7684\u666e\u904d\u60c5\u51b5\uff0c\u4e5f\u5373\u6709n\u4e2a \\(\\theta\\) \u7684\u60c5\u51b5\u3002 Hypothesis: \\(h_\\theta(x)=\\theta^Tx=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\) Parameters: \\(\\theta_0,\\theta_1,...,\\theta_n\\) Cost function: \\[ J(\\theta_0,\\theta_1,...,\\theta_n)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 \\] Gradient descent: Repeat{ $$ \\theta_j :=\\theta_j-\\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1,...,\\theta_n) $$ \u200b }\uff08simultaneously update for every j=0, 1, 2, ..., n)\uff09 \u63a8\u5bfc\uff1a \\(\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1,...,\\theta_n)=?\\) Let's first work it for the case if we have only one training example \\((x,y)\\) , so that we can neglect the sum in the definition \\(J\\) . We have: $$ \\begin{split} \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1,...,\\theta_n) & = \\frac {1} {2}\\frac{\\partial}{\\partial \\theta_j}\\Big(h_\\theta(x)-y\\Big)^2 \\\\ & = 2\\cdot\\frac{1}{2}\\cdot(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial \\theta_j}(h_\\theta(x)-y) \\\\ & = (h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial \\theta_j}(\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n) \\\\ & = (h_\\theta(x)-y)\\cdot x_j \\end{split} $$ 3. \u68af\u5ea6\u4e0b\u964d\u6cd5 - \u7279\u5f81\u7f29\u653e \u53c2\u8003\u89c6\u9891: 4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv \u5728\u6211\u4eec\u9762\u5bf9\u591a\u7ef4\u7279\u5f81\u95ee\u9898\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u4fdd\u8bc1\u8fd9\u4e9b\u7279\u5f81\u90fd\u5177\u6709\u76f8\u8fd1\u7684\u5c3a\u5ea6\uff0c\u8fd9\u5c06\u5e2e\u52a9\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u5feb\u5730\u6536\u655b\u3002 \u4ee5\u623f\u4ef7\u95ee\u9898\u4e3a\u4f8b\uff0c\u5047\u8bbe\u6211\u4eec\u4f7f\u7528\u4e24\u4e2a\u7279\u5f81\uff0c\u623f\u5c4b\u7684\u5c3a\u5bf8\u548c\u623f\u95f4\u7684\u6570\u91cf\uff0c\u5c3a\u5bf8\u7684\u503c\u4e3a 0-2000\u5e73\u65b9\u82f1\u5c3a\uff0c\u800c\u623f\u95f4\u6570\u91cf\u7684\u503c\u5219\u662f0-5\uff0c\u4ee5\u4e24\u4e2a\u53c2\u6570\u5206\u522b\u4e3a\u6a2a\u7eb5\u5750\u6807\uff0c\u7ed8\u5236\u4ee3\u4ef7\u51fd\u6570\u7684\u7b49\u9ad8\u7ebf\u56fe\u80fd\uff0c\u770b\u51fa\u56fe\u50cf\u4f1a\u663e\u5f97\u5f88\u6241\uff0c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u9700\u8981\u975e\u5e38\u591a\u6b21\u7684\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u3002 \u89e3\u51b3\u7684\u65b9\u6cd5\u662f\u5c1d\u8bd5\u5c06\u6240\u6709\u7279\u5f81\u7684\u5c3a\u5ea6\u90fd\u5c3d\u91cf\u7f29\u653e\u5230-1\u52301\u4e4b\u95f4\u3002\u5982\u56fe\uff1a \u89c6\u9891\u91cc\u5434\u6069\u8fbe\u8001\u5e08\u7684\u65b9\u6cd5\u662f\u4ee4\uff1a \\({{x}_{n}}=\\frac{{{x}_{n}}-mean}{{max(x)-min(x)}}\\) , \u5176\u4e2d \\(mean\\) \u662f\u5e73\u5747\u503c\uff0c \\(max(x)-min(x)\\) \u5206\u522b\u662f\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002 \u66f4\u901a\u7528\u7684\u662f\u4ee4\uff1a \\({{x}_{n}}=\\frac{{{x}_{n}}-{mean}}{\\sigma}\\) \uff0c\u5176\u4e2d \\(mean\\) \u662f\u5e73\u5747\u503c\uff0c \\(\\sigma\\) \u662f\u6807\u51c6\u5dee\u3002 python\u91cc\u7684api\uff1a sklearn.preprocessing.StandardScaler() \u5904\u7406\u4e4b\u540e\u6bcf\u5217\u6765\u8bf4 \u6240\u6709\u6570\u636e\u90fd\u805a\u96c6\u5728\u5747\u503c0\u9644\u8fd1\u6807\u51c6\u5dee\u5dee\u4e3a1 StandardScaler.fit_transform(X) X:numpy array\u683c\u5f0f\u7684\u6570\u636e[n_samples,n_features] \u8fd4\u56de\u503c\uff1a\u8f6c\u6362\u540e\u7684\u5f62\u72b6\u76f8\u540c\u7684array \u90e8\u5206\u53c2\u8003\u4ee3\u7801\uff1a from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def linear_model1 (): # 1.\u83b7\u53d6\u6570\u636e boston = load_boston () # 2. \u6570\u636e\u96c6\u5212\u5206 x_train , x_test , y_train , y_test = train_test_split ( boston . data , boston . target , test_size = 0.2 ) # 3. \u7279\u5f81\u5de5\u7a0b-\u6807\u51c6\u5316 transfer = StandardScaler () x_train = transfer . fit_transform ( x_train ) x_test = transfer . fit_transform ( x_test ) 4. \u68af\u5ea6\u4e0b\u964d\u6cd5 - \u5b66\u4e60\u7387 \u53c2\u8003\u89c6\u9891: 4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6536\u655b\u6240\u9700\u8981\u7684\u8fed\u4ee3\u6b21\u6570\u6839\u636e\u6a21\u578b\u7684\u4e0d\u540c\u800c\u4e0d\u540c\uff0c\u6211\u4eec\u4e0d\u80fd\u63d0\u524d\u9884\u77e5\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed8\u5236\u8fed\u4ee3\u6b21\u6570\u548c\u4ee3\u4ef7\u51fd\u6570\u7684\u56fe\u8868\u6765\u89c2\u6d4b\u7b97\u6cd5\u5728\u4f55\u65f6\u8d8b\u4e8e\u6536\u655b\u3002 \u4e5f\u6709\u4e00\u4e9b\u81ea\u52a8\u6d4b\u8bd5\u662f\u5426\u6536\u655b\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u5c06\u4ee3\u4ef7\u51fd\u6570\u7684\u53d8\u5316\u503c\u4e0e\u67d0\u4e2a\u9600\u503c\uff08\u4f8b\u59820.001\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u901a\u5e38\u770b\u5de6\u4e0a\u65b9\u8fd9\u6837\u7684\u56fe\u8868\u66f4\u597d\u3002 \u4e0d\u6b63\u786e\u7684\u5b66\u4e60\u7387\uff0c\u4f1a\u4ea7\u751f\u5de6\u4fa7\u4e0a\u4e0b\u4e24\u4e2a\u56fe\u50cf\u3002 \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u6bcf\u6b21\u8fed\u4ee3\u53d7\u5230\u5b66\u4e60\u7387\u7684\u5f71\u54cd\uff0c\u5982\u679c\u5b66\u4e60\u7387 \\(\\alpha\\) \u8fc7\u5c0f\uff0c\u5219\u8fbe\u5230\u6536\u655b\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u4f1a\u975e\u5e38\u9ad8\uff1b\u5982\u679c\u5b66\u4e60\u7387 \\(\\alpha\\) \u8fc7\u5927\uff0c\u6bcf\u6b21\u8fed\u4ee3\u53ef\u80fd\u4e0d\u4f1a\u51cf\u5c0f\u4ee3\u4ef7\u51fd\u6570\uff0c\u53ef\u80fd\u4f1a\u8d8a\u8fc7\u5c40\u90e8\u6700\u5c0f\u503c\u5bfc\u81f4\u65e0\u6cd5\u6536\u655b\u3002 \u901a\u5e38\u53ef\u4ee5\u8003\u8651\u5c1d\u8bd5\u4e9b\u5b66\u4e60\u7387\uff1a \\(\\alpha= 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\\) 5. \u7279\u5f81\u548c\u591a\u9879\u5f0f\u56de\u5f52 \u53c2\u8003\u89c6\u9891: 4 - 5 - Features and Polynomial Regression (8 min).mkv \u5982\u623f\u4ef7\u9884\u6d4b\u95ee\u9898\uff0c \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}\\times{frontage}+{\\theta_{2}}\\times{depth}\\) \u5f53\u6211\u4eec\u771f\u6b63\u5e94\u7528\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u521b\u9020\u81ea\u5df1\u7684\u7279\u5f81\u5373\uff1a \\({x_{1}}=frontage\\) \uff08\u4e34\u8857\u5bbd\u5ea6\uff09\uff0c \\({x_{2}}=depth\\) \uff08\u7eb5\u5411\u6df1\u5ea6\uff09\uff0c \\(x=frontage*depth=area\\) \uff08\u9762\u79ef\uff09\uff0c \u5219\uff1a \\({h_{\\theta}}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}x\\) \u3002 \u7ebf\u6027\u56de\u5f52\u5e76\u4e0d\u9002\u7528\u4e8e\u6240\u6709\u6570\u636e\uff0c\u6709\u65f6\u6211\u4eec\u9700\u8981\u5176\u4ed6\u6a21\u578b\u6765\u9002\u5e94\u6211\u4eec\u7684\u6570\u636e\uff0c\u6bd4\u5982\u4e00\u4e2a\u4e8c\u6b21\u65b9\u6a21\u578b\uff1a \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}^2}\\) \u6216\u8005\u4e09\u6b21\u65b9\u6a21\u578b\uff1a \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}^2}+{\\theta_{3}}{x_{3}^3}\\) \u4ece\u4e0a\u9762\u56fe\u53f3\u4fa7\uff0c\u53ef\u4ee5\u770b\u51fa\u5982\u679c\u6211\u4eec\u91c7\u7528\u591a\u9879\u5f0f\u56de\u5f52\u6a21\u578b\uff0c\u5728\u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u524d\u7279\u5f81\u7f29\u653e\u7684\u91cd\u8981\u6027\u4e86\u3002 \u901a\u5e38\u6211\u4eec\u9700\u8981\u5148\u89c2\u5bdf\u6570\u636e\u7136\u540e\u518d\u51b3\u5b9a\u51c6\u5907\u5c1d\u8bd5\u600e\u6837\u7684\u6a21\u578b\u3002 \u53e6\u5916\uff0c\u6211\u4eec\u53ef\u4ee5\u4ee4\uff1a \\({{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}\\) \uff0c\u4ece\u800c\u5c06\u6a21\u578b\u8f6c\u5316\u4e3a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002 \u6839\u636e\u51fd\u6570\u56fe\u5f62\u7279\u6027\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u4f7f\uff1a \\({{{h}}_{\\theta}}(x)={{\\theta }_{0}}\\text{+}{{\\theta }_{1}}(size)+{{\\theta}_{2}}{{(size)}^{2}}\\) \u6216\u8005: \\({{{h}}_{\\theta}}(x)={{\\theta }_{0}}\\text{+}{{\\theta }_{1}}(size)+{{\\theta }_{2}}\\sqrt{size}\\) 6. \u6b63\u89c4\u65b9\u7a0b \u53c2\u8003\u89c6\u9891: 4 - 6 - Normal Equation (16 min).mkv \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u90fd\u5728\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u4f46\u662f\u5bf9\u4e8e\u67d0\u4e9b\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff0c\u6b63\u89c4\u65b9\u7a0b\u65b9\u6cd5\u662f\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5982\uff1a \u6b63\u89c4\u65b9\u7a0b\u662f\u901a\u8fc7\u6c42\u89e3\u4e0b\u9762\u7684\u65b9\u7a0b\u6765\u627e\u51fa\u4f7f\u5f97\u4ee3\u4ef7\u51fd\u6570\u6700\u5c0f\u7684\u53c2\u6570\u7684\uff1a \\(\\frac{\\partial}{\\partial{\\theta_{j}}}J\\left( {\\theta_{j}} \\right)=0\\) \u3002 \u5047\u8bbe\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u7279\u5f81\u77e9\u9635\u4e3a \\(X\\) \uff08\u5305\u542b\u4e86 \\({{x}_{0}}=1\\) \uff09\u5e76\u4e14\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u7ed3\u679c\u4e3a\u5411\u91cf \\(y\\) \uff0c\u5219\u5229\u7528\u6b63\u89c4\u65b9\u7a0b\u89e3\u51fa\u5411\u91cf \\(\\theta ={{\\left( {X^T}X \\right)}^{-1}}{X^{T}}y\\) \u3002 \u4ee5\u4e0b\u9762\u8868\u683c\u6570\u636e\u4e3a\u4f8b \\(m=4\\) \uff1a \\(x_0\\) Size (feet2) Number of bedrooms Number of floors Age of home (years) Price ($1000) 1 2104 5 1 45 460 1 1416 3 2 40 232 1 1534 3 2 30 315 1 852 2 1 36 178 \\(X\\ \\text{=}\\begin{bmatrix}1 & 2104 & 5 & 1 & 45\\\\1 & 1416 & 3 & 2 & 40\\\\ 1 &1534 & 3 & 2 & 30\\\\ 1 & 852 & 2 & 1 & 36\\end{bmatrix}\\) \uff0c \\(y\\ \\text{=}\\begin{bmatrix} 460\\\\232\\\\315\\\\178\\end{bmatrix}\\) \\(X\u7ef4\u5ea6\uff1a(m,n+1),\\ y\u7684\u7ef4\u5ea6\uff1a(m,1)\\) \u8fd9\u65f6\u5019\u6c42\u89e3 \\(\\theta\\) \u53ea\u9700\u4e00\u6b65: \\(\\theta ={{\\left( {X^T}X \\right)}^{-1}}{X^{T}}y\\) \u3002 \u5c06\u4e0a\u9762\u7684\u4f8b\u5b50\u63a8\u5e7f\u5230\u4e00\u822c\u60c5\u51b5 \u6ce8\uff1a\u5bf9\u4e8e\u90a3\u4e9b\u4e0d\u53ef\u9006\u7684\u77e9\u9635\uff08\u901a\u5e38\u662f\u56e0\u4e3a\u7279\u5f81\u4e4b\u95f4\u4e0d\u72ec\u7acb\uff0c\u5982\u540c\u65f6\u5305\u542b\u82f1\u5c3a\u4e3a\u5355\u4f4d\u7684\u5c3a\u5bf8\u548c\u7c73\u4e3a\u5355\u4f4d\u7684\u5c3a\u5bf8\u4e24\u4e2a\u7279\u5f81\uff0c\u4e5f\u6709\u53ef\u80fd\u662f\u7279\u5f81\u6570\u91cf\u5927\u4e8e\u8bad\u7ec3\u96c6\u7684\u6570\u91cf\uff09\uff0c\u6b63\u89c4\u65b9\u7a0b\u65b9\u6cd5\u662f\u4e0d\u80fd\u7528\u7684\u3002 \u5343\u4e07\u8981\u6ce8\u610f\u8fd9\u91cc\u7684\u8bbe\u8ba1\u77e9\u9635X\u5b83\u7684\u6784\u6210\uff0c\u8bbe\u8ba1\u5b8c\u6210\u540e\uff0c\u5047\u8bbe\u51fd\u6570\u53ef\u4ee5\u5411\u91cf\u5316\u4e3a \\(h_\\theta(x)=X\\theta\\) \u68af\u5ea6\u4e0b\u964d\u4e0e\u6b63\u89c4\u65b9\u7a0b\u7684\u6bd4\u8f83\uff1a \u68af\u5ea6\u4e0b\u964d \u6b63\u89c4\u65b9\u7a0b \u9700\u8981\u9009\u62e9\u5b66\u4e60\u7387 \\(\\alpha\\) \u4e0d\u9700\u8981 \u9700\u8981\u591a\u6b21\u8fed\u4ee3 \u4e00\u6b21\u8fd0\u7b97\u5f97\u51fa \u5f53\u7279\u5f81\u6570\u91cf \\(n\\) \u5927\u65f6\u4e5f\u80fd\u8f83\u597d\u9002\u7528 \u9700\u8981\u8ba1\u7b97 \\({{\\left( {{X}^{T}}X \\right)}^{-1}}\\) \u5982\u679c\u7279\u5f81\u6570\u91cfn\u8f83\u5927\u5219\u8fd0\u7b97\u4ee3\u4ef7\u5927\uff0c\u56e0\u4e3a\u77e9\u9635\u9006\u7684\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a \\(O\\left( {{n}^{3}} \\right)\\) \uff0c\u901a\u5e38\u6765\u8bf4\u5f53 \\(n\\) \u5c0f\u4e8e10000 \u65f6\u8fd8\u662f\u53ef\u4ee5\u63a5\u53d7\u7684 \u9002\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u6a21\u578b \u53ea\u9002\u7528\u4e8e\u7ebf\u6027\u6a21\u578b\uff0c\u4e0d\u9002\u5408\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7b49\u5176\u4ed6\u6a21\u578b \u603b\u7ed3\u4e00\u4e0b\uff0c\u53ea\u8981\u7279\u5f81\u53d8\u91cf\u7684\u6570\u76ee\u5e76\u4e0d\u5927\uff0c\u6807\u51c6\u65b9\u7a0b\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u8ba1\u7b97\u53c2\u6570$\\theta $\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002\u5177\u4f53\u5730\u8bf4\uff0c \u53ea\u8981\u7279\u5f81\u53d8\u91cf\u6570\u91cf\u5c0f\u4e8e\u4e00\u4e07\uff0c\u6211\u901a\u5e38\u4f7f\u7528\u6b63\u89c4\u65b9\u7a0b\u6cd5\uff0c\u800c\u4e0d\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u3002 \u968f\u7740\u6211\u4eec\u8981\u8bb2\u7684\u5b66\u4e60\u7b97\u6cd5\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4f8b\u5982\uff0c\u5f53\u6211\u4eec\u8bb2\u5230\u5206\u7c7b\u7b97\u6cd5\uff0c\u50cf\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\uff0c\u6211\u4eec\u4f1a\u770b\u5230\uff0c\u5b9e\u9645\u4e0a\u5bf9\u4e8e\u90a3\u4e9b\u7b97\u6cd5\uff0c\u5e76\u4e0d\u80fd\u4f7f\u7528\u6807\u51c6\u65b9\u7a0b\u6cd5\u3002\u5bf9\u4e8e\u90a3\u4e9b\u66f4\u590d\u6742\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u6211\u4eec\u5c06\u4e0d\u5f97\u4e0d\u4ecd\u7136\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u3002\u56e0\u6b64\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u662f\u4e00\u4e2a\u975e\u5e38\u6709\u7528\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u7528\u5728\u6709\u5927\u91cf\u7279\u5f81\u53d8\u91cf\u7684\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u3002\u6216\u8005\u6211\u4eec\u4ee5\u540e\u5728\u8bfe\u7a0b\u4e2d\uff0c\u4f1a\u8bb2\u5230\u7684\u4e00\u4e9b\u5176\u4ed6\u7684\u7b97\u6cd5\uff0c\u56e0\u4e3a\u6807\u51c6\u65b9\u7a0b\u6cd5\u4e0d\u9002\u5408\u6216\u8005\u4e0d\u80fd\u7528\u5728\u5b83\u4eec\u4e0a\u3002\u4f46\u5bf9\u4e8e\u8fd9\u4e2a\u7279\u5b9a\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u6807\u51c6\u65b9\u7a0b\u6cd5\u662f\u4e00\u4e2a\u6bd4\u68af\u5ea6\u4e0b\u964d\u6cd5\u66f4\u5feb\u7684\u66ff\u4ee3\u7b97\u6cd5\u3002\u6240\u4ee5\uff0c\u6839\u636e\u5177\u4f53\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f60\u7684\u7279\u5f81\u53d8\u91cf\u7684\u6570\u91cf\uff0c\u8fd9\u4e24\u79cd\u7b97\u6cd5\u90fd\u662f\u503c\u5f97\u5b66\u4e60\u7684\u3002 \u6b63\u89c4\u65b9\u7a0b\u7684 python \u5b9e\u73b0\uff1a import numpy as np def normalEqn ( X , y ): theta = np . linalg . inv ( X . T @X ) @X . T @y #X.T@X\u7b49\u4ef7\u4e8eX.T.dot(X) return theta \u4ee5\u4e0b\u662f\u6b63\u89c4\u65b9\u7a0b\u82f1\u6587\u7248(\u6709\u8be6\u7ec6\u63a8\u5bfc-\u81ea\u5df1\u52a0\u4e0a\u7684)\uff0c\u5176\u4ed6\u5185\u5bb9\u6765\u6e90\u4e8e2006\u5e74\u9ebb\u7701\u7406\u5de5cs 2009\u673a\u5668\u5b66\u4e60\u7684note\u3002 7. The normal equations p { text-align: justify; /*\u6587\u672c\u4e24\u7aef\u5bf9\u9f50*/ } Gradient descent gives one way of minimizing \\(J\\) . Lets discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize \\(J\\) by explicitly taking its derivatives with respect to the \\(\u03b8_j's\\) , and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, lets introduce some notation for doing calculus with matrices. 7.1 Matrix derivatives For a function \\(f\\) : \\(\\mathbb{R^{m\\times n}} \\rightarrow \\mathbb{R}\\) mapping from \\(\\text{m-by-n}\\) matrices to the real numbers, we define the derivative of f with respect to A to be: \\[ \\nabla _Af(A)=\\begin{bmatrix} \\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1n}} \\\\\\\\ \\vdots & \\ddots & \\vdots \\\\\\\\ \\frac{\\partial f}{\\partial A_{m1}} & \\cdots & \\frac{\\partial f}{\\partial A_{mn}}\\end{bmatrix} \\] Thus, the gradient \\(\\nabla _Af(A)\\) is itself an \\(\\text{m-by-n}\\) matrix, whose \\((i,j)\\) -element is \\(\\frac{\\partial f}{\\partial A_{ij}}\\) . For example, suppose \\(A =\\bigl( \\begin{smallmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{smallmatrix} \\bigr)\\) is a 2-by-2 matrix, and the function \\(f\\) : \\(\\mathbb{R^{2\\times 2}}\\rightarrow \\mathbb{R}\\) is given by \\[ f(A)=\\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22} \\] Here, \\(A_{ij}\\) denotes the \\((i,j)\\) entry of the matrix \\(A\\) . We then have \\[ \\nabla _Af(A)=\\begin{bmatrix} \\frac{3}{2} & 10A_{12}\\\\\\\\ A_{22} & A_{21} \\end{bmatrix} \\] \u2002 We also introduce the trace opertator, written by \"tr.\". For an \\(\\text{n-by-n}\\) (square) matrix A, the trace of A is defined to be the sum of its diagonal entries: \\[ \\operatorname{tr}A=\\sum_{i=1}^nA_{ii} \\] If \\(a\\) is a real number (i.e., a 1-by-1 matrix), then \\(\\operatorname{tr} a = a\\) . (If you haven't seen this \"opertator notation\" before, you should think of the trace of \\(A\\) as \\(\\operatorname{tr}(A)\\) , or as application of the \"trace\" function to the matrix \\(A\\) . It's more commonly written without the parentheses, however.\uff09 \u2002 The trace opertator has the property that for two matrices \\(A\\) and \\(B\\) such that \\(AB\\) is square, we have that \\(\\operatorname{tr}(AB)=\\operatorname{tr}(BA)\\) . \u8bc1\u660e\u8bf7\u67e5\u770b\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6570\u5b66\u77e5\u8bc6\u4e2d\u5173\u4e8e\u8ff9\u7684\u4ea4\u6362\u5f8b\u7684\u8bc1\u660e\u3002 As corollaries of this, we also have, e.g., \\[ \\operatorname{tr}(ABC)=\\operatorname{tr}(CAB)=\\operatorname{tr}(BCA)\\] \\[ \\operatorname{tr}(ABCD)=\\operatorname{tr}(DABC)=\\operatorname{tr}(CDAB)=\\operatorname{tr}(BCDA)\\] The following properties of the trace operator are also easily verified. Here, \\(A\\) and \\(B\\) are square matrices, and \\(a\\) is a real number: \\[ \\operatorname{tr}(A) = \\operatorname{tr}(A^T)\\] \\[ \\operatorname{tr}(A+B) = \\operatorname{tr}(A)+\\operatorname{tr}(B)\\] \\[ \\operatorname{tr}(aA) = a \\operatorname{tr}(A)\\] \u2002 We now state without proof some facts of matrix derivatives (we won\u2019t need some of these until later this quarter). Equation (4) applies only to non-singular square matrices A, where |A| denotes the determinant of A. We have: \\[ \\nabla_A \\operatorname{tr}(AB)=B^T\\\\\\\\ \\tag{1}\\] \\[ \\nabla_{A^T}f(A) = \\big(\\nabla_Af(A)\\big)^T \\\\\\\\\\tag{2}\\] \\[ \\nabla_A \\operatorname{tr}ABA^TC=CAB+C^TAB^T \\\\\\\\\\tag{3} \\] \\[\\nabla_A|A|=|A|(A^{-1})^T\\\\\\\\\\tag{4}\\] \u5434\u6069\u8fbe\u8001\u5e08\u4e0d\u7ed9\u4f60\u4eec\u8bc1\u660e\uff0c\u6211\u6765\u7ed9\u4f60\u4eec\u8bc1\u660e \u8bf7\u5728\u770b\u4e0b\u9762\u63a8\u5bfc\u4e4b\u524d, \u52a1\u5fc5\u5148\u770b\u61c2\u6211\u8fd9\u4e2a\u77e5\u8bc6\u5e93\u4e2d \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6570\u5b66\u77e5\u8bc6 \u2002---\u2002( \u4e00. \u77e9\u9635\u6c42\u5bfc\u672c\u8d28 &\u4e8c. \u77e9\u9635\u6c42\u5bfc )\u3002 I. \\(\\nabla_A \\operatorname{tr}(AB)=B^T\\) \u8bc1\u660e: \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u4e2d---(6)\u5f0f\u7684\u8bc1\u660e \u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff1a \u5bf9\u4e8e\u4e24\u4e2a\u9636\u6570\u90fd\u662f \\(m \\times n\\) \u7684\u77e9\u9635 \\(C_{m \\times n}, D_{m \\times n}\\) \u5176\u4e2d \u4e00\u4e2a\u77e9\u9635\u4e58\u4ee5\uff08\u5de6\u4e58\u53f3\u4e58\u90fd\u53ef\u4ee5\uff09\u53e6\u4e00\u4e2a\u77e9\u9635\u7684 \u8f6c\u7f6e \u7684\u8ff9\uff0c\u672c\u8d28\u662f \\(C_{m \\times n}, D_{m \\times n}\\) \u4e24\u4e2a\u77e9\u9635\u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0 \u3002 \u6240\u4ee5\u8fd9\u91cc \\(\\operatorname{tr}(AB)\\) \u76f8\u5f53\u4e8e\u5c31\u662f \\(A\\) \u548c \\(B^T\\) \u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u5143\u7d20 \u76f8\u4e58\u5e76\u76f8\u52a0 \u3002 \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times m}\\) \u3002 \u63a5\u7740, \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(11)\u5f0f \u6211\u4eec\u53ef\u4ee5\u77e5\u9053: \u8fd9\u91cc\u6c42 \\(\\nabla_A \\operatorname{tr}(AB)\\) \u76f8\u5f53\u4e8e \\(\\operatorname{tr}(AB)\\) \u6309\u7167 \\(A\\) \u77e9\u9635\u5206\u5e03\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5143\u7d20\u6c42\u504f\u5bfc\u3002 \u6240\u4ee5, \u7efc\u4e0a\u6240\u8ff0, \\(\\nabla_A \\operatorname{tr}(AB)=B^T\\) \u3002 \u8bc1\u6bd5\u3002 II. \\(\\nabla_{A^T}f(A) = \\big(\\nabla_Af(A)\\big)^T\\) \u8bc1\u660e: \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(11)\u5f0f \u53ef\u77e5: \\[ \\begin{align} \\nabla_{A^T}f(A) &= \\begin{bmatrix} \\frac{\\partial f}{\\partial a_{11}} & \\cdots & \\frac{\\partial f}{\\partial a_{m1}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f}{\\partial x_{1n}} & \\cdots & \\frac{\\partial f}{\\partial x_{mn}} \\\\ \\end{bmatrix}\\\\\\\\ & = \\begin{bmatrix} \\frac{\\partial f}{\\partial a_{11}} & \\cdots & \\frac{\\partial f}{\\partial a_{1n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f}{\\partial x_{m1}} & \\cdots & \\frac{\\partial f}{\\partial x_{mn}} \\\\ \\end{bmatrix}\\\\\\\\ & = \\big(\\nabla_Af(A)\\big)^T \\end{align} \\] \u8bc1\u6bd5\u3002 III. \\(\\nabla_A \\operatorname{tr}ABA^TC=CAB+C^TAB^T\\) \u8bc1\u660e: \u9996\u5148, \u6211\u4eec\u8981\u660e\u786e\u8fd9\u91cc\u7684 \\(ABA^TC\\) \u662f\u5173\u4e8e \\(A\\) \u77e9\u9635\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570, \u6240\u4ee5, \u6211\u4eec\u53ef\u4ee5\u4ee4 \\(f(A)=ABA^TC\\) \u3002 \u6ce8\u610f: \u8fd9\u91cc\u7684\u6807\u8bb0, \u7531\u4e8e \\(A\\) \u5176\u5b9e\u662f\u77e9\u9635\u53d8\u5143, \u5e94\u8be5\u6807\u8bb0\u4e3a \\(f(\\pmb A)=\\pmb A B \\pmb A^TC\\) , \u6240\u4ee5\u540e\u9762\u7684\u63a8\u5bfc\u8fc7\u7a0b,\u6211\u4eec\u4e25\u8c28\u4e00\u4e9b, \u5c06\u77e9\u9635\u53d8\u5143 \\(A\\) \u6807\u8bb0\u4e3a \\(\\pmb A\\) \u3002 \u4ed4\u7ec6\u60f3\u4f60\u4f1a\u53d1\u73b0\uff0c\u5bf9\u4e8e\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f (\\pmb{A})\\) , \\(\\operatorname{tr}\\big( f(\\pmb A) \\big)=f(\\pmb A)\\) , \\(\\mathbb{d}f(\\pmb A)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb A) \\big)\\) \u6240\u4ee5\u6709 \\(\\mathbb{d}f(\\pmb A)=\\mathbb{d}\\big(\\operatorname{tr}f(\\pmb A)\\big)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb A) \\big)\\) \u3002 \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(24)\u5f0f \u5373: \\[ \\mathbb{d}f(\\pmb{X})= \\operatorname{tr}\\Big(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\mathbb{d}\\pmb{X}\\Big) \\] \u6211\u4eec\u53ef\u4ee5\u628a\u4e00\u4e2a\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\u5199\u6210\u4e0a\u5f0f\uff0c\u6211\u4eec\u5c31\u627e\u5230\u4e86\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c, \u4e5f\u5373: \\[ \\mathbb{d}f(\\pmb{A})= \\operatorname{tr}\\Big(\\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}\\mathbb{d}\\pmb{A}\\Big)\\\\\\\\ \\tag{II.1} \\] \u7531\u6211\u4eec\u8bc1\u660e\u7684 II. \\(\\nabla_{A^T}f(A) = \\big(\\nabla_Af(A)\\big)^T\\) \u5f97: \\[ \\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T} = \\nabla _{\\pmb A^T}f(\\pmb A)=\\big(\\nabla _{\\pmb A}f(\\pmb A)\\big)^T \\] \u6240\u4ee5\u6211\u4eec\u8981\u6c42\u7684: \\[ \\nabla _{\\pmb A}f(\\pmb A)=\\Big(\\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}\\Big)^T \\\\\\\\ \\tag{II.2} \\] \u6700\u7ec8\u6211\u4eec\u7684\u4efb\u52a1\u5c31\u662f\u8f6c\u5316\u4e3a\u6c42 \\(\\mathbb{d}f(\\pmb{A})\\) \u7684\u5168\u5fae\u5206, \u4e0b\u9762\u5f00\u59cb\u63a8\u5bfc\uff1a \\[ \\begin{aligned} \\mathbb{d}f(\\pmb{A}) & = \\mathbb{d}\\pmb A B \\pmb A^TC \\\\\\\\ & = \\mathbb{d}\\operatorname{tr}(\\pmb A B \\pmb A^TC)\\\\\\\\ & = \\mathbb{d}\\operatorname{tr}(C \\pmb A B \\pmb A^T)\\\\\\\\ & = \\operatorname{tr}\\mathbb{d}(C \\pmb A B \\pmb A^T)\\\\\\\\ & = \\operatorname{tr}\\Big(\\mathbb{d}(C \\pmb A) B \\pmb A^T + C \\pmb A \\mathbb{d}(B \\pmb A^T)\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(C(\\mathbb{d}\\pmb A) B \\pmb A^T + C \\pmb A B \\mathbb{d}\\pmb A^T\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big( C \\pmb A B \\mathbb{d}\\pmb A^T\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big( C \\pmb A B (\\mathbb{d}\\pmb A)^T\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big(\\mathbb{d}\\pmb A (B^T \\pmb A^T C^T )\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big( B^T\\pmb A^T C^T \\mathbb{d}\\pmb A\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big[(B \\pmb A^T C + B^T\\pmb A^T C^T) \\mathbb{d}\\pmb A\\Big]\\\\\\\\ \\end{aligned} \\] \u6570\u5b57\u662f\u6b65\u9aa4 \u6c49\u5b57\u662f\u6bcf\u4e00\u6b65\u4f9d\u636e 01 -> 02 \u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u6027\u8d28 02 -> 03 \u8ff9\u7684\u4ea4\u6362\u5f8b 03 -> 04 \u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u6027\u8d28 04 -> 05 \u77e9\u9635\u5fae\u5206\u7684\u4e58\u79ef\u6cd5\u5219 05 -> 06 \u5939\u5c42\u997c 06 -> 07 \u8ff9\u7684\u7ebf\u6027\u6cd5\u5219, \u8ff9\u7684\u4ea4\u6362\u5f8b 07 -> 08 \u77e9\u9635\u5fae\u5206\u7684\u8f6c\u7f6e\u6cd5\u5219 08 -> 09 \u8f6c\u7f6e\u7684\u8ff9\u7b49\u4e8e\u539f\u77e9\u9635\u7684\u8ff9 09 -> 10 \u8ff9\u7684\u4ea4\u6362\u5f8b 10 -> 11 \u8ff9\u7684\u7ebf\u6027\u6cd5\u5219 \u7ed3\u5408\u524d\u9762\u7684 \\((II.1)\\) \u5f0f\u53ef\u5f97: \\[ \\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}=B \\pmb A^T C + B^T\\pmb A^T C^T \\] \u518d\u7ed3\u5408\u524d\u9762\u7684 \\((II.2)\\) \u5f0f\u53ef\u5f97: \\[ \\begin{aligned} \\nabla _{\\pmb A}f(\\pmb A) & = \\Big(\\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}\\Big)^T \\\\\\\\ & = (B \\pmb A^T C + B^T\\pmb A^T C^T)^T\\\\\\\\ & = C^T \\pmb A B^T + C \\pmb A B \\end{aligned} \\] \u8bc1\u6bd5\u3002 IV. \\(\\nabla_A|A|=|A|(A^{-1})^T\\\\\\\\\\) \u8bc1\u660e: \u7531 \u77e9\u9635\u884c\u5217\u5f0f\u5fae\u5206 \u5373(25.2.1)\u5f0f \u7684\u8bc1\u660e\u53ef\u77e5: \\[ \\mathbb{d} |\\pmb A| = \\operatorname{tr}(|\\pmb A|\\pmb A^{-1}\\mathbb{d}\\pmb A) \\] \u518d\u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(24)\u5f0f \u53ef\u5f97\uff1a \\[ \\frac{\\partial |\\pmb A|}{\\partial \\pmb A^T}=|\\pmb A|\\pmb A^{-1} \\] \u56e0\u6b64, \\[ \\begin{aligned} \\nabla _{\\pmb A}|\\pmb A| & = (\\frac{\\partial |\\pmb A|}{\\partial \\pmb A^T})^T \\\\\\\\ &=(|\\pmb A|\\pmb A^{-1})^T\\\\\\\\ &=|\\pmb A|(\\pmb A^{-1})^T \\end{aligned} \\] \u8bc1\u6bd5\u3002 \u81f3\u6b64, \u5434\u6069\u8fbe\u8001\u5e08\u7701\u7565\u7684\u8bc1\u660e, \u5168\u90e8\u8bc1\u660e\u5b8c\u6bd5\u3002\u4e0b\u9762\u7ee7\u7eed\u8bb0\u7b14\u8bb0\u3002 To make our martix notation more concrete, let us now explain in detail the meaning of the first of these equations. Suppose we have some fixed matrix \\(B \\in \\mathbb{R}^{n\\times m}\\) . We can then define a function \\(f :\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}\\) according to \\(f(A)=AB\\) . Note that this definition makes sense, because if \\(A \\in \\mathbb{R}^{m\\times n}\\) , then \\(AB\\) is a square matrix, and we can apply the trace operator to it; thus, \\(f\\) does indeed map from \\(\\mathbb{R}^{m\\times n}\\) to \\(\\mathbb{R}\\) . We can then apply our definition of matrix derivatives to find \\(\\nabla _Af(A)\\) , which will itself by an m-by-n matrix. Equation (1) above states that the \\((i,j)\\) entry of this matrix will given by the \\((i,j)\\) -entry of \\(B^T\\) , or equivalently, by \\(B_{j,i}\\) . \u2002 The proofs of Equation (1-3) are reasonably simply, and are left as an exercise to the reader. Equation (4) can be derived using adjoint representation of the inverse of a martix. 7.2 Least squares revisited Armed with the tools of matrix derivatives, let us now proceed to find in closed-form the value of \\(\\theta\\) that minimizes \\(J(\\theta)\\) . We begin by re-writing \\(J\\) in matrix-vectorial notation. \u2002Giving a training set, define the design matrix \\(X\\) to be the m-by-n matrix (actually m-by-n+1, if we include the intercept term) that contains the training examples' input values in its row: \\[ X = \\begin{bmatrix} --- & (x^{(1)})^T & --- \\\\\\\\ --- & (x^{(2)})^T & --- \\\\\\\\ & \\vdots & \\\\\\\\ --- & (x^{(m)})^T & --- \\end{bmatrix} \\] Also, let \\(\\vec{y}\\) be the m-dimensional vector containing all the target values from the training set: \\[ \\vec{y}=\\begin{bmatrix} y^{(1)}\\\\\\\\ y^{(2)}\\\\\\\\ \\vdots\\\\\\\\ y^{(m)} \\end{bmatrix} \\] Now, since \\(h_\\theta\\big(x^{(i)}\\big)=(x^{(i)})^T\\theta\\) , we can easily verify that \\[ \\begin{aligned} X\\theta-\\vec{y} &= \\begin{bmatrix} (x^{(1)})^T\\theta \\\\\\\\ \\vdots \\\\\\\\ (x^{(m)})^T\\theta \\end{bmatrix} - \\begin{bmatrix} y^{(1)}\\\\\\\\ \\vdots\\\\\\\\ y^{(m)} \\end{bmatrix}\\\\\\\\ & = \\begin{bmatrix} (x^{(1)})^T\\theta- y^{(1)} \\\\\\\\ \\vdots \\\\\\\\ (x^{(m)})^T\\theta- y^{(m)} \\end{bmatrix} \\end{aligned} \\] Thus, using the fact for a vector \\(z\\) , we have that \\(z^Tz=\\sum_{i}z_i^2\\) . \\[ \\begin{aligned} \\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})&=\\frac{1}{2}\\sum_{i=1}^m\\Big(h_\\theta(x^{(i)})- y^{(i)} \\Big)^2\\\\\\\\ &=J(\\theta) \\end{aligned} \\] Finally , to minimize \\(J\\) , lets find its derivatives with respect to \\(\\theta\\) . Combining Equations (2) and (3) , we find that \\[ \\nabla _{A^T}\\operatorname{tr}(ABA^TC)=B^TA^TC^T+BA^TC \\\\\\\\ \\tag{5} \\] Hence, \\[ \\begin{aligned} \\nabla _\\theta J(\\theta) & = \\nabla _\\theta \\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y}) \\\\\\\\ & = \\frac{1}{2} \\nabla _\\theta(\\theta^TX^TX\\theta-\\theta^TX^T \\vec{y}-\\vec{y}^TX \\theta+\\vec{y}^T \\vec{y})\\\\\\\\ & = \\frac{1}{2} \\nabla _\\theta \\operatorname{tr}(\\theta^TX^TX\\theta-\\theta^TX^T \\vec{y}-\\vec{y}^TX \\theta+\\vec{y}^T \\vec{y})\\\\\\\\ & = \\frac{1}{2} \\nabla _\\theta\\Big(\\operatorname{tr}(\\theta^TX^TX\\theta)-2\\operatorname{tr}(\\vec{y}^TX \\theta)\\Big)\\\\\\\\ & = \\frac{1}{2}(X^TX \\theta+X^TX \\theta-2X^T \\vec{y})\\\\\\\\ & = X^TX \\theta-X^T \\vec{y} \\end{aligned} \\] In the third step, we used the fact that the trace if a real number is just the real number; the fourth step used the fact that \\(\\operatorname{tr}(A)=\\operatorname{tr}(A^T)\\) , and the fifth step used Equation (5) with \\(A^T=\\theta\\) , \\(B=B^T=X^TX\\) , and \\(C=I\\) , and Equation (1). To minimize \\(J\\) , we set its derivatives to zero, and obtain the normal equations: \\[ X^TX \\theta=X^T \\vec{y} \\] Thus, the value of \\(\\theta\\) that minimize \\(J(\\theta)\\) is given in closed form by the equation \\[ \\theta = (X^TX)^{-1}X^T \\vec{y} \\] 8. \u6b63\u89c4\u65b9\u7a0b\u53ca\u4e0d\u53ef\u9006\u6027 \u53c2\u8003\u89c6\u9891: 4 - 7 - Normal Equation Noninvertibility (Optional) (6 min).mkv \u5728\u8fd9\u6bb5\u89c6\u9891\u4e2d\u8c08\u8c08\u6b63\u89c4\u65b9\u7a0b ( normal equation )\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u4e0d\u53ef\u9006\u6027\u3002 \u6211\u4eec\u8981\u8bb2\u7684\u95ee\u9898\u5982\u4e0b\uff1a \\(\\theta ={{\\left( {X^{T}}X \\right)}^{-1}}{X^{T}}y\\) \u5f53\u8ba1\u7b97 \\(\\theta\\) = inv(X'X ) X'y \uff0c\u90a3\u5bf9\u4e8e\u77e9\u9635 \\(X'X\\) \u7684\u7ed3\u679c\u662f\u4e0d\u53ef\u9006\u7684\u60c5\u51b5\u548b\u529e\u5462? \u6211\u4eec\u90fd\u77e5\u9053\uff0c\u6709\u4e9b\u77e9\u9635\u53ef\u9006( invertible )\uff0c\u800c\u6709\u4e9b\u77e9\u9635\u4e0d\u53ef\u9006( non-invertible )\u3002\u6211\u4eec\u79f0\u90a3\u4e9b\u4e0d\u53ef\u9006\u77e9\u9635\u4e3a\u5947\u5f02( singular )\u6216\u9000\u5316( dgenerate )\u77e9\u9635\u3002 \u9996\u5148, \u8bf4\u4e00\u4e0b \\(\\pmb X'\\pmb X\\) \u4e0d\u53ef\u9006\u7684\u539f\u56e0 \u3002 \u7279\u5f81\u503c\u7ebf\u6027\u76f8\u5173 \u4f8b\u5982\uff0c\u5728\u9884\u6d4b\u4f4f\u623f\u4ef7\u683c\u65f6\uff0c\u5982\u679c \\({x_{1}}\\) \u662f\u4ee5\u82f1\u5c3a\u4e3a\u5c3a\u5bf8\u89c4\u683c\u8ba1\u7b97\u7684\u623f\u5b50\uff0c \\({x_{2}}\\) \u662f\u4ee5\u5e73\u65b9\u7c73\u4e3a\u5c3a\u5bf8\u89c4\u683c\u8ba1\u7b97\u7684\u623f\u5b50\uff0c\u540c\u65f6\uff0c\u4f60\u4e5f\u77e5\u90531\u7c73\u7b49\u4e8e3.28\u82f1\u5c3a ( \u56db\u820d\u4e94\u5165\u5230\u4e24\u4f4d\u5c0f\u6570 )\uff0c\u8fd9\u6837\uff0c\u4f60\u7684\u8fd9\u4e24\u4e2a\u7279\u5f81\u503c\u5c06\u59cb\u7ec8\u6ee1\u8db3\u7ea6\u675f\uff1a \\({x_{1}}={x_{2}}*{{\\left( 3.28 \\right)}^{2}}\\) \u3002 \u5b9e\u9645\u4e0a\uff0c\u5982\u679c\u4f60\u7528\u8fd9\u6837\u7684\u4e00\u4e2a\u7ebf\u6027\u65b9\u7a0b\uff0c\u6765\u5c55\u793a\u90a3\u4e24\u4e2a\u76f8\u5173\u8054\u7684\u7279\u5f81\u503c\uff0c\u77e9\u9635 \\(X'X\\) \u5c06\u662f\u4e0d\u53ef\u9006\u7684\u3002 \u7279\u5f81\u503c\u7684\u6570\u91cf\u5c0f\u4e8e\u8bad\u7ec3\u96c6\u7684\u6570\u91cf \u5177\u4f53\u5730\u8bf4\uff0c\u5728 \\(m\\) \u5c0f\u4e8e\u6216\u7b49\u4e8en\u7684\u65f6\u5019\uff0c\u4f8b\u5982\uff0c\u6709 \\(m\\) \u7b49\u4e8e10\u4e2a\u7684\u8bad\u7ec3\u6837\u672c\u4e5f\u6709 \\(n\\) \u7b49\u4e8e100\u7684\u7279\u5f81\u6570\u91cf\u3002\u8981\u627e\u5230\u9002\u5408\u7684 \\((n +1)\\) \u7ef4\u53c2\u6570\u77e2\u91cf \\(\\theta\\) \uff0c\u8fd9\u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a101\u7ef4\u7684\u77e2\u91cf\uff0c\u5c1d\u8bd5\u4ece10\u4e2a\u8bad\u7ec3\u6837\u672c\u4e2d\u627e\u5230\u6ee1\u8db3101\u4e2a\u53c2\u6570\u7684\u503c\uff0c\u8fd9\u5de5\u4f5c\u53ef\u80fd\u4f1a\u8ba9\u4f60\u82b1\u4e0a\u4e00\u9635\u5b50\u65f6\u95f4\uff0c\u4f46\u8fd9\u5e76\u4e0d\u603b\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\u3002\u56e0\u4e3a\uff0c\u6b63\u5982\u6211\u4eec\u6240\u770b\u5230\u4f60\u53ea\u670910\u4e2a\u6837\u672c\uff0c\u4ee5\u9002\u5e94\u8fd9100\u6216101\u4e2a\u53c2\u6570\uff0c\u6570\u636e\u8fd8\u662f\u6709\u4e9b\u5c11\u3002 \u8fd9\u76f8\u5f53\u4e8e\u8bf4\u662f\u591a\u5143\u65b9\u7a0b\u7ec4\u4e2d\u672a\u77e5\u6570\u7684\u4e2a\u6570\u8fdc\u5927\u4e8e\u65b9\u7a0b\u7684\u4e2a\u6570\u3002 \u7a0d\u540e\u6211\u4eec\u5c06\u770b\u5230\uff0c \u5982\u4f55\u4f7f\u7528\u5c0f\u6570\u636e\u6837\u672c\u4ee5\u5f97\u5230\u8fd9100\u6216101\u4e2a\u53c2\u6570 \uff0c \u901a\u5e38\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528 \u4e00\u79cd\u53eb\u505a \u6b63\u5219\u5316 \u7684\u7ebf\u6027\u4ee3\u6570\u65b9\u6cd5\uff0c \u901a\u8fc7\u5220\u9664\u67d0\u4e9b\u7279\u5f81\u6216\u8005\u662f\u4f7f\u7528\u67d0\u4e9b\u6280\u672f\uff0c\u6765\u89e3\u51b3\u5f53 \\(m\\) \u6bd4 \\(n\\) \u5c0f\u7684\u65f6\u5019\u7684\u95ee\u9898 \u3002\u5373\u4f7f\u4f60\u6709\u4e00\u4e2a\u76f8\u5bf9\u8f83\u5c0f\u7684\u8bad\u7ec3\u96c6\uff0c\u4e5f\u53ef\u4f7f\u7528\u5f88\u591a\u7684\u7279\u5f81\u6765\u627e\u5230\u5f88\u591a\u5408\u9002\u7684\u53c2\u6570\u3002 \u603b\u4e4b\u5f53\u4f60\u53d1\u73b0\u7684\u77e9\u9635 \\(X'X\\) \u7684\u7ed3\u679c\u662f\u5947\u5f02\u77e9\u9635\uff0c\u6216\u8005\u627e\u5230\u7684\u5176\u5b83\u77e9\u9635\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u6211\u4f1a\u5efa\u8bae\u4f60\u8fd9\u4e48\u505a\u3002 \u9996\u5148\uff0c \u770b\u7279\u5f81\u503c\u91cc\u662f\u5426\u6709\u4e00\u4e9b\u591a\u4f59\u7684\u7279\u5f81 \uff0c\u50cf\u8fd9\u4e9b \\({x_{1}}\\) \u548c \\({x_{2}}\\) \u662f \u7ebf\u6027\u76f8\u5173 \u7684\uff0c\u4e92\u4e3a\u7ebf\u6027\u51fd\u6570\u3002\u540c\u65f6\uff0c\u5f53\u6709\u4e00\u4e9b\u591a\u4f59\u7684\u7279\u5f81\u65f6\uff0c\u53ef\u4ee5 \u5220\u9664 \u8fd9\u4e24\u4e2a\u91cd\u590d\u7279\u5f81\u91cc\u7684 \u5176\u4e2d\u4e00\u4e2a \uff0c\u65e0\u987b\u4e24\u4e2a\u7279\u5f81\u540c\u65f6\u4fdd\u7559\uff0c\u5c06\u89e3\u51b3\u4e0d\u53ef\u9006\u6027\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9996\u5148\u5e94\u8be5\u901a\u8fc7\u89c2\u5bdf\u6240\u6709\u7279\u5f81\u68c0\u67e5\u662f\u5426\u6709\u591a\u4f59\u7684\u7279\u5f81\uff0c\u5982\u679c\u6709\u591a\u4f59\u7684\u5c31\u5220\u9664\u6389\uff0c\u76f4\u5230\u4ed6\u4eec\u4e0d\u518d\u662f\u591a\u4f59\u7684\u4e3a\u6b62\uff0c\u5982\u679c \u7279\u5f81\u6570\u91cf \u5b9e\u5728 \u592a\u591a \uff0c\u6211\u4f1a \u7528\u8f83\u5c11\u7684\u7279\u5f81 \u6765 \u53cd\u6620\u5c3d\u53ef\u80fd\u591a\u5185\u5bb9 \uff0c \u5426\u5219 \u6211\u4f1a\u8003\u8651 \u4f7f\u7528\u6b63\u89c4\u5316\u65b9\u6cd5 \u3002 \u5982\u679c\u77e9\u9635 \\(X'X\\) \u662f\u4e0d\u53ef\u9006\u7684\uff0c\uff08\u901a\u5e38\u6765\u8bf4\uff0c\u4e0d\u4f1a\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\uff09\uff0c\u5982\u679c\u5728 Octave \u91cc\uff0c\u53ef\u4ee5\u7528\u4f2a\u9006\u51fd\u6570 pinv() \u6765\u5b9e\u73b0\u3002\u8fd9\u79cd\u4f7f\u7528\u4e0d\u540c\u7684\u7ebf\u6027\u4ee3\u6570\u5e93\u7684\u65b9\u6cd5\u88ab\u79f0\u4e3a\u4f2a\u9006\u3002\u5373\u4f7f \\(X'X\\) \u7684\u7ed3\u679c\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u4f46\u7b97\u6cd5\u6267\u884c\u7684\u6d41\u7a0b\u662f\u6b63\u786e\u7684\u3002\u603b\u4e4b\uff0c\u51fa\u73b0\u4e0d\u53ef\u9006\u77e9\u9635\u7684\u60c5\u51b5\u6781\u5c11\u53d1\u751f\uff0c\u6240\u4ee5\u5728\u5927\u591a\u6570\u5b9e\u73b0\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u51fa\u73b0\u4e0d\u53ef\u9006\u7684\u95ee\u9898\u4e0d\u5e94\u8be5\u8fc7\u591a\u7684\u5173\u6ce8 \\({X^{T}}X\\) \u662f\u4e0d\u53ef\u9006\u7684\u3002 \u4e60\u9898 && \u53c2\u8003\u7b54\u6848 \u7b2c\u4e00\u9898 \u5047\u8bbem=4\u4e2a\u5b66\u751f\u4e0a\u4e86\u4e00\u8282\u8bfe, \u6709\u671f\u4e2d\u8003\u8bd5\u548c\u671f\u672b\u8003\u8bd5\u3002\u4f60\u5df2\u7ecf\u6536\u96c6\u4e86\u4ed6\u4eec\u5728\u4e24\u6b21\u8003\u8bd5\u4e2d\u7684\u5206\u6570\u6570\u636e\u96c6\uff0c\u5982\u4e0b\u6240\u793a\uff1a \u671f\u4e2d\u5f97\u5206 (\u671f\u4e2d\u5f97\u5206)^2 \u671f\u672b\u5f97\u5206 89 7921 96 72 5184 74 94 8836 87 69 4761 78 \u4f60\u60f3\u7528\u591a\u9879\u5f0f\u56de\u5f52\u6765\u9884\u6d4b\u4e00\u4e2a\u5b66\u751f\u7684\u671f\u4e2d\u8003\u8bd5\u6210\u7ee9\u3002\u5177\u4f53\u5730 \u8bf4, \u5047\u8bbe\u4f60\u60f3\u62df\u5408\u4e00\u4e2a \\(h_ \\theta (x) = \\theta _0+\\theta _1x_1++\\theta _2x_1\\) \u7684\u6a21\u578b, \u5176\u4e2dx1\u662f\u671f\u4e2d\u5f97\u5206, x2\u662f\uff08\u671f\u4e2d\u5f97\u5206\uff09^2\u3002\u6b64\u5916, \u4f60\u8ba1\u5212\u540c\u65f6\u4f7f\u7528\u7279\u5f81\u7f29\u653e\uff08\u9664\u4ee5\u7279\u5f81\u7684\u201c\u6700\u5927\u503c-\u6700\u5c0f\u503c\u201d\u6216\u8303\u56f4\uff09\u548c\u5747\u503c\u5f52\u4e00\u5316\u3002 \u90a3\u4e48\u6807\u51c6\u5316\u540e\u7684 \\(x_2^{(4)}\\) \u7279\u5f81\u503c\u662f\u591a\u5c11\uff1f\uff08\u63d0\u793a\uff1a\u671f\u4e2d=89\uff0c\u671f\u672b=96\u662f\u8bad\u7ec3\u793a\u4f8b1\uff09 \u7b2c\u4e8c\u9898 \u7528 \\(\\alpha=0.3\\) \u8fdb\u884c15\u6b21\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3, \u6bcf\u6b21\u8fed\u4ee3 \\(j(\\theta)\\) \u540e\u8ba1\u7b97\u3002\u4f60\u4f1a\u53d1\u73b0 \\(j(\\theta)\\) \u7684\u503c\u4e0b\u964d\u7f13\u6162, \u5e76\u4e14\u572815\u6b21\u8fed\u4ee3\u540e\u4ecd\u5728\u4e0b\u964d\u3002\u57fa\u4e8e\u6b64, \u4ee5\u4e0b\u54ea\u4e2a\u7ed3\u8bba\u4f3c\u4e4e\u6700\u53ef\u4fe1\uff1f A. \\(\\alpha=0.3\\) \u662f\u5b66\u4e60\u7387\u7684\u6709\u6548\u9009\u62e9\u3002 B. \u4e0e\u5176\u4f7f\u7528 \\(\\alpha\\) \u5f53\u524d\u503c, \u4e0d\u5982\u5c1d\u8bd5\u66f4\u5c0f\u7684 \\(\\alpha\\) \u503c\uff08\u6bd4\u5982 \\(\\alpha=0.1\\) \uff09 C. \u4e0e\u5176\u4f7f\u7528 \\(\\alpha\\) \u5f53\u524d\u503c, \u4e0d\u5982\u5c1d\u8bd5\u66f4\u5927\u7684 \\(\\alpha\\) \u503c\uff08\u6bd4\u5982 \\(\\alpha=1.0\\) \uff09 \u7b2c\u4e09\u9898 \u5047\u8bbe\u60a8\u6709m=14\u4e2a\u8bad\u7ec3\u793a\u4f8b, \u6709n=3\u4e2a\u7279\u6027\uff08\u4e0d\u5305\u62ec\u9700\u8981\u53e6\u5916\u6dfb\u52a0\u7684\u6052\u4e3a1\u7684\u622a\u8ddd\u9879\uff09, \u6b63\u89c4\u65b9\u7a0b\u662f \\(\\theta=(X^TX)^{-1}X^Ty\\) \u3002\u5bf9\u4e8e\u7ed9\u5b9am\u548cn\u7684\u503c, \u8fd9\u4e2a\u65b9\u7a0b\u4e2d \\(\\theta, X, y\\) \u7684\u7ef4\u6570\u5206\u522b\u662f\u591a\u5c11\uff1f A. \\(X \\ 14 \\times 3, y \\ 14\\times 1, \\ \\theta 3 \\times 3\\) B. \\(X \\ 14 \\times 4, y \\ 14\\times 1, \\ \\theta 4 \\times 1\\) C. \\(X \\ 14 \\times 3, y \\ 14\\times 1, \\ \\theta 3 \\times 1\\) D. \\(X \\ 14 \\times 4, y \\ 14\\times 4, \\ \\theta 4 \\times 4\\) \u7b2c \u56db \u9898 \u5047\u8bbe\u60a8\u6709\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u793a\u4f8b\u6709m=1000000\u4e2a\u793a\u4f8b\u548cn=200000\u4e2a\u7279\u6027\u3002\u4f60\u60f3\u7528\u591a\u5143\u7ebf\u6027\u56de\u5f52\u6765\u62df\u5408\u53c2\u6570 \\(\\theta\\) \u5230\u6211\u4eec\u7684\u6570\u636e\u3002\u4f60\u66f4\u5e94\u8be5\u7528\u68af\u5ea6\u4e0b\u964d\u8fd8\u662f\u6b63\u89c4\u65b9\u7a0b\uff1f A. \u68af\u5ea6\u4e0b\u964d\uff0c\u56e0\u4e3a\u6b63\u89c4\u65b9\u7a0b\u4e2d \\(\\theta=(X^TX)^{-1}\\) \u4e2d\u8ba1\u7b97\u975e\u5e38\u6162 B. \u6b63\u89c4\u65b9\u7a0b\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u6c42\u89e3\u7684\u6709\u6548\u65b9\u6cd5 C. \u68af\u5ea6\u4e0b\u964d\uff0c\u56e0\u4e3a\u5b83\u603b\u662f\u6536\u655b\u5230\u6700\u4f18 \\(\\theta\\) D. \u6b63\u89c4\u65b9\u7a0b\uff0c\u56e0\u4e3a\u68af\u5ea6\u4e0b\u964d\u53ef\u80fd\u65e0\u6cd5\u627e\u5230\u6700\u4f18 \\(\\theta\\) \u7b2c \u4e94 \u9898 \u4ee5\u4e0b\u54ea\u4e9b\u662f\u4f7f\u7528\u7279\u5f81\u7f29\u653e\u7684\u539f\u56e0\uff1f A. \u5b83\u53ef\u4ee5\u9632\u6b62\u68af\u5ea6\u4e0b\u964d\u9677\u5165\u5c40\u90e8\u6700\u4f18 B. \u5b83\u901a\u8fc7\u964d\u4f4e\u68af\u5ea6\u4e0b\u964d\u7684\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u6210\u672c\u6765\u52a0\u901f\u68af\u5ea6\u4e0b\u964d C. \u5b83\u901a\u8fc7\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u6765\u83b7\u5f97\u4e00\u4e2a\u597d\u7684\u89e3\uff0c\u4ece\u800c\u52a0\u5feb\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u901f\u5ea6 D. \u5b83\u9632\u6b62\u77e9\u9635 \\(X^TX\\) \uff08\u7528\u4e8e\u6b63\u89c4\u65b9\u7a0b\uff09\u4e0d\u53ef\u9006\uff08\u5947\u5f02/\u9000\u5316\uff09 \u53c2\u8003\u7b54\u6848 \u7b2c\u4e00\u9898\uff1a-0.47 \u7b2c\u4e8c\u9898\uff1aC \u7b2c\u4e09\u9898\uff1aB \u7b2c\u56db\u9898\uff1aA \u7b2c\u4e94\u9898\uff1aC \u4e0a\u673a\u7ec3\u4e60 In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices. \u2002 The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house. 1\u3001Feature Normalization \u505a\u4e4b\u524d\u770b\u4e00\u4e0b\u6570\u636e\u7684\u5927\u6982\u6837\u5b50 1 2 3 4 5 6 7 import pandas as pd # path : \u6570\u636e\u5b58\u653e\u7684\u8def\u5f84 path = \"ex1data2.txt\" # names : \u6307\u5b9a\u5217\u540d; head=None : \u539f\u6570\u636e\u6ca1\u6709\u5217\u540d data = pd . read_csv ( path , header = None , names = [ \"Size\" , \"Bedrooms\" , \"Price\" ]) # \u5c55\u793a\u6570\u636e\u7684\u540e\u4e94\u884c data . tail () By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When feature differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. Substract the value of each feature from the dataset. After substracting the mean, additionally scale (divide) the feature values by their repective \"standar deviations\". \u2002 The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within \\(\\pm 2\\) standard deviation of the mean); this is an alternative to taking the range of values(max-min). \u2002 You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix X corresponds to one feature. \u5bf9\u6570\u636e\u6807\u51c6\u5316\u5e76\u63d2\u5165x0 1 2 3 4 5 6 7 8 9 # \u5bf9\u6570\u636e\u8fdb\u884c\u7279\u5f81-\u6807\u51c6\u5316 # \u6ce8\u610fpandas\u6309\u5217\u540d\u53d6\u5217\u662f\u4e24\u4e2a\u4e2d\u62ec\u53f7 data2 [[ \"Size\" , \"Bedrooms\" ]] = ( data2 [[ \"Size\" , \"Bedrooms\" ]] - data2 [[ \"Size\" , \"Bedrooms\" ]] . mean ()) / data2 [[ \"Size\" , \"Bedrooms\" ]] . std () # \u611f\u89c9\u628a\u623f\u4ef7\u7a0d\u5fae\u7f29\u653e\u4e00\u4e0b\uff0c\u4e0d\u7136\u5bf9\u68af\u5ea6\u5f71\u54cd\u592a\u5927 data2 [[ \"Price\" ]] = data2 [[ \"Price\" ]] / 100000 # \u63d2\u5165x0 data2 . insert ( 0 , 'Ones' , 1 ) data2 . tail () 2\u3001Gradient Descent Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged. Implement Note: In the multivariate case, the cost function can also be written in the following vectorized form: \\(J(\\theta)=\\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)\\) where \\(X=\\bigl( \\begin{bmatrix} --- & (x^{(1)})^T & --- \\\\\\\\ --- & (x^{(2)})^T & ---\\\\\\\\ & \\vdots & \\\\\\\\ --- & (x^{(m)})^T & --- \\end{bmatrix} \\bigr)\\) \\(y= \\begin{bmatrix} y^{(1)} \\\\\\\\ y^{(2)} \\\\\\\\ \\vdots \\\\\\\\ y^{(m)} \\end{bmatrix}\\) . The vectorized version is efficient when you are working with numerical cumputing tools like numpy. If you are an expert with matrix operations, you can prove to yourself that the two forms are equivalent. \u8fd0\u884c\u591a\u53d8\u91cf\u68af\u5ea6\u4e0b\u964d 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import numpy as np # \u83b7\u53d6\u6570\u636e\u7684\u603b\u5217\u6570 cols = data2 . shape [ 1 ] # \u5c06\u6570\u636e\u62c6\u5206\u5e76\u8f6c\u4e3andarry\u5206\u522b\u653e\u5230X,y\u4e2d X = np . array ( data2 . iloc [:, 0 : cols - 1 ] . values ) y = np . array ( data2 . iloc [:, cols - 1 ] . values ) # \u4e2d\u95f4\u770b\u4e00\u4e0bX\uff0cy\u4ee5\u53caX\uff0cy\u7684\u7ef4\u5ea6\u662f\u5426\u6b63\u786e # X.shape,y.shape # X, y # \u521d\u59cb\u5316theta, \u5b66\u4e60\u7387alpha\uff0c\u8fed\u4ee3\u6b21\u6570iters theta = np . zeros ([ 3 ,]) alpha = 0.01 iters = 1000 # X.shape,y.shape,theta.shape # \u5b9a\u4e49\u4ee3\u4ef7\u51fd\u6570\u5e76\u8ba1\u7b97 def computeCost ( X , y , theta ): m = X . shape [ 0 ] # print((np.dot(X,theta)-y).shape) inner = np . dot (( np . dot ( X , theta ) - y ) . T , ( np . dot ( X , theta ) - y )) return np . sum ( inner ) / ( 2 * m ) # \u5b9a\u4e49\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 def gradientDescent ( X , y , theta , alpha , iterations ): m = X . shape [ 0 ] # m: \u6837\u672c\u7684\u603b\u4e2a\u6570 n = len ( theta ) # n: theta\u7684\u603b\u4e2a\u6570 # \u7528\u4e00\u4e2a\u5411\u91cf\u6765\u8bb0\u5f55\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u6240\u6709\u7684cost\u503c costs = np . zeros ( iterations ) for i in range ( iterations ): costs [ i ] = computeCost ( X , y , theta ) # theta\u662f\u51e0\u4e2a\u5c31\u8981\u66f4\u65b0\u51e0\u4e2a for j in range ( n ): theta [ j ] = theta [ j ] - alpha * \\ ( 1 / m ) * np . sum (( np . dot ( X , theta ) - y ) * X [:, j ]) return theta , costs # \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u5206\u522b\u63a5\u6536\u66f4\u65b0\u540e\u7684theta\u503c\u548c\u6bcf\u4e00\u6b65\u8fed\u4ee3\u7684cost\u503c theta_hat , costs = gradientDescent ( X , y , theta , alpha , iters ) # get the cost (error) of the model costs [ iters - 1 ], theta_hat \u8f93\u51fa ( 0.20435384903675252 , array ([ 3.40397964 , 1.09859063 , - 0.05879178 ])) 2.1 Selecting learning rates In this part of the exercise, you will get to try out different learning rates for the dataset and find a learning rate that converages quickly. \u2002 We recommend trying values of the learning rate \\(\\alpha\\) on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on). You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve. \u2002 Notice the changes in the convergence curves as the learning rate changes. With a small learning rate, you should find that gradient descent takes a very long time to converge to the optimal value. Conversely, with a large learning rate, gradient descent might not converge or might even diverge! 3\u3001Noramal Equation In the lecture videos, you learned that the closed-form solution to linear regression is \\[ \\theta = (X^TX)^{-1}X^Ty \\] \u2002 Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \u201cloop until convergence\u201d like in gradient descent. \u518d\u6b21\u63d0\u9192\uff1a \u68af\u5ea6\u4e0b\u964d \uff1a\u9700\u8981\u9009\u62e9\u5b66\u4e60\u7387 \\(\\alpha\\) \uff0c\u9700\u8981\u591a\u6b21\u8fed\u4ee3\uff0c\u5f53\u7279\u5f81\u6570\u91cfn\u5927\u65f6\u4e5f\u80fd\u8f83\u597d\u9002\u7528\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u6a21\u578b \u6b63\u89c4\u65b9\u7a0b \uff1a\u4e0d\u9700\u8981\u9009\u62e9\u5b66\u4e60\u7387 \\(\\alpha\\) \uff0c\u4e00\u6b21\u8ba1\u7b97\u5f97\u51fa\uff0c\u9700\u8981\u8ba1\u7b97 \\(X^TX\\) \uff0c\u5982\u679c\u7279\u5f81\u6570\u91cfn\u8f83\u5927\u5219\u8fd0\u7b97\u4ee3\u4ef7\u5927\uff0c\u56e0\u4e3a\u77e9\u9635\u9006\u7684\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a\ud835\udc42(\ud835\udc5b3)\uff0c \u901a\u5e38\u6765\u8bf4\u5f53\ud835\udc5b\u5c0f\u4e8e10000 \u65f6 \u8fd8\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\uff0c\u53ea\u9002\u7528\u4e8e \u7ebf\u6027\u6a21\u578b \uff0c\u4e0d\u9002\u5408\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7b49\u5176\u4ed6\u6a21\u578b\u3002 \u6b63\u89c4\u65b9\u7a0b\u8ba1\u7b97theta 1 2 3 # \u5229\u7528\u516c\u5f0f\u4e00\u6b65\u8ba1\u7b97\u51fa\u53c2\u6570theta theta_ne = np . linalg . inv ( X . T @X ) @X . T @y #X.T@X\u7b49\u4ef7\u4e8eX.T.dot(X) theta_ne \u8f93\u51fa array ([ 3.4041266 , 1.1063105 , - 0.06649474 ]) \u6211\u4eec\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8ba1\u7b97\u7684theta\u4e3a\uff1a [ 3.40397964, 1.09859063, -0.05879178] \u4e24\u8005\u76f8\u5dee\u4e0d\u662f\u7279\u522b\u5927\uff0c\u8bf4\u660e\u6211\u4eec\u68af\u5ea6\u4e0b\u964d\u6a21\u578b\u8fd8\u662f\u6bd4\u8f83\u51c6\u786e\u7684\uff0c\u54c8\u54c8\u54c8\u3002\u81f3\u6b64\uff0c\u6211\u5df2\u7ecf\u638c\u63e1\u4e86\u4e24\u4e2a\u7b97\u6cd5\uff0c\u68af\u5ea6\u4e0b\u964d\u6a21\u578b\u548c\u6b63\u89c4\u65b9\u7a0b\u6c42\u89e3\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002\u6211\u771f\u68d2\uff0c\u660e\u5929\u5f00\u59cb\u5b66\u4e60\u65b0\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u52a0\u6cb9\uff01\uff01\uff01","title":"\u4e8c. \u591a\u53d8\u91cf\u7ebf\u6027\u56de\u5f52"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#1","text":"\u53c2\u8003\u89c6\u9891: 4 - 1 - Multiple Features (8 min).mkv \u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u5355\u53d8\u91cf/\u7279\u5f81\u7684\u56de\u5f52\u6a21\u578b\uff0c\u73b0\u5728\u6211\u4eec\u5bf9\u623f\u4ef7\u6a21\u578b\u589e\u52a0\u66f4\u591a\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u623f\u95f4\u6570\u697c\u5c42\u7b49\uff0c\u6784\u6210\u4e00\u4e2a\u542b\u6709\u591a\u4e2a\u53d8\u91cf\u7684\u6a21\u578b\uff0c\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u4e3a \\(\\left( {x_{1}},{x_{2}},...,{x_{n}} \\right)\\) \u3002 Size (feet2) Number of bedrooms Number of floors Age of home (years) Price ($1000) 2104 5 1 45 460 1416 3 2 40 232 1534 3 2 30 315 852 2 1 36 178 \u2026 \u2026 \u2026 \u2026 \u2026 \u589e\u6dfb\u66f4\u591a\u7279\u5f81\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e00\u7cfb\u5217\u65b0\u7684\u6ce8\u91ca\uff1a \\(n\\) \u4ee3\u8868\u7279\u5f81\u7684\u6570\u91cf \\({x^{\\left( i \\right)}}\\) \u4ee3\u8868\u7b2c \\(i\\) \u4e2a\u8bad\u7ec3\u5b9e\u4f8b\uff0c\u662f\u7279\u5f81\u77e9\u9635\u4e2d\u7684\u7b2c \\(i\\) \u884c\uff0c\u662f\u4e00\u4e2a \u5411\u91cf \uff08 vector \uff09\u3002 \u6bd4\u65b9\u8bf4\uff0c\u4e0a\u56fe\u7684 \\({x}^{(2)}\\text{=}\\begin{bmatrix} 1416\\\\\\ 3\\\\\\ 2\\\\\\ 40 \\end{bmatrix}\\) \uff0c \\({x}_{j}^{\\left( i \\right)}\\) \u4ee3\u8868\u7279\u5f81\u77e9\u9635\u4e2d\u7b2c \\(i\\) \u884c\u7684\u7b2c \\(j\\) \u4e2a\u7279\u5f81\uff0c\u4e5f\u5c31\u662f\u7b2c \\(i\\) \u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u7b2c \\(j\\) \u4e2a\u7279\u5f81\u3002 \u5982\u4e0a\u56fe\u7684 \\(x_{2}^{\\left( 2 \\right)}=3,x_{3}^{\\left( 2 \\right)}=2\\) \uff0c \u652f\u6301\u591a\u53d8\u91cf\u7684\u5047\u8bbe \\(h\\) \u8868\u793a\u4e3a\uff1a \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \uff0c \u8fd9\u4e2a\u516c\u5f0f\u4e2d\u6709 \\(n+1\\) \u4e2a\u53c2\u6570\u548c \\(n\\) \u4e2a\u53d8\u91cf\uff0c\u4e3a\u4e86\u4f7f\u5f97\u516c\u5f0f\u80fd\u591f\u7b80\u5316\u4e00\u4e9b\uff0c\u5f15\u5165 \\(x_{0}=1\\) \uff0c\u5219\u516c\u5f0f\u8f6c\u5316\u4e3a\uff1a \\(h_{\\theta} \\left( x \\right)={\\theta_{0}}{x_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \u6b64\u65f6\u6a21\u578b\u4e2d\u7684\u53c2\u6570\u662f\u4e00\u4e2a \\(n+1\\) \u7ef4\u7684\u5411\u91cf\uff0c\u4efb\u4f55\u4e00\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e5f\u90fd\u662f \\(n+1\\) \u7ef4\u7684\u5411\u91cf\uff0c\u7279\u5f81\u77e9\u9635 \\(X\\) \u7684\u7ef4\u5ea6\u662f \\(m*(n+1)\\) \u3002 \u56e0\u6b64\u516c\u5f0f\u53ef\u4ee5\u7b80\u5316\u4e3a\uff1a \\(h_{\\theta} \\left( x \\right)={\\theta^{T}}X\\) \uff0c\u5176\u4e2d\u4e0a\u6807 \\(T\\) \u4ee3\u8868\u77e9\u9635\u8f6c\u7f6e\u3002 \u6ce8\u610f\uff1a \u4ece\u4e0a\u56fe\u53ef\u77e5, \\(\\theta\\) , \\(X\\) \u6211\u4eec\u662f\u7528 \u5217\u5411\u91cf \u6765\u6807\u8bb0\u3002\u8ba1\u7b97 \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \u7684\u65f6\u5019, \u53ef\u4ee5\u5c06 \\(h\\) \u8868\u793a\u4e3a \\(\\theta^TX.\\)","title":"1. \u591a\u7ef4\u7279\u5f81"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#2","text":"\u53c2\u8003\u89c6\u9891: 4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv \u5feb\u901f\u56de\u987e\u6211\u4eec\u7684\u8bb0\u53f7\uff0c\u5e76\u7528\u5411\u91cf\u7b80\u5316\u3002 Hypothesis: \\(h_\\theta(x)=\\theta_0+\\theta_1x+\\theta_2x+...+\\theta_nx\\) \u7b80\u8bb0\u4e3a\uff1a \\(h_\\theta(x)=\\theta^TX\\) Parameters: \\(\\theta_0,\\theta_1,\\theta_2,...\\theta_n\\) \u7b80\u8bb0\u4e3a\uff1a \\(\\theta\\) , \\(n+1\\ dimension\\ vector\\) Cost Function: \\[ J(\\theta_0,\\theta_1,\\ \\theta_2,...\\theta_n)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 \\] \u7b80\u8bb0\u4e3a\uff1a \\[ J(\\theta)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 \\] Gradient descent: Repeat { \u200b \\({\\theta_{j}}:={\\theta_{j}}-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,...\\theta_n)\\) \u200b } (simultaneously update for every j=0, 1, 2, ..., n) \u7b80\u8bb0\u4e3a\uff1a\u628a\u4e0a\u9762\u7684 \\(J(\\theta_0,...\\theta_n)\\) \u6362\u6210 \\(J(\\theta)\\) \u4e0e\u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u7c7b\u4f3c\uff0c\u5728\u591a\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u6211\u4eec\u4e5f\u6784\u5efa\u4e00\u4e2a\u4ee3\u4ef7\u51fd\u6570\uff0c\u5219\u8fd9\u4e2a\u4ee3\u4ef7\u51fd\u6570\u662f\u6240\u6709\u5efa\u6a21\u8bef\u5dee\u7684\u5e73\u65b9\u548c\uff0c\u5373\uff1a \\(J\\left( {\\theta_{0}},{\\theta_{1}}...{\\theta_{n}} \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( h_{\\theta} \\left({x}^{\\left( i \\right)} \\right)-{y}^{\\left( i \\right)} \\right)}^{2}}}\\) \uff0c \u5176\u4e2d\uff1a \\(h_{\\theta}\\left( x \\right)=\\theta^{T}X={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) \uff0c \u6211\u4eec\u7684\u76ee\u6807\u548c\u5355\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u4e2d\u4e00\u6837\uff0c\u662f\u8981\u627e\u51fa\u4f7f\u5f97\u4ee3\u4ef7\u51fd\u6570\u6700\u5c0f\u7684\u4e00\u7cfb\u5217\u53c2\u6570\u3002 \u591a\u53d8\u91cf\u7ebf\u6027\u56de\u5f52\u7684\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e3a\uff1a \u5f53 \\(n>=1\\) \u65f6\uff0c \\({{\\theta }_{0}}:={{\\theta }_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}\\) \\({{\\theta }_{1}}:={{\\theta }_{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}\\) \\({{\\theta }_{2}}:={{\\theta }_{2}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}\\) ... \\({{\\theta }_{n}}:={{\\theta }_{n}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{n}^{(i)}\\) \u6211\u4eec\u5f00\u59cb\u968f\u673a\u9009\u62e9\u4e00\u7cfb\u5217\u7684\u53c2\u6570\u503c\uff0c\u8ba1\u7b97\u6240\u6709\u7684\u9884\u6d4b\u7ed3\u679c\u540e\uff0c\u518d\u7ed9\u6240\u6709\u7684\u53c2\u6570\u4e00\u4e2a\u65b0\u7684\u503c\uff0c\u5982\u6b64\u5faa\u73af\u76f4\u5230\u6536\u655b\u3002 \u4ee3\u7801\u793a\u4f8b\uff1a \u8ba1\u7b97\u4ee3\u4ef7\u51fd\u6570 \\(J\\left( \\theta \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {h_{\\theta}}\\left( {x^{(i)}} \\right)-{y^{(i)}} \\right)}^{2}}}\\) \u5176\u4e2d\uff1a \\({h_{\\theta}}\\left( x \\right)={\\theta^{T}}X={\\theta_{0}}{x_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}}+...+{\\theta_{n}}{x_{n}}\\) Python \u4ee3\u7801\uff1a def computeCost ( X , y , theta ): inner = np . power ((( X * theta . T ) - y ), 2 ) return np . sum ( inner ) / ( 2 * len ( X )) \u4e0b\u9762\uff0c\u6211\u4eec\u5f00\u59cb\u63a8\u5bfc \\(\\frac{\\partial}{\\partial\\theta}J(\\theta)\\) \u3002\u8fd9\u91cc\u4e3a\u4e86\u666e\u904d\u6027\uff0c\u8fd8\u539f\u4e86 \\(\\theta\\) \u7684\u666e\u904d\u60c5\u51b5\uff0c\u4e5f\u5373\u6709n\u4e2a \\(\\theta\\) \u7684\u60c5\u51b5\u3002 Hypothesis: \\(h_\\theta(x)=\\theta^Tx=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\\) Parameters: \\(\\theta_0,\\theta_1,...,\\theta_n\\) Cost function: \\[ J(\\theta_0,\\theta_1,...,\\theta_n)=\\frac {1} {2m}\\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 \\] Gradient descent: Repeat{ $$ \\theta_j :=\\theta_j-\\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1,...,\\theta_n) $$ \u200b }\uff08simultaneously update for every j=0, 1, 2, ..., n)\uff09 \u63a8\u5bfc\uff1a \\(\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1,...,\\theta_n)=?\\) Let's first work it for the case if we have only one training example \\((x,y)\\) , so that we can neglect the sum in the definition \\(J\\) . We have: $$ \\begin{split} \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1,...,\\theta_n) & = \\frac {1} {2}\\frac{\\partial}{\\partial \\theta_j}\\Big(h_\\theta(x)-y\\Big)^2 \\\\ & = 2\\cdot\\frac{1}{2}\\cdot(h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial \\theta_j}(h_\\theta(x)-y) \\\\ & = (h_\\theta(x)-y)\\cdot\\frac{\\partial}{\\partial \\theta_j}(\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n) \\\\ & = (h_\\theta(x)-y)\\cdot x_j \\end{split} $$","title":"2. \u591a\u53d8\u91cf\u68af\u5ea6\u4e0b\u964d"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#3-","text":"\u53c2\u8003\u89c6\u9891: 4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv \u5728\u6211\u4eec\u9762\u5bf9\u591a\u7ef4\u7279\u5f81\u95ee\u9898\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u4fdd\u8bc1\u8fd9\u4e9b\u7279\u5f81\u90fd\u5177\u6709\u76f8\u8fd1\u7684\u5c3a\u5ea6\uff0c\u8fd9\u5c06\u5e2e\u52a9\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u5feb\u5730\u6536\u655b\u3002 \u4ee5\u623f\u4ef7\u95ee\u9898\u4e3a\u4f8b\uff0c\u5047\u8bbe\u6211\u4eec\u4f7f\u7528\u4e24\u4e2a\u7279\u5f81\uff0c\u623f\u5c4b\u7684\u5c3a\u5bf8\u548c\u623f\u95f4\u7684\u6570\u91cf\uff0c\u5c3a\u5bf8\u7684\u503c\u4e3a 0-2000\u5e73\u65b9\u82f1\u5c3a\uff0c\u800c\u623f\u95f4\u6570\u91cf\u7684\u503c\u5219\u662f0-5\uff0c\u4ee5\u4e24\u4e2a\u53c2\u6570\u5206\u522b\u4e3a\u6a2a\u7eb5\u5750\u6807\uff0c\u7ed8\u5236\u4ee3\u4ef7\u51fd\u6570\u7684\u7b49\u9ad8\u7ebf\u56fe\u80fd\uff0c\u770b\u51fa\u56fe\u50cf\u4f1a\u663e\u5f97\u5f88\u6241\uff0c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u9700\u8981\u975e\u5e38\u591a\u6b21\u7684\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u3002 \u89e3\u51b3\u7684\u65b9\u6cd5\u662f\u5c1d\u8bd5\u5c06\u6240\u6709\u7279\u5f81\u7684\u5c3a\u5ea6\u90fd\u5c3d\u91cf\u7f29\u653e\u5230-1\u52301\u4e4b\u95f4\u3002\u5982\u56fe\uff1a \u89c6\u9891\u91cc\u5434\u6069\u8fbe\u8001\u5e08\u7684\u65b9\u6cd5\u662f\u4ee4\uff1a \\({{x}_{n}}=\\frac{{{x}_{n}}-mean}{{max(x)-min(x)}}\\) , \u5176\u4e2d \\(mean\\) \u662f\u5e73\u5747\u503c\uff0c \\(max(x)-min(x)\\) \u5206\u522b\u662f\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002 \u66f4\u901a\u7528\u7684\u662f\u4ee4\uff1a \\({{x}_{n}}=\\frac{{{x}_{n}}-{mean}}{\\sigma}\\) \uff0c\u5176\u4e2d \\(mean\\) \u662f\u5e73\u5747\u503c\uff0c \\(\\sigma\\) \u662f\u6807\u51c6\u5dee\u3002 python\u91cc\u7684api\uff1a sklearn.preprocessing.StandardScaler() \u5904\u7406\u4e4b\u540e\u6bcf\u5217\u6765\u8bf4 \u6240\u6709\u6570\u636e\u90fd\u805a\u96c6\u5728\u5747\u503c0\u9644\u8fd1\u6807\u51c6\u5dee\u5dee\u4e3a1 StandardScaler.fit_transform(X) X:numpy array\u683c\u5f0f\u7684\u6570\u636e[n_samples,n_features] \u8fd4\u56de\u503c\uff1a\u8f6c\u6362\u540e\u7684\u5f62\u72b6\u76f8\u540c\u7684array \u90e8\u5206\u53c2\u8003\u4ee3\u7801\uff1a from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def linear_model1 (): # 1.\u83b7\u53d6\u6570\u636e boston = load_boston () # 2. \u6570\u636e\u96c6\u5212\u5206 x_train , x_test , y_train , y_test = train_test_split ( boston . data , boston . target , test_size = 0.2 ) # 3. \u7279\u5f81\u5de5\u7a0b-\u6807\u51c6\u5316 transfer = StandardScaler () x_train = transfer . fit_transform ( x_train ) x_test = transfer . fit_transform ( x_test )","title":"3. \u68af\u5ea6\u4e0b\u964d\u6cd5 - \u7279\u5f81\u7f29\u653e"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#4-","text":"\u53c2\u8003\u89c6\u9891: 4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6536\u655b\u6240\u9700\u8981\u7684\u8fed\u4ee3\u6b21\u6570\u6839\u636e\u6a21\u578b\u7684\u4e0d\u540c\u800c\u4e0d\u540c\uff0c\u6211\u4eec\u4e0d\u80fd\u63d0\u524d\u9884\u77e5\uff0c\u6211\u4eec\u53ef\u4ee5\u7ed8\u5236\u8fed\u4ee3\u6b21\u6570\u548c\u4ee3\u4ef7\u51fd\u6570\u7684\u56fe\u8868\u6765\u89c2\u6d4b\u7b97\u6cd5\u5728\u4f55\u65f6\u8d8b\u4e8e\u6536\u655b\u3002 \u4e5f\u6709\u4e00\u4e9b\u81ea\u52a8\u6d4b\u8bd5\u662f\u5426\u6536\u655b\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u5c06\u4ee3\u4ef7\u51fd\u6570\u7684\u53d8\u5316\u503c\u4e0e\u67d0\u4e2a\u9600\u503c\uff08\u4f8b\u59820.001\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f46\u901a\u5e38\u770b\u5de6\u4e0a\u65b9\u8fd9\u6837\u7684\u56fe\u8868\u66f4\u597d\u3002 \u4e0d\u6b63\u786e\u7684\u5b66\u4e60\u7387\uff0c\u4f1a\u4ea7\u751f\u5de6\u4fa7\u4e0a\u4e0b\u4e24\u4e2a\u56fe\u50cf\u3002 \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u6bcf\u6b21\u8fed\u4ee3\u53d7\u5230\u5b66\u4e60\u7387\u7684\u5f71\u54cd\uff0c\u5982\u679c\u5b66\u4e60\u7387 \\(\\alpha\\) \u8fc7\u5c0f\uff0c\u5219\u8fbe\u5230\u6536\u655b\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u4f1a\u975e\u5e38\u9ad8\uff1b\u5982\u679c\u5b66\u4e60\u7387 \\(\\alpha\\) \u8fc7\u5927\uff0c\u6bcf\u6b21\u8fed\u4ee3\u53ef\u80fd\u4e0d\u4f1a\u51cf\u5c0f\u4ee3\u4ef7\u51fd\u6570\uff0c\u53ef\u80fd\u4f1a\u8d8a\u8fc7\u5c40\u90e8\u6700\u5c0f\u503c\u5bfc\u81f4\u65e0\u6cd5\u6536\u655b\u3002 \u901a\u5e38\u53ef\u4ee5\u8003\u8651\u5c1d\u8bd5\u4e9b\u5b66\u4e60\u7387\uff1a \\(\\alpha= 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\\)","title":"4. \u68af\u5ea6\u4e0b\u964d\u6cd5 - \u5b66\u4e60\u7387"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#5","text":"\u53c2\u8003\u89c6\u9891: 4 - 5 - Features and Polynomial Regression (8 min).mkv \u5982\u623f\u4ef7\u9884\u6d4b\u95ee\u9898\uff0c \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}\\times{frontage}+{\\theta_{2}}\\times{depth}\\) \u5f53\u6211\u4eec\u771f\u6b63\u5e94\u7528\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u521b\u9020\u81ea\u5df1\u7684\u7279\u5f81\u5373\uff1a \\({x_{1}}=frontage\\) \uff08\u4e34\u8857\u5bbd\u5ea6\uff09\uff0c \\({x_{2}}=depth\\) \uff08\u7eb5\u5411\u6df1\u5ea6\uff09\uff0c \\(x=frontage*depth=area\\) \uff08\u9762\u79ef\uff09\uff0c \u5219\uff1a \\({h_{\\theta}}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}x\\) \u3002 \u7ebf\u6027\u56de\u5f52\u5e76\u4e0d\u9002\u7528\u4e8e\u6240\u6709\u6570\u636e\uff0c\u6709\u65f6\u6211\u4eec\u9700\u8981\u5176\u4ed6\u6a21\u578b\u6765\u9002\u5e94\u6211\u4eec\u7684\u6570\u636e\uff0c\u6bd4\u5982\u4e00\u4e2a\u4e8c\u6b21\u65b9\u6a21\u578b\uff1a \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}^2}\\) \u6216\u8005\u4e09\u6b21\u65b9\u6a21\u578b\uff1a \\(h_{\\theta}\\left( x \\right)={\\theta_{0}}+{\\theta_{1}}{x_{1}}+{\\theta_{2}}{x_{2}^2}+{\\theta_{3}}{x_{3}^3}\\) \u4ece\u4e0a\u9762\u56fe\u53f3\u4fa7\uff0c\u53ef\u4ee5\u770b\u51fa\u5982\u679c\u6211\u4eec\u91c7\u7528\u591a\u9879\u5f0f\u56de\u5f52\u6a21\u578b\uff0c\u5728\u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u524d\u7279\u5f81\u7f29\u653e\u7684\u91cd\u8981\u6027\u4e86\u3002 \u901a\u5e38\u6211\u4eec\u9700\u8981\u5148\u89c2\u5bdf\u6570\u636e\u7136\u540e\u518d\u51b3\u5b9a\u51c6\u5907\u5c1d\u8bd5\u600e\u6837\u7684\u6a21\u578b\u3002 \u53e6\u5916\uff0c\u6211\u4eec\u53ef\u4ee5\u4ee4\uff1a \\({{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}\\) \uff0c\u4ece\u800c\u5c06\u6a21\u578b\u8f6c\u5316\u4e3a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002 \u6839\u636e\u51fd\u6570\u56fe\u5f62\u7279\u6027\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u4f7f\uff1a \\({{{h}}_{\\theta}}(x)={{\\theta }_{0}}\\text{+}{{\\theta }_{1}}(size)+{{\\theta}_{2}}{{(size)}^{2}}\\) \u6216\u8005: \\({{{h}}_{\\theta}}(x)={{\\theta }_{0}}\\text{+}{{\\theta }_{1}}(size)+{{\\theta }_{2}}\\sqrt{size}\\)","title":"5. \u7279\u5f81\u548c\u591a\u9879\u5f0f\u56de\u5f52"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#6","text":"\u53c2\u8003\u89c6\u9891: 4 - 6 - Normal Equation (16 min).mkv \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u90fd\u5728\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u4f46\u662f\u5bf9\u4e8e\u67d0\u4e9b\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff0c\u6b63\u89c4\u65b9\u7a0b\u65b9\u6cd5\u662f\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5982\uff1a \u6b63\u89c4\u65b9\u7a0b\u662f\u901a\u8fc7\u6c42\u89e3\u4e0b\u9762\u7684\u65b9\u7a0b\u6765\u627e\u51fa\u4f7f\u5f97\u4ee3\u4ef7\u51fd\u6570\u6700\u5c0f\u7684\u53c2\u6570\u7684\uff1a \\(\\frac{\\partial}{\\partial{\\theta_{j}}}J\\left( {\\theta_{j}} \\right)=0\\) \u3002 \u5047\u8bbe\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u7279\u5f81\u77e9\u9635\u4e3a \\(X\\) \uff08\u5305\u542b\u4e86 \\({{x}_{0}}=1\\) \uff09\u5e76\u4e14\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u7ed3\u679c\u4e3a\u5411\u91cf \\(y\\) \uff0c\u5219\u5229\u7528\u6b63\u89c4\u65b9\u7a0b\u89e3\u51fa\u5411\u91cf \\(\\theta ={{\\left( {X^T}X \\right)}^{-1}}{X^{T}}y\\) \u3002 \u4ee5\u4e0b\u9762\u8868\u683c\u6570\u636e\u4e3a\u4f8b \\(m=4\\) \uff1a \\(x_0\\) Size (feet2) Number of bedrooms Number of floors Age of home (years) Price ($1000) 1 2104 5 1 45 460 1 1416 3 2 40 232 1 1534 3 2 30 315 1 852 2 1 36 178 \\(X\\ \\text{=}\\begin{bmatrix}1 & 2104 & 5 & 1 & 45\\\\1 & 1416 & 3 & 2 & 40\\\\ 1 &1534 & 3 & 2 & 30\\\\ 1 & 852 & 2 & 1 & 36\\end{bmatrix}\\) \uff0c \\(y\\ \\text{=}\\begin{bmatrix} 460\\\\232\\\\315\\\\178\\end{bmatrix}\\) \\(X\u7ef4\u5ea6\uff1a(m,n+1),\\ y\u7684\u7ef4\u5ea6\uff1a(m,1)\\) \u8fd9\u65f6\u5019\u6c42\u89e3 \\(\\theta\\) \u53ea\u9700\u4e00\u6b65: \\(\\theta ={{\\left( {X^T}X \\right)}^{-1}}{X^{T}}y\\) \u3002 \u5c06\u4e0a\u9762\u7684\u4f8b\u5b50\u63a8\u5e7f\u5230\u4e00\u822c\u60c5\u51b5 \u6ce8\uff1a\u5bf9\u4e8e\u90a3\u4e9b\u4e0d\u53ef\u9006\u7684\u77e9\u9635\uff08\u901a\u5e38\u662f\u56e0\u4e3a\u7279\u5f81\u4e4b\u95f4\u4e0d\u72ec\u7acb\uff0c\u5982\u540c\u65f6\u5305\u542b\u82f1\u5c3a\u4e3a\u5355\u4f4d\u7684\u5c3a\u5bf8\u548c\u7c73\u4e3a\u5355\u4f4d\u7684\u5c3a\u5bf8\u4e24\u4e2a\u7279\u5f81\uff0c\u4e5f\u6709\u53ef\u80fd\u662f\u7279\u5f81\u6570\u91cf\u5927\u4e8e\u8bad\u7ec3\u96c6\u7684\u6570\u91cf\uff09\uff0c\u6b63\u89c4\u65b9\u7a0b\u65b9\u6cd5\u662f\u4e0d\u80fd\u7528\u7684\u3002 \u5343\u4e07\u8981\u6ce8\u610f\u8fd9\u91cc\u7684\u8bbe\u8ba1\u77e9\u9635X\u5b83\u7684\u6784\u6210\uff0c\u8bbe\u8ba1\u5b8c\u6210\u540e\uff0c\u5047\u8bbe\u51fd\u6570\u53ef\u4ee5\u5411\u91cf\u5316\u4e3a \\(h_\\theta(x)=X\\theta\\) \u68af\u5ea6\u4e0b\u964d\u4e0e\u6b63\u89c4\u65b9\u7a0b\u7684\u6bd4\u8f83\uff1a \u68af\u5ea6\u4e0b\u964d \u6b63\u89c4\u65b9\u7a0b \u9700\u8981\u9009\u62e9\u5b66\u4e60\u7387 \\(\\alpha\\) \u4e0d\u9700\u8981 \u9700\u8981\u591a\u6b21\u8fed\u4ee3 \u4e00\u6b21\u8fd0\u7b97\u5f97\u51fa \u5f53\u7279\u5f81\u6570\u91cf \\(n\\) \u5927\u65f6\u4e5f\u80fd\u8f83\u597d\u9002\u7528 \u9700\u8981\u8ba1\u7b97 \\({{\\left( {{X}^{T}}X \\right)}^{-1}}\\) \u5982\u679c\u7279\u5f81\u6570\u91cfn\u8f83\u5927\u5219\u8fd0\u7b97\u4ee3\u4ef7\u5927\uff0c\u56e0\u4e3a\u77e9\u9635\u9006\u7684\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a \\(O\\left( {{n}^{3}} \\right)\\) \uff0c\u901a\u5e38\u6765\u8bf4\u5f53 \\(n\\) \u5c0f\u4e8e10000 \u65f6\u8fd8\u662f\u53ef\u4ee5\u63a5\u53d7\u7684 \u9002\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u6a21\u578b \u53ea\u9002\u7528\u4e8e\u7ebf\u6027\u6a21\u578b\uff0c\u4e0d\u9002\u5408\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7b49\u5176\u4ed6\u6a21\u578b \u603b\u7ed3\u4e00\u4e0b\uff0c\u53ea\u8981\u7279\u5f81\u53d8\u91cf\u7684\u6570\u76ee\u5e76\u4e0d\u5927\uff0c\u6807\u51c6\u65b9\u7a0b\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u8ba1\u7b97\u53c2\u6570$\\theta $\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002\u5177\u4f53\u5730\u8bf4\uff0c \u53ea\u8981\u7279\u5f81\u53d8\u91cf\u6570\u91cf\u5c0f\u4e8e\u4e00\u4e07\uff0c\u6211\u901a\u5e38\u4f7f\u7528\u6b63\u89c4\u65b9\u7a0b\u6cd5\uff0c\u800c\u4e0d\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u3002 \u968f\u7740\u6211\u4eec\u8981\u8bb2\u7684\u5b66\u4e60\u7b97\u6cd5\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4f8b\u5982\uff0c\u5f53\u6211\u4eec\u8bb2\u5230\u5206\u7c7b\u7b97\u6cd5\uff0c\u50cf\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\uff0c\u6211\u4eec\u4f1a\u770b\u5230\uff0c\u5b9e\u9645\u4e0a\u5bf9\u4e8e\u90a3\u4e9b\u7b97\u6cd5\uff0c\u5e76\u4e0d\u80fd\u4f7f\u7528\u6807\u51c6\u65b9\u7a0b\u6cd5\u3002\u5bf9\u4e8e\u90a3\u4e9b\u66f4\u590d\u6742\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u6211\u4eec\u5c06\u4e0d\u5f97\u4e0d\u4ecd\u7136\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u3002\u56e0\u6b64\uff0c\u68af\u5ea6\u4e0b\u964d\u6cd5\u662f\u4e00\u4e2a\u975e\u5e38\u6709\u7528\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u7528\u5728\u6709\u5927\u91cf\u7279\u5f81\u53d8\u91cf\u7684\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u3002\u6216\u8005\u6211\u4eec\u4ee5\u540e\u5728\u8bfe\u7a0b\u4e2d\uff0c\u4f1a\u8bb2\u5230\u7684\u4e00\u4e9b\u5176\u4ed6\u7684\u7b97\u6cd5\uff0c\u56e0\u4e3a\u6807\u51c6\u65b9\u7a0b\u6cd5\u4e0d\u9002\u5408\u6216\u8005\u4e0d\u80fd\u7528\u5728\u5b83\u4eec\u4e0a\u3002\u4f46\u5bf9\u4e8e\u8fd9\u4e2a\u7279\u5b9a\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u6807\u51c6\u65b9\u7a0b\u6cd5\u662f\u4e00\u4e2a\u6bd4\u68af\u5ea6\u4e0b\u964d\u6cd5\u66f4\u5feb\u7684\u66ff\u4ee3\u7b97\u6cd5\u3002\u6240\u4ee5\uff0c\u6839\u636e\u5177\u4f53\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f60\u7684\u7279\u5f81\u53d8\u91cf\u7684\u6570\u91cf\uff0c\u8fd9\u4e24\u79cd\u7b97\u6cd5\u90fd\u662f\u503c\u5f97\u5b66\u4e60\u7684\u3002 \u6b63\u89c4\u65b9\u7a0b\u7684 python \u5b9e\u73b0\uff1a import numpy as np def normalEqn ( X , y ): theta = np . linalg . inv ( X . T @X ) @X . T @y #X.T@X\u7b49\u4ef7\u4e8eX.T.dot(X) return theta \u4ee5\u4e0b\u662f\u6b63\u89c4\u65b9\u7a0b\u82f1\u6587\u7248(\u6709\u8be6\u7ec6\u63a8\u5bfc-\u81ea\u5df1\u52a0\u4e0a\u7684)\uff0c\u5176\u4ed6\u5185\u5bb9\u6765\u6e90\u4e8e2006\u5e74\u9ebb\u7701\u7406\u5de5cs 2009\u673a\u5668\u5b66\u4e60\u7684note\u3002","title":"6. \u6b63\u89c4\u65b9\u7a0b"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#7-the-normal-equations","text":"p { text-align: justify; /*\u6587\u672c\u4e24\u7aef\u5bf9\u9f50*/ } Gradient descent gives one way of minimizing \\(J\\) . Lets discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize \\(J\\) by explicitly taking its derivatives with respect to the \\(\u03b8_j's\\) , and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, lets introduce some notation for doing calculus with matrices.","title":"7. The normal equations"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#71-matrix-derivatives","text":"For a function \\(f\\) : \\(\\mathbb{R^{m\\times n}} \\rightarrow \\mathbb{R}\\) mapping from \\(\\text{m-by-n}\\) matrices to the real numbers, we define the derivative of f with respect to A to be: \\[ \\nabla _Af(A)=\\begin{bmatrix} \\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1n}} \\\\\\\\ \\vdots & \\ddots & \\vdots \\\\\\\\ \\frac{\\partial f}{\\partial A_{m1}} & \\cdots & \\frac{\\partial f}{\\partial A_{mn}}\\end{bmatrix} \\] Thus, the gradient \\(\\nabla _Af(A)\\) is itself an \\(\\text{m-by-n}\\) matrix, whose \\((i,j)\\) -element is \\(\\frac{\\partial f}{\\partial A_{ij}}\\) . For example, suppose \\(A =\\bigl( \\begin{smallmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{smallmatrix} \\bigr)\\) is a 2-by-2 matrix, and the function \\(f\\) : \\(\\mathbb{R^{2\\times 2}}\\rightarrow \\mathbb{R}\\) is given by \\[ f(A)=\\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22} \\] Here, \\(A_{ij}\\) denotes the \\((i,j)\\) entry of the matrix \\(A\\) . We then have \\[ \\nabla _Af(A)=\\begin{bmatrix} \\frac{3}{2} & 10A_{12}\\\\\\\\ A_{22} & A_{21} \\end{bmatrix} \\] \u2002 We also introduce the trace opertator, written by \"tr.\". For an \\(\\text{n-by-n}\\) (square) matrix A, the trace of A is defined to be the sum of its diagonal entries: \\[ \\operatorname{tr}A=\\sum_{i=1}^nA_{ii} \\] If \\(a\\) is a real number (i.e., a 1-by-1 matrix), then \\(\\operatorname{tr} a = a\\) . (If you haven't seen this \"opertator notation\" before, you should think of the trace of \\(A\\) as \\(\\operatorname{tr}(A)\\) , or as application of the \"trace\" function to the matrix \\(A\\) . It's more commonly written without the parentheses, however.\uff09 \u2002 The trace opertator has the property that for two matrices \\(A\\) and \\(B\\) such that \\(AB\\) is square, we have that \\(\\operatorname{tr}(AB)=\\operatorname{tr}(BA)\\) . \u8bc1\u660e\u8bf7\u67e5\u770b\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6570\u5b66\u77e5\u8bc6\u4e2d\u5173\u4e8e\u8ff9\u7684\u4ea4\u6362\u5f8b\u7684\u8bc1\u660e\u3002 As corollaries of this, we also have, e.g., \\[ \\operatorname{tr}(ABC)=\\operatorname{tr}(CAB)=\\operatorname{tr}(BCA)\\] \\[ \\operatorname{tr}(ABCD)=\\operatorname{tr}(DABC)=\\operatorname{tr}(CDAB)=\\operatorname{tr}(BCDA)\\] The following properties of the trace operator are also easily verified. Here, \\(A\\) and \\(B\\) are square matrices, and \\(a\\) is a real number: \\[ \\operatorname{tr}(A) = \\operatorname{tr}(A^T)\\] \\[ \\operatorname{tr}(A+B) = \\operatorname{tr}(A)+\\operatorname{tr}(B)\\] \\[ \\operatorname{tr}(aA) = a \\operatorname{tr}(A)\\] \u2002 We now state without proof some facts of matrix derivatives (we won\u2019t need some of these until later this quarter). Equation (4) applies only to non-singular square matrices A, where |A| denotes the determinant of A. We have: \\[ \\nabla_A \\operatorname{tr}(AB)=B^T\\\\\\\\ \\tag{1}\\] \\[ \\nabla_{A^T}f(A) = \\big(\\nabla_Af(A)\\big)^T \\\\\\\\\\tag{2}\\] \\[ \\nabla_A \\operatorname{tr}ABA^TC=CAB+C^TAB^T \\\\\\\\\\tag{3} \\] \\[\\nabla_A|A|=|A|(A^{-1})^T\\\\\\\\\\tag{4}\\] \u5434\u6069\u8fbe\u8001\u5e08\u4e0d\u7ed9\u4f60\u4eec\u8bc1\u660e\uff0c\u6211\u6765\u7ed9\u4f60\u4eec\u8bc1\u660e \u8bf7\u5728\u770b\u4e0b\u9762\u63a8\u5bfc\u4e4b\u524d, \u52a1\u5fc5\u5148\u770b\u61c2\u6211\u8fd9\u4e2a\u77e5\u8bc6\u5e93\u4e2d \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6570\u5b66\u77e5\u8bc6 \u2002---\u2002( \u4e00. \u77e9\u9635\u6c42\u5bfc\u672c\u8d28 &\u4e8c. \u77e9\u9635\u6c42\u5bfc )\u3002 I. \\(\\nabla_A \\operatorname{tr}(AB)=B^T\\) \u8bc1\u660e: \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u4e2d---(6)\u5f0f\u7684\u8bc1\u660e \u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff1a \u5bf9\u4e8e\u4e24\u4e2a\u9636\u6570\u90fd\u662f \\(m \\times n\\) \u7684\u77e9\u9635 \\(C_{m \\times n}, D_{m \\times n}\\) \u5176\u4e2d \u4e00\u4e2a\u77e9\u9635\u4e58\u4ee5\uff08\u5de6\u4e58\u53f3\u4e58\u90fd\u53ef\u4ee5\uff09\u53e6\u4e00\u4e2a\u77e9\u9635\u7684 \u8f6c\u7f6e \u7684\u8ff9\uff0c\u672c\u8d28\u662f \\(C_{m \\times n}, D_{m \\times n}\\) \u4e24\u4e2a\u77e9\u9635\u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0 \u3002 \u6240\u4ee5\u8fd9\u91cc \\(\\operatorname{tr}(AB)\\) \u76f8\u5f53\u4e8e\u5c31\u662f \\(A\\) \u548c \\(B^T\\) \u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u5143\u7d20 \u76f8\u4e58\u5e76\u76f8\u52a0 \u3002 \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times m}\\) \u3002 \u63a5\u7740, \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(11)\u5f0f \u6211\u4eec\u53ef\u4ee5\u77e5\u9053: \u8fd9\u91cc\u6c42 \\(\\nabla_A \\operatorname{tr}(AB)\\) \u76f8\u5f53\u4e8e \\(\\operatorname{tr}(AB)\\) \u6309\u7167 \\(A\\) \u77e9\u9635\u5206\u5e03\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5143\u7d20\u6c42\u504f\u5bfc\u3002 \u6240\u4ee5, \u7efc\u4e0a\u6240\u8ff0, \\(\\nabla_A \\operatorname{tr}(AB)=B^T\\) \u3002 \u8bc1\u6bd5\u3002 II. \\(\\nabla_{A^T}f(A) = \\big(\\nabla_Af(A)\\big)^T\\) \u8bc1\u660e: \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(11)\u5f0f \u53ef\u77e5: \\[ \\begin{align} \\nabla_{A^T}f(A) &= \\begin{bmatrix} \\frac{\\partial f}{\\partial a_{11}} & \\cdots & \\frac{\\partial f}{\\partial a_{m1}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f}{\\partial x_{1n}} & \\cdots & \\frac{\\partial f}{\\partial x_{mn}} \\\\ \\end{bmatrix}\\\\\\\\ & = \\begin{bmatrix} \\frac{\\partial f}{\\partial a_{11}} & \\cdots & \\frac{\\partial f}{\\partial a_{1n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f}{\\partial x_{m1}} & \\cdots & \\frac{\\partial f}{\\partial x_{mn}} \\\\ \\end{bmatrix}\\\\\\\\ & = \\big(\\nabla_Af(A)\\big)^T \\end{align} \\] \u8bc1\u6bd5\u3002 III. \\(\\nabla_A \\operatorname{tr}ABA^TC=CAB+C^TAB^T\\) \u8bc1\u660e: \u9996\u5148, \u6211\u4eec\u8981\u660e\u786e\u8fd9\u91cc\u7684 \\(ABA^TC\\) \u662f\u5173\u4e8e \\(A\\) \u77e9\u9635\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570, \u6240\u4ee5, \u6211\u4eec\u53ef\u4ee5\u4ee4 \\(f(A)=ABA^TC\\) \u3002 \u6ce8\u610f: \u8fd9\u91cc\u7684\u6807\u8bb0, \u7531\u4e8e \\(A\\) \u5176\u5b9e\u662f\u77e9\u9635\u53d8\u5143, \u5e94\u8be5\u6807\u8bb0\u4e3a \\(f(\\pmb A)=\\pmb A B \\pmb A^TC\\) , \u6240\u4ee5\u540e\u9762\u7684\u63a8\u5bfc\u8fc7\u7a0b,\u6211\u4eec\u4e25\u8c28\u4e00\u4e9b, \u5c06\u77e9\u9635\u53d8\u5143 \\(A\\) \u6807\u8bb0\u4e3a \\(\\pmb A\\) \u3002 \u4ed4\u7ec6\u60f3\u4f60\u4f1a\u53d1\u73b0\uff0c\u5bf9\u4e8e\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f (\\pmb{A})\\) , \\(\\operatorname{tr}\\big( f(\\pmb A) \\big)=f(\\pmb A)\\) , \\(\\mathbb{d}f(\\pmb A)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb A) \\big)\\) \u6240\u4ee5\u6709 \\(\\mathbb{d}f(\\pmb A)=\\mathbb{d}\\big(\\operatorname{tr}f(\\pmb A)\\big)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb A) \\big)\\) \u3002 \u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(24)\u5f0f \u5373: \\[ \\mathbb{d}f(\\pmb{X})= \\operatorname{tr}\\Big(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\mathbb{d}\\pmb{X}\\Big) \\] \u6211\u4eec\u53ef\u4ee5\u628a\u4e00\u4e2a\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\u5199\u6210\u4e0a\u5f0f\uff0c\u6211\u4eec\u5c31\u627e\u5230\u4e86\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c, \u4e5f\u5373: \\[ \\mathbb{d}f(\\pmb{A})= \\operatorname{tr}\\Big(\\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}\\mathbb{d}\\pmb{A}\\Big)\\\\\\\\ \\tag{II.1} \\] \u7531\u6211\u4eec\u8bc1\u660e\u7684 II. \\(\\nabla_{A^T}f(A) = \\big(\\nabla_Af(A)\\big)^T\\) \u5f97: \\[ \\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T} = \\nabla _{\\pmb A^T}f(\\pmb A)=\\big(\\nabla _{\\pmb A}f(\\pmb A)\\big)^T \\] \u6240\u4ee5\u6211\u4eec\u8981\u6c42\u7684: \\[ \\nabla _{\\pmb A}f(\\pmb A)=\\Big(\\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}\\Big)^T \\\\\\\\ \\tag{II.2} \\] \u6700\u7ec8\u6211\u4eec\u7684\u4efb\u52a1\u5c31\u662f\u8f6c\u5316\u4e3a\u6c42 \\(\\mathbb{d}f(\\pmb{A})\\) \u7684\u5168\u5fae\u5206, \u4e0b\u9762\u5f00\u59cb\u63a8\u5bfc\uff1a \\[ \\begin{aligned} \\mathbb{d}f(\\pmb{A}) & = \\mathbb{d}\\pmb A B \\pmb A^TC \\\\\\\\ & = \\mathbb{d}\\operatorname{tr}(\\pmb A B \\pmb A^TC)\\\\\\\\ & = \\mathbb{d}\\operatorname{tr}(C \\pmb A B \\pmb A^T)\\\\\\\\ & = \\operatorname{tr}\\mathbb{d}(C \\pmb A B \\pmb A^T)\\\\\\\\ & = \\operatorname{tr}\\Big(\\mathbb{d}(C \\pmb A) B \\pmb A^T + C \\pmb A \\mathbb{d}(B \\pmb A^T)\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(C(\\mathbb{d}\\pmb A) B \\pmb A^T + C \\pmb A B \\mathbb{d}\\pmb A^T\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big( C \\pmb A B \\mathbb{d}\\pmb A^T\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big( C \\pmb A B (\\mathbb{d}\\pmb A)^T\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big(\\mathbb{d}\\pmb A (B^T \\pmb A^T C^T )\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big(B \\pmb A^T C\\mathbb{d}\\pmb A \\Big) + \\operatorname{tr}\\Big( B^T\\pmb A^T C^T \\mathbb{d}\\pmb A\\Big)\\\\\\\\ & = \\operatorname{tr}\\Big[(B \\pmb A^T C + B^T\\pmb A^T C^T) \\mathbb{d}\\pmb A\\Big]\\\\\\\\ \\end{aligned} \\] \u6570\u5b57\u662f\u6b65\u9aa4 \u6c49\u5b57\u662f\u6bcf\u4e00\u6b65\u4f9d\u636e 01 -> 02 \u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u6027\u8d28 02 -> 03 \u8ff9\u7684\u4ea4\u6362\u5f8b 03 -> 04 \u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u6027\u8d28 04 -> 05 \u77e9\u9635\u5fae\u5206\u7684\u4e58\u79ef\u6cd5\u5219 05 -> 06 \u5939\u5c42\u997c 06 -> 07 \u8ff9\u7684\u7ebf\u6027\u6cd5\u5219, \u8ff9\u7684\u4ea4\u6362\u5f8b 07 -> 08 \u77e9\u9635\u5fae\u5206\u7684\u8f6c\u7f6e\u6cd5\u5219 08 -> 09 \u8f6c\u7f6e\u7684\u8ff9\u7b49\u4e8e\u539f\u77e9\u9635\u7684\u8ff9 09 -> 10 \u8ff9\u7684\u4ea4\u6362\u5f8b 10 -> 11 \u8ff9\u7684\u7ebf\u6027\u6cd5\u5219 \u7ed3\u5408\u524d\u9762\u7684 \\((II.1)\\) \u5f0f\u53ef\u5f97: \\[ \\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}=B \\pmb A^T C + B^T\\pmb A^T C^T \\] \u518d\u7ed3\u5408\u524d\u9762\u7684 \\((II.2)\\) \u5f0f\u53ef\u5f97: \\[ \\begin{aligned} \\nabla _{\\pmb A}f(\\pmb A) & = \\Big(\\frac{\\partial f(\\pmb{A})}{\\partial \\pmb{A}^T}\\Big)^T \\\\\\\\ & = (B \\pmb A^T C + B^T\\pmb A^T C^T)^T\\\\\\\\ & = C^T \\pmb A B^T + C \\pmb A B \\end{aligned} \\] \u8bc1\u6bd5\u3002 IV. \\(\\nabla_A|A|=|A|(A^{-1})^T\\\\\\\\\\) \u8bc1\u660e: \u7531 \u77e9\u9635\u884c\u5217\u5f0f\u5fae\u5206 \u5373(25.2.1)\u5f0f \u7684\u8bc1\u660e\u53ef\u77e5: \\[ \\mathbb{d} |\\pmb A| = \\operatorname{tr}(|\\pmb A|\\pmb A^{-1}\\mathbb{d}\\pmb A) \\] \u518d\u7531 \u4e8c.\u77e9\u9635\u6c42\u5bfc\u672c\u8d28---(24)\u5f0f \u53ef\u5f97\uff1a \\[ \\frac{\\partial |\\pmb A|}{\\partial \\pmb A^T}=|\\pmb A|\\pmb A^{-1} \\] \u56e0\u6b64, \\[ \\begin{aligned} \\nabla _{\\pmb A}|\\pmb A| & = (\\frac{\\partial |\\pmb A|}{\\partial \\pmb A^T})^T \\\\\\\\ &=(|\\pmb A|\\pmb A^{-1})^T\\\\\\\\ &=|\\pmb A|(\\pmb A^{-1})^T \\end{aligned} \\] \u8bc1\u6bd5\u3002 \u81f3\u6b64, \u5434\u6069\u8fbe\u8001\u5e08\u7701\u7565\u7684\u8bc1\u660e, \u5168\u90e8\u8bc1\u660e\u5b8c\u6bd5\u3002\u4e0b\u9762\u7ee7\u7eed\u8bb0\u7b14\u8bb0\u3002 To make our martix notation more concrete, let us now explain in detail the meaning of the first of these equations. Suppose we have some fixed matrix \\(B \\in \\mathbb{R}^{n\\times m}\\) . We can then define a function \\(f :\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}\\) according to \\(f(A)=AB\\) . Note that this definition makes sense, because if \\(A \\in \\mathbb{R}^{m\\times n}\\) , then \\(AB\\) is a square matrix, and we can apply the trace operator to it; thus, \\(f\\) does indeed map from \\(\\mathbb{R}^{m\\times n}\\) to \\(\\mathbb{R}\\) . We can then apply our definition of matrix derivatives to find \\(\\nabla _Af(A)\\) , which will itself by an m-by-n matrix. Equation (1) above states that the \\((i,j)\\) entry of this matrix will given by the \\((i,j)\\) -entry of \\(B^T\\) , or equivalently, by \\(B_{j,i}\\) . \u2002 The proofs of Equation (1-3) are reasonably simply, and are left as an exercise to the reader. Equation (4) can be derived using adjoint representation of the inverse of a martix.","title":"7.1 Matrix derivatives"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#72-least-squares-revisited","text":"Armed with the tools of matrix derivatives, let us now proceed to find in closed-form the value of \\(\\theta\\) that minimizes \\(J(\\theta)\\) . We begin by re-writing \\(J\\) in matrix-vectorial notation. \u2002Giving a training set, define the design matrix \\(X\\) to be the m-by-n matrix (actually m-by-n+1, if we include the intercept term) that contains the training examples' input values in its row: \\[ X = \\begin{bmatrix} --- & (x^{(1)})^T & --- \\\\\\\\ --- & (x^{(2)})^T & --- \\\\\\\\ & \\vdots & \\\\\\\\ --- & (x^{(m)})^T & --- \\end{bmatrix} \\] Also, let \\(\\vec{y}\\) be the m-dimensional vector containing all the target values from the training set: \\[ \\vec{y}=\\begin{bmatrix} y^{(1)}\\\\\\\\ y^{(2)}\\\\\\\\ \\vdots\\\\\\\\ y^{(m)} \\end{bmatrix} \\] Now, since \\(h_\\theta\\big(x^{(i)}\\big)=(x^{(i)})^T\\theta\\) , we can easily verify that \\[ \\begin{aligned} X\\theta-\\vec{y} &= \\begin{bmatrix} (x^{(1)})^T\\theta \\\\\\\\ \\vdots \\\\\\\\ (x^{(m)})^T\\theta \\end{bmatrix} - \\begin{bmatrix} y^{(1)}\\\\\\\\ \\vdots\\\\\\\\ y^{(m)} \\end{bmatrix}\\\\\\\\ & = \\begin{bmatrix} (x^{(1)})^T\\theta- y^{(1)} \\\\\\\\ \\vdots \\\\\\\\ (x^{(m)})^T\\theta- y^{(m)} \\end{bmatrix} \\end{aligned} \\] Thus, using the fact for a vector \\(z\\) , we have that \\(z^Tz=\\sum_{i}z_i^2\\) . \\[ \\begin{aligned} \\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y})&=\\frac{1}{2}\\sum_{i=1}^m\\Big(h_\\theta(x^{(i)})- y^{(i)} \\Big)^2\\\\\\\\ &=J(\\theta) \\end{aligned} \\] Finally , to minimize \\(J\\) , lets find its derivatives with respect to \\(\\theta\\) . Combining Equations (2) and (3) , we find that \\[ \\nabla _{A^T}\\operatorname{tr}(ABA^TC)=B^TA^TC^T+BA^TC \\\\\\\\ \\tag{5} \\] Hence, \\[ \\begin{aligned} \\nabla _\\theta J(\\theta) & = \\nabla _\\theta \\frac{1}{2}(X\\theta-\\vec{y})^T(X\\theta-\\vec{y}) \\\\\\\\ & = \\frac{1}{2} \\nabla _\\theta(\\theta^TX^TX\\theta-\\theta^TX^T \\vec{y}-\\vec{y}^TX \\theta+\\vec{y}^T \\vec{y})\\\\\\\\ & = \\frac{1}{2} \\nabla _\\theta \\operatorname{tr}(\\theta^TX^TX\\theta-\\theta^TX^T \\vec{y}-\\vec{y}^TX \\theta+\\vec{y}^T \\vec{y})\\\\\\\\ & = \\frac{1}{2} \\nabla _\\theta\\Big(\\operatorname{tr}(\\theta^TX^TX\\theta)-2\\operatorname{tr}(\\vec{y}^TX \\theta)\\Big)\\\\\\\\ & = \\frac{1}{2}(X^TX \\theta+X^TX \\theta-2X^T \\vec{y})\\\\\\\\ & = X^TX \\theta-X^T \\vec{y} \\end{aligned} \\] In the third step, we used the fact that the trace if a real number is just the real number; the fourth step used the fact that \\(\\operatorname{tr}(A)=\\operatorname{tr}(A^T)\\) , and the fifth step used Equation (5) with \\(A^T=\\theta\\) , \\(B=B^T=X^TX\\) , and \\(C=I\\) , and Equation (1). To minimize \\(J\\) , we set its derivatives to zero, and obtain the normal equations: \\[ X^TX \\theta=X^T \\vec{y} \\] Thus, the value of \\(\\theta\\) that minimize \\(J(\\theta)\\) is given in closed form by the equation \\[ \\theta = (X^TX)^{-1}X^T \\vec{y} \\]","title":"7.2 Least squares revisited"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#8","text":"\u53c2\u8003\u89c6\u9891: 4 - 7 - Normal Equation Noninvertibility (Optional) (6 min).mkv \u5728\u8fd9\u6bb5\u89c6\u9891\u4e2d\u8c08\u8c08\u6b63\u89c4\u65b9\u7a0b ( normal equation )\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u4e0d\u53ef\u9006\u6027\u3002 \u6211\u4eec\u8981\u8bb2\u7684\u95ee\u9898\u5982\u4e0b\uff1a \\(\\theta ={{\\left( {X^{T}}X \\right)}^{-1}}{X^{T}}y\\) \u5f53\u8ba1\u7b97 \\(\\theta\\) = inv(X'X ) X'y \uff0c\u90a3\u5bf9\u4e8e\u77e9\u9635 \\(X'X\\) \u7684\u7ed3\u679c\u662f\u4e0d\u53ef\u9006\u7684\u60c5\u51b5\u548b\u529e\u5462? \u6211\u4eec\u90fd\u77e5\u9053\uff0c\u6709\u4e9b\u77e9\u9635\u53ef\u9006( invertible )\uff0c\u800c\u6709\u4e9b\u77e9\u9635\u4e0d\u53ef\u9006( non-invertible )\u3002\u6211\u4eec\u79f0\u90a3\u4e9b\u4e0d\u53ef\u9006\u77e9\u9635\u4e3a\u5947\u5f02( singular )\u6216\u9000\u5316( dgenerate )\u77e9\u9635\u3002 \u9996\u5148, \u8bf4\u4e00\u4e0b \\(\\pmb X'\\pmb X\\) \u4e0d\u53ef\u9006\u7684\u539f\u56e0 \u3002 \u7279\u5f81\u503c\u7ebf\u6027\u76f8\u5173 \u4f8b\u5982\uff0c\u5728\u9884\u6d4b\u4f4f\u623f\u4ef7\u683c\u65f6\uff0c\u5982\u679c \\({x_{1}}\\) \u662f\u4ee5\u82f1\u5c3a\u4e3a\u5c3a\u5bf8\u89c4\u683c\u8ba1\u7b97\u7684\u623f\u5b50\uff0c \\({x_{2}}\\) \u662f\u4ee5\u5e73\u65b9\u7c73\u4e3a\u5c3a\u5bf8\u89c4\u683c\u8ba1\u7b97\u7684\u623f\u5b50\uff0c\u540c\u65f6\uff0c\u4f60\u4e5f\u77e5\u90531\u7c73\u7b49\u4e8e3.28\u82f1\u5c3a ( \u56db\u820d\u4e94\u5165\u5230\u4e24\u4f4d\u5c0f\u6570 )\uff0c\u8fd9\u6837\uff0c\u4f60\u7684\u8fd9\u4e24\u4e2a\u7279\u5f81\u503c\u5c06\u59cb\u7ec8\u6ee1\u8db3\u7ea6\u675f\uff1a \\({x_{1}}={x_{2}}*{{\\left( 3.28 \\right)}^{2}}\\) \u3002 \u5b9e\u9645\u4e0a\uff0c\u5982\u679c\u4f60\u7528\u8fd9\u6837\u7684\u4e00\u4e2a\u7ebf\u6027\u65b9\u7a0b\uff0c\u6765\u5c55\u793a\u90a3\u4e24\u4e2a\u76f8\u5173\u8054\u7684\u7279\u5f81\u503c\uff0c\u77e9\u9635 \\(X'X\\) \u5c06\u662f\u4e0d\u53ef\u9006\u7684\u3002 \u7279\u5f81\u503c\u7684\u6570\u91cf\u5c0f\u4e8e\u8bad\u7ec3\u96c6\u7684\u6570\u91cf \u5177\u4f53\u5730\u8bf4\uff0c\u5728 \\(m\\) \u5c0f\u4e8e\u6216\u7b49\u4e8en\u7684\u65f6\u5019\uff0c\u4f8b\u5982\uff0c\u6709 \\(m\\) \u7b49\u4e8e10\u4e2a\u7684\u8bad\u7ec3\u6837\u672c\u4e5f\u6709 \\(n\\) \u7b49\u4e8e100\u7684\u7279\u5f81\u6570\u91cf\u3002\u8981\u627e\u5230\u9002\u5408\u7684 \\((n +1)\\) \u7ef4\u53c2\u6570\u77e2\u91cf \\(\\theta\\) \uff0c\u8fd9\u5c06\u4f1a\u53d8\u6210\u4e00\u4e2a101\u7ef4\u7684\u77e2\u91cf\uff0c\u5c1d\u8bd5\u4ece10\u4e2a\u8bad\u7ec3\u6837\u672c\u4e2d\u627e\u5230\u6ee1\u8db3101\u4e2a\u53c2\u6570\u7684\u503c\uff0c\u8fd9\u5de5\u4f5c\u53ef\u80fd\u4f1a\u8ba9\u4f60\u82b1\u4e0a\u4e00\u9635\u5b50\u65f6\u95f4\uff0c\u4f46\u8fd9\u5e76\u4e0d\u603b\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\u3002\u56e0\u4e3a\uff0c\u6b63\u5982\u6211\u4eec\u6240\u770b\u5230\u4f60\u53ea\u670910\u4e2a\u6837\u672c\uff0c\u4ee5\u9002\u5e94\u8fd9100\u6216101\u4e2a\u53c2\u6570\uff0c\u6570\u636e\u8fd8\u662f\u6709\u4e9b\u5c11\u3002 \u8fd9\u76f8\u5f53\u4e8e\u8bf4\u662f\u591a\u5143\u65b9\u7a0b\u7ec4\u4e2d\u672a\u77e5\u6570\u7684\u4e2a\u6570\u8fdc\u5927\u4e8e\u65b9\u7a0b\u7684\u4e2a\u6570\u3002 \u7a0d\u540e\u6211\u4eec\u5c06\u770b\u5230\uff0c \u5982\u4f55\u4f7f\u7528\u5c0f\u6570\u636e\u6837\u672c\u4ee5\u5f97\u5230\u8fd9100\u6216101\u4e2a\u53c2\u6570 \uff0c \u901a\u5e38\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528 \u4e00\u79cd\u53eb\u505a \u6b63\u5219\u5316 \u7684\u7ebf\u6027\u4ee3\u6570\u65b9\u6cd5\uff0c \u901a\u8fc7\u5220\u9664\u67d0\u4e9b\u7279\u5f81\u6216\u8005\u662f\u4f7f\u7528\u67d0\u4e9b\u6280\u672f\uff0c\u6765\u89e3\u51b3\u5f53 \\(m\\) \u6bd4 \\(n\\) \u5c0f\u7684\u65f6\u5019\u7684\u95ee\u9898 \u3002\u5373\u4f7f\u4f60\u6709\u4e00\u4e2a\u76f8\u5bf9\u8f83\u5c0f\u7684\u8bad\u7ec3\u96c6\uff0c\u4e5f\u53ef\u4f7f\u7528\u5f88\u591a\u7684\u7279\u5f81\u6765\u627e\u5230\u5f88\u591a\u5408\u9002\u7684\u53c2\u6570\u3002 \u603b\u4e4b\u5f53\u4f60\u53d1\u73b0\u7684\u77e9\u9635 \\(X'X\\) \u7684\u7ed3\u679c\u662f\u5947\u5f02\u77e9\u9635\uff0c\u6216\u8005\u627e\u5230\u7684\u5176\u5b83\u77e9\u9635\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u6211\u4f1a\u5efa\u8bae\u4f60\u8fd9\u4e48\u505a\u3002 \u9996\u5148\uff0c \u770b\u7279\u5f81\u503c\u91cc\u662f\u5426\u6709\u4e00\u4e9b\u591a\u4f59\u7684\u7279\u5f81 \uff0c\u50cf\u8fd9\u4e9b \\({x_{1}}\\) \u548c \\({x_{2}}\\) \u662f \u7ebf\u6027\u76f8\u5173 \u7684\uff0c\u4e92\u4e3a\u7ebf\u6027\u51fd\u6570\u3002\u540c\u65f6\uff0c\u5f53\u6709\u4e00\u4e9b\u591a\u4f59\u7684\u7279\u5f81\u65f6\uff0c\u53ef\u4ee5 \u5220\u9664 \u8fd9\u4e24\u4e2a\u91cd\u590d\u7279\u5f81\u91cc\u7684 \u5176\u4e2d\u4e00\u4e2a \uff0c\u65e0\u987b\u4e24\u4e2a\u7279\u5f81\u540c\u65f6\u4fdd\u7559\uff0c\u5c06\u89e3\u51b3\u4e0d\u53ef\u9006\u6027\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9996\u5148\u5e94\u8be5\u901a\u8fc7\u89c2\u5bdf\u6240\u6709\u7279\u5f81\u68c0\u67e5\u662f\u5426\u6709\u591a\u4f59\u7684\u7279\u5f81\uff0c\u5982\u679c\u6709\u591a\u4f59\u7684\u5c31\u5220\u9664\u6389\uff0c\u76f4\u5230\u4ed6\u4eec\u4e0d\u518d\u662f\u591a\u4f59\u7684\u4e3a\u6b62\uff0c\u5982\u679c \u7279\u5f81\u6570\u91cf \u5b9e\u5728 \u592a\u591a \uff0c\u6211\u4f1a \u7528\u8f83\u5c11\u7684\u7279\u5f81 \u6765 \u53cd\u6620\u5c3d\u53ef\u80fd\u591a\u5185\u5bb9 \uff0c \u5426\u5219 \u6211\u4f1a\u8003\u8651 \u4f7f\u7528\u6b63\u89c4\u5316\u65b9\u6cd5 \u3002 \u5982\u679c\u77e9\u9635 \\(X'X\\) \u662f\u4e0d\u53ef\u9006\u7684\uff0c\uff08\u901a\u5e38\u6765\u8bf4\uff0c\u4e0d\u4f1a\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\uff09\uff0c\u5982\u679c\u5728 Octave \u91cc\uff0c\u53ef\u4ee5\u7528\u4f2a\u9006\u51fd\u6570 pinv() \u6765\u5b9e\u73b0\u3002\u8fd9\u79cd\u4f7f\u7528\u4e0d\u540c\u7684\u7ebf\u6027\u4ee3\u6570\u5e93\u7684\u65b9\u6cd5\u88ab\u79f0\u4e3a\u4f2a\u9006\u3002\u5373\u4f7f \\(X'X\\) \u7684\u7ed3\u679c\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u4f46\u7b97\u6cd5\u6267\u884c\u7684\u6d41\u7a0b\u662f\u6b63\u786e\u7684\u3002\u603b\u4e4b\uff0c\u51fa\u73b0\u4e0d\u53ef\u9006\u77e9\u9635\u7684\u60c5\u51b5\u6781\u5c11\u53d1\u751f\uff0c\u6240\u4ee5\u5728\u5927\u591a\u6570\u5b9e\u73b0\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u51fa\u73b0\u4e0d\u53ef\u9006\u7684\u95ee\u9898\u4e0d\u5e94\u8be5\u8fc7\u591a\u7684\u5173\u6ce8 \\({X^{T}}X\\) \u662f\u4e0d\u53ef\u9006\u7684\u3002","title":"8. \u6b63\u89c4\u65b9\u7a0b\u53ca\u4e0d\u53ef\u9006\u6027"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_1","text":"","title":"\u4e60\u9898 &amp;&amp; \u53c2\u8003\u7b54\u6848"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_2","text":"\u5047\u8bbem=4\u4e2a\u5b66\u751f\u4e0a\u4e86\u4e00\u8282\u8bfe, \u6709\u671f\u4e2d\u8003\u8bd5\u548c\u671f\u672b\u8003\u8bd5\u3002\u4f60\u5df2\u7ecf\u6536\u96c6\u4e86\u4ed6\u4eec\u5728\u4e24\u6b21\u8003\u8bd5\u4e2d\u7684\u5206\u6570\u6570\u636e\u96c6\uff0c\u5982\u4e0b\u6240\u793a\uff1a \u671f\u4e2d\u5f97\u5206 (\u671f\u4e2d\u5f97\u5206)^2 \u671f\u672b\u5f97\u5206 89 7921 96 72 5184 74 94 8836 87 69 4761 78 \u4f60\u60f3\u7528\u591a\u9879\u5f0f\u56de\u5f52\u6765\u9884\u6d4b\u4e00\u4e2a\u5b66\u751f\u7684\u671f\u4e2d\u8003\u8bd5\u6210\u7ee9\u3002\u5177\u4f53\u5730 \u8bf4, \u5047\u8bbe\u4f60\u60f3\u62df\u5408\u4e00\u4e2a \\(h_ \\theta (x) = \\theta _0+\\theta _1x_1++\\theta _2x_1\\) \u7684\u6a21\u578b, \u5176\u4e2dx1\u662f\u671f\u4e2d\u5f97\u5206, x2\u662f\uff08\u671f\u4e2d\u5f97\u5206\uff09^2\u3002\u6b64\u5916, \u4f60\u8ba1\u5212\u540c\u65f6\u4f7f\u7528\u7279\u5f81\u7f29\u653e\uff08\u9664\u4ee5\u7279\u5f81\u7684\u201c\u6700\u5927\u503c-\u6700\u5c0f\u503c\u201d\u6216\u8303\u56f4\uff09\u548c\u5747\u503c\u5f52\u4e00\u5316\u3002 \u90a3\u4e48\u6807\u51c6\u5316\u540e\u7684 \\(x_2^{(4)}\\) \u7279\u5f81\u503c\u662f\u591a\u5c11\uff1f\uff08\u63d0\u793a\uff1a\u671f\u4e2d=89\uff0c\u671f\u672b=96\u662f\u8bad\u7ec3\u793a\u4f8b1\uff09","title":"\u7b2c\u4e00\u9898"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_3","text":"\u7528 \\(\\alpha=0.3\\) \u8fdb\u884c15\u6b21\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3, \u6bcf\u6b21\u8fed\u4ee3 \\(j(\\theta)\\) \u540e\u8ba1\u7b97\u3002\u4f60\u4f1a\u53d1\u73b0 \\(j(\\theta)\\) \u7684\u503c\u4e0b\u964d\u7f13\u6162, \u5e76\u4e14\u572815\u6b21\u8fed\u4ee3\u540e\u4ecd\u5728\u4e0b\u964d\u3002\u57fa\u4e8e\u6b64, \u4ee5\u4e0b\u54ea\u4e2a\u7ed3\u8bba\u4f3c\u4e4e\u6700\u53ef\u4fe1\uff1f A. \\(\\alpha=0.3\\) \u662f\u5b66\u4e60\u7387\u7684\u6709\u6548\u9009\u62e9\u3002 B. \u4e0e\u5176\u4f7f\u7528 \\(\\alpha\\) \u5f53\u524d\u503c, \u4e0d\u5982\u5c1d\u8bd5\u66f4\u5c0f\u7684 \\(\\alpha\\) \u503c\uff08\u6bd4\u5982 \\(\\alpha=0.1\\) \uff09 C. \u4e0e\u5176\u4f7f\u7528 \\(\\alpha\\) \u5f53\u524d\u503c, \u4e0d\u5982\u5c1d\u8bd5\u66f4\u5927\u7684 \\(\\alpha\\) \u503c\uff08\u6bd4\u5982 \\(\\alpha=1.0\\) \uff09","title":"\u7b2c\u4e8c\u9898"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_4","text":"\u5047\u8bbe\u60a8\u6709m=14\u4e2a\u8bad\u7ec3\u793a\u4f8b, \u6709n=3\u4e2a\u7279\u6027\uff08\u4e0d\u5305\u62ec\u9700\u8981\u53e6\u5916\u6dfb\u52a0\u7684\u6052\u4e3a1\u7684\u622a\u8ddd\u9879\uff09, \u6b63\u89c4\u65b9\u7a0b\u662f \\(\\theta=(X^TX)^{-1}X^Ty\\) \u3002\u5bf9\u4e8e\u7ed9\u5b9am\u548cn\u7684\u503c, \u8fd9\u4e2a\u65b9\u7a0b\u4e2d \\(\\theta, X, y\\) \u7684\u7ef4\u6570\u5206\u522b\u662f\u591a\u5c11\uff1f A. \\(X \\ 14 \\times 3, y \\ 14\\times 1, \\ \\theta 3 \\times 3\\) B. \\(X \\ 14 \\times 4, y \\ 14\\times 1, \\ \\theta 4 \\times 1\\) C. \\(X \\ 14 \\times 3, y \\ 14\\times 1, \\ \\theta 3 \\times 1\\) D. \\(X \\ 14 \\times 4, y \\ 14\\times 4, \\ \\theta 4 \\times 4\\)","title":"\u7b2c\u4e09\u9898"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_5","text":"\u5047\u8bbe\u60a8\u6709\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u793a\u4f8b\u6709m=1000000\u4e2a\u793a\u4f8b\u548cn=200000\u4e2a\u7279\u6027\u3002\u4f60\u60f3\u7528\u591a\u5143\u7ebf\u6027\u56de\u5f52\u6765\u62df\u5408\u53c2\u6570 \\(\\theta\\) \u5230\u6211\u4eec\u7684\u6570\u636e\u3002\u4f60\u66f4\u5e94\u8be5\u7528\u68af\u5ea6\u4e0b\u964d\u8fd8\u662f\u6b63\u89c4\u65b9\u7a0b\uff1f A. \u68af\u5ea6\u4e0b\u964d\uff0c\u56e0\u4e3a\u6b63\u89c4\u65b9\u7a0b\u4e2d \\(\\theta=(X^TX)^{-1}\\) \u4e2d\u8ba1\u7b97\u975e\u5e38\u6162 B. \u6b63\u89c4\u65b9\u7a0b\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u6c42\u89e3\u7684\u6709\u6548\u65b9\u6cd5 C. \u68af\u5ea6\u4e0b\u964d\uff0c\u56e0\u4e3a\u5b83\u603b\u662f\u6536\u655b\u5230\u6700\u4f18 \\(\\theta\\) D. \u6b63\u89c4\u65b9\u7a0b\uff0c\u56e0\u4e3a\u68af\u5ea6\u4e0b\u964d\u53ef\u80fd\u65e0\u6cd5\u627e\u5230\u6700\u4f18 \\(\\theta\\)","title":"\u7b2c \u56db \u9898"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_6","text":"\u4ee5\u4e0b\u54ea\u4e9b\u662f\u4f7f\u7528\u7279\u5f81\u7f29\u653e\u7684\u539f\u56e0\uff1f A. \u5b83\u53ef\u4ee5\u9632\u6b62\u68af\u5ea6\u4e0b\u964d\u9677\u5165\u5c40\u90e8\u6700\u4f18 B. \u5b83\u901a\u8fc7\u964d\u4f4e\u68af\u5ea6\u4e0b\u964d\u7684\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u6210\u672c\u6765\u52a0\u901f\u68af\u5ea6\u4e0b\u964d C. \u5b83\u901a\u8fc7\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u6765\u83b7\u5f97\u4e00\u4e2a\u597d\u7684\u89e3\uff0c\u4ece\u800c\u52a0\u5feb\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u901f\u5ea6 D. \u5b83\u9632\u6b62\u77e9\u9635 \\(X^TX\\) \uff08\u7528\u4e8e\u6b63\u89c4\u65b9\u7a0b\uff09\u4e0d\u53ef\u9006\uff08\u5947\u5f02/\u9000\u5316\uff09","title":"\u7b2c \u4e94 \u9898"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_7","text":"\u7b2c\u4e00\u9898\uff1a-0.47 \u7b2c\u4e8c\u9898\uff1aC \u7b2c\u4e09\u9898\uff1aB \u7b2c\u56db\u9898\uff1aA \u7b2c\u4e94\u9898\uff1aC","title":"\u53c2\u8003\u7b54\u6848"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#_8","text":"In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices. \u2002 The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.","title":"\u4e0a\u673a\u7ec3\u4e60"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#1feature-normalization","text":"\u505a\u4e4b\u524d\u770b\u4e00\u4e0b\u6570\u636e\u7684\u5927\u6982\u6837\u5b50 1 2 3 4 5 6 7 import pandas as pd # path : \u6570\u636e\u5b58\u653e\u7684\u8def\u5f84 path = \"ex1data2.txt\" # names : \u6307\u5b9a\u5217\u540d; head=None : \u539f\u6570\u636e\u6ca1\u6709\u5217\u540d data = pd . read_csv ( path , header = None , names = [ \"Size\" , \"Bedrooms\" , \"Price\" ]) # \u5c55\u793a\u6570\u636e\u7684\u540e\u4e94\u884c data . tail () By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When feature differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. Substract the value of each feature from the dataset. After substracting the mean, additionally scale (divide) the feature values by their repective \"standar deviations\". \u2002 The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within \\(\\pm 2\\) standard deviation of the mean); this is an alternative to taking the range of values(max-min). \u2002 You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix X corresponds to one feature. \u5bf9\u6570\u636e\u6807\u51c6\u5316\u5e76\u63d2\u5165x0 1 2 3 4 5 6 7 8 9 # \u5bf9\u6570\u636e\u8fdb\u884c\u7279\u5f81-\u6807\u51c6\u5316 # \u6ce8\u610fpandas\u6309\u5217\u540d\u53d6\u5217\u662f\u4e24\u4e2a\u4e2d\u62ec\u53f7 data2 [[ \"Size\" , \"Bedrooms\" ]] = ( data2 [[ \"Size\" , \"Bedrooms\" ]] - data2 [[ \"Size\" , \"Bedrooms\" ]] . mean ()) / data2 [[ \"Size\" , \"Bedrooms\" ]] . std () # \u611f\u89c9\u628a\u623f\u4ef7\u7a0d\u5fae\u7f29\u653e\u4e00\u4e0b\uff0c\u4e0d\u7136\u5bf9\u68af\u5ea6\u5f71\u54cd\u592a\u5927 data2 [[ \"Price\" ]] = data2 [[ \"Price\" ]] / 100000 # \u63d2\u5165x0 data2 . insert ( 0 , 'Ones' , 1 ) data2 . tail ()","title":"1\u3001Feature Normalization"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#2gradient-descent","text":"Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged. Implement Note: In the multivariate case, the cost function can also be written in the following vectorized form: \\(J(\\theta)=\\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)\\) where \\(X=\\bigl( \\begin{bmatrix} --- & (x^{(1)})^T & --- \\\\\\\\ --- & (x^{(2)})^T & ---\\\\\\\\ & \\vdots & \\\\\\\\ --- & (x^{(m)})^T & --- \\end{bmatrix} \\bigr)\\) \\(y= \\begin{bmatrix} y^{(1)} \\\\\\\\ y^{(2)} \\\\\\\\ \\vdots \\\\\\\\ y^{(m)} \\end{bmatrix}\\) . The vectorized version is efficient when you are working with numerical cumputing tools like numpy. If you are an expert with matrix operations, you can prove to yourself that the two forms are equivalent. \u8fd0\u884c\u591a\u53d8\u91cf\u68af\u5ea6\u4e0b\u964d 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import numpy as np # \u83b7\u53d6\u6570\u636e\u7684\u603b\u5217\u6570 cols = data2 . shape [ 1 ] # \u5c06\u6570\u636e\u62c6\u5206\u5e76\u8f6c\u4e3andarry\u5206\u522b\u653e\u5230X,y\u4e2d X = np . array ( data2 . iloc [:, 0 : cols - 1 ] . values ) y = np . array ( data2 . iloc [:, cols - 1 ] . values ) # \u4e2d\u95f4\u770b\u4e00\u4e0bX\uff0cy\u4ee5\u53caX\uff0cy\u7684\u7ef4\u5ea6\u662f\u5426\u6b63\u786e # X.shape,y.shape # X, y # \u521d\u59cb\u5316theta, \u5b66\u4e60\u7387alpha\uff0c\u8fed\u4ee3\u6b21\u6570iters theta = np . zeros ([ 3 ,]) alpha = 0.01 iters = 1000 # X.shape,y.shape,theta.shape # \u5b9a\u4e49\u4ee3\u4ef7\u51fd\u6570\u5e76\u8ba1\u7b97 def computeCost ( X , y , theta ): m = X . shape [ 0 ] # print((np.dot(X,theta)-y).shape) inner = np . dot (( np . dot ( X , theta ) - y ) . T , ( np . dot ( X , theta ) - y )) return np . sum ( inner ) / ( 2 * m ) # \u5b9a\u4e49\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 def gradientDescent ( X , y , theta , alpha , iterations ): m = X . shape [ 0 ] # m: \u6837\u672c\u7684\u603b\u4e2a\u6570 n = len ( theta ) # n: theta\u7684\u603b\u4e2a\u6570 # \u7528\u4e00\u4e2a\u5411\u91cf\u6765\u8bb0\u5f55\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u6240\u6709\u7684cost\u503c costs = np . zeros ( iterations ) for i in range ( iterations ): costs [ i ] = computeCost ( X , y , theta ) # theta\u662f\u51e0\u4e2a\u5c31\u8981\u66f4\u65b0\u51e0\u4e2a for j in range ( n ): theta [ j ] = theta [ j ] - alpha * \\ ( 1 / m ) * np . sum (( np . dot ( X , theta ) - y ) * X [:, j ]) return theta , costs # \u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u5206\u522b\u63a5\u6536\u66f4\u65b0\u540e\u7684theta\u503c\u548c\u6bcf\u4e00\u6b65\u8fed\u4ee3\u7684cost\u503c theta_hat , costs = gradientDescent ( X , y , theta , alpha , iters ) # get the cost (error) of the model costs [ iters - 1 ], theta_hat \u8f93\u51fa ( 0.20435384903675252 , array ([ 3.40397964 , 1.09859063 , - 0.05879178 ]))","title":"2\u3001Gradient Descent"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#21-selecting-learning-rates","text":"In this part of the exercise, you will get to try out different learning rates for the dataset and find a learning rate that converages quickly. \u2002 We recommend trying values of the learning rate \\(\\alpha\\) on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on). You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve. \u2002 Notice the changes in the convergence curves as the learning rate changes. With a small learning rate, you should find that gradient descent takes a very long time to converge to the optimal value. Conversely, with a large learning rate, gradient descent might not converge or might even diverge!","title":"2.1 Selecting learning rates"},{"location":"machine%20learning/2.%20linear%20regression-m%20v/#3noramal-equation","text":"In the lecture videos, you learned that the closed-form solution to linear regression is \\[ \\theta = (X^TX)^{-1}X^Ty \\] \u2002 Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \u201cloop until convergence\u201d like in gradient descent. \u518d\u6b21\u63d0\u9192\uff1a \u68af\u5ea6\u4e0b\u964d \uff1a\u9700\u8981\u9009\u62e9\u5b66\u4e60\u7387 \\(\\alpha\\) \uff0c\u9700\u8981\u591a\u6b21\u8fed\u4ee3\uff0c\u5f53\u7279\u5f81\u6570\u91cfn\u5927\u65f6\u4e5f\u80fd\u8f83\u597d\u9002\u7528\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u6a21\u578b \u6b63\u89c4\u65b9\u7a0b \uff1a\u4e0d\u9700\u8981\u9009\u62e9\u5b66\u4e60\u7387 \\(\\alpha\\) \uff0c\u4e00\u6b21\u8ba1\u7b97\u5f97\u51fa\uff0c\u9700\u8981\u8ba1\u7b97 \\(X^TX\\) \uff0c\u5982\u679c\u7279\u5f81\u6570\u91cfn\u8f83\u5927\u5219\u8fd0\u7b97\u4ee3\u4ef7\u5927\uff0c\u56e0\u4e3a\u77e9\u9635\u9006\u7684\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a\ud835\udc42(\ud835\udc5b3)\uff0c \u901a\u5e38\u6765\u8bf4\u5f53\ud835\udc5b\u5c0f\u4e8e10000 \u65f6 \u8fd8\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\uff0c\u53ea\u9002\u7528\u4e8e \u7ebf\u6027\u6a21\u578b \uff0c\u4e0d\u9002\u5408\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7b49\u5176\u4ed6\u6a21\u578b\u3002 \u6b63\u89c4\u65b9\u7a0b\u8ba1\u7b97theta 1 2 3 # \u5229\u7528\u516c\u5f0f\u4e00\u6b65\u8ba1\u7b97\u51fa\u53c2\u6570theta theta_ne = np . linalg . inv ( X . T @X ) @X . T @y #X.T@X\u7b49\u4ef7\u4e8eX.T.dot(X) theta_ne \u8f93\u51fa array ([ 3.4041266 , 1.1063105 , - 0.06649474 ]) \u6211\u4eec\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8ba1\u7b97\u7684theta\u4e3a\uff1a [ 3.40397964, 1.09859063, -0.05879178] \u4e24\u8005\u76f8\u5dee\u4e0d\u662f\u7279\u522b\u5927\uff0c\u8bf4\u660e\u6211\u4eec\u68af\u5ea6\u4e0b\u964d\u6a21\u578b\u8fd8\u662f\u6bd4\u8f83\u51c6\u786e\u7684\uff0c\u54c8\u54c8\u54c8\u3002\u81f3\u6b64\uff0c\u6211\u5df2\u7ecf\u638c\u63e1\u4e86\u4e24\u4e2a\u7b97\u6cd5\uff0c\u68af\u5ea6\u4e0b\u964d\u6a21\u578b\u548c\u6b63\u89c4\u65b9\u7a0b\u6c42\u89e3\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002\u6211\u771f\u68d2\uff0c\u660e\u5929\u5f00\u59cb\u5b66\u4e60\u65b0\u7684\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u52a0\u6cb9\uff01\uff01\uff01","title":"3\u3001Noramal Equation"},{"location":"machine%20learning/3.%20logistic%20regression/","text":"\u53c2\u8003\u94fe\u63a5: https://scruel.gitee.io/ml-andrewng-notes/week3.html 1\u3001\u5206\u7c7b\u95ee\u9898 \u53c2\u8003\u89c6\u9891: 6 - 1 - Classification (8 min).mkv p { text-align: justify; /*\u6587\u672c\u4e24\u7aef\u5bf9\u9f50*/ } center img{ border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08); } center div{ color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px; } \u4e0b\u9762\u6211\u4eec\u5f00\u59cb\u8ba8\u8bba\u5206\u7c7b\u95ee\u9898\u3002\u5728\u5206\u7c7b\u95ee\u9898\u4e2d, \u4f60\u8981\u9884\u6d4b\u7684\u53d8\u91cf \\(y\\) \u662f\u79bb\u6563\u7684\u503c, \u6211\u4eec\u5c06\u5b66\u4e60\u4e00\u79cd\u53eb\u505a\u903b\u8f91\u56de\u5f52 (Logistic Regression) \u7684\u7b97\u6cd5, \u8fd9\u662f\u76ee\u524d\u6700\u6d41\u884c\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u4e00\u79cd\u5b66\u4e60\u7b97\u6cd5\u3002\u6240\u4ee5\u8bf4 \u903b\u8f91\u56de\u5f52\u662f\u89e3\u51b3\u5206\u7c7b\u95ee\u9898 \u3002 \u5728\u5206\u7c7b\u95ee\u9898\u4e2d, \u6211\u4eec\u5c1d\u8bd5\u9884\u6d4b\u7684\u662f\u7ed3\u679c\u662f\u5426\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u7c7b\uff08\u4f8b\u5982\u6b63\u786e\u6216\u9519\u8bef\uff09\u3002\u5206\u7c7b\u95ee\u9898\u7684\u4f8b\u5b50\u6709: \u5224\u65ad\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\u662f\u5426\u662f\u5783\u573e\u90ae\u4ef6; \u5224\u65ad\u4e00\u6b21\u91d1\u878d\u4ea4\u6613\u662f\u5426\u662f\u6b3a\u8bc8; \u4e4b\u524d\u6211\u4eec\u4e5f\u8c08\u5230\u4e86\u80bf\u7624\u5206\u7c7b\u95ee\u9898\u7684\u4f8b\u5b50, \u533a\u522b\u4e00\u4e2a\u80bf\u7624\u662f\u6076\u6027\u7684\u8fd8\u662f\u826f\u6027\u7684\u3002 \u6211\u4eec\u73b0\u5728\u8981\u4ece\u53ea\u5305\u542b\u4e24\u7c7b0\u548c1\u7684\u4e8c\u5206\u7c7b\u5f00\u59cb, \u540e\u9762\u6211\u4eec\u5c06\u8ba8\u8bba\u591a\u5206\u7c7b\u95ee\u9898, \u4f8b\u5982\u53d8\u91cfy\u53ef\u4ee5\u53d6 0, 1, 2, 3 \u8fd9\u51e0\u4e2a\u503c\u3002\u6211\u4eec\u5c06\u56e0\u53d8\u91cf (dependent variable) \u53ef\u80fd\u5c5e\u4e8e\u7684\u4e24\u4e2a\u7c7b\u5206\u522b\u79f0\u4e3a\u8d1f\u5411\u7c7b (negative class) \u548c\u6b63\u5411\u7c7b (positive class) ,\u5219\u56e0\u53d8\u91cf, \u5176\u4e2d 0 \u8868\u793a\u8d1f\u5411\u7c7b, 1 \u8868\u793a\u6b63\u5411\u7c7b\u3002 \u6211\u4eec\u5982\u4f55\u5f00\u53d1\u4e00\u4e2a\u5206\u7c7b\u7b97\u6cd5? \u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u7684\u8bad\u7ec3\u96c6\u662f\u5bf9\u80bf\u7624\u8fdb\u884c\u6076\u6027\u548c\u826f\u6027\u5206\u7c7b\u5f97\u5230\u7684\u6570\u636e, \u6ce8\u610f\u5230\u6076\u6027\u4e0e\u5426\u53ea\u6709\u4e24\u4e2a\u503c, 0 (No) \u4ee5\u53ca 1 (Yes)\u3002\u6240\u4ee5\u5bf9\u4e8e\u8fd9\u4e2a\u8bad\u7ec3\u96c6, \u6211\u4eec\u53ef\u4ee5\u505a\u7684\u662f\u628a\u6211\u4eec\u5df2\u7ecf\u5b66\u4f1a\u7684\u7ebf\u6027\u56de\u5f52\u7b97\u6cd5\u5e94\u7528\u5230\u8fd9\u4e2a\u6570\u636e\u96c6: \u7528\u76f4\u7ebf\u5bf9\u6570\u636e\u8fdb\u884c\u62df\u5408\u3002 \u5982\u679c\u6211\u4eec\u60f3\u505a\u51fa\u9884\u6d4b\uff0c\u6211\u4eec\u53ef\u4ee5\u8bbe\u7f6e\u5206\u7c7b\u5668\u7684\u9608\u503c\u4e3a0.5, \u5373\u7eb5\u5750\u6807\u4e3a0.5\u3002 \u5982\u679c\u5047\u8bbe\u51fd\u6570\u8f93\u51fa\u4e00\u4e2a\u503c >= 0.5, \u5219\u53ef\u4ee5\u9884\u6d4b y=1 \u5982\u679c\u5047\u8bbe\u51fd\u6570\u8f93\u51fa\u4e00\u4e2a\u503c < 0.5, \u5219\u53ef\u4ee5\u9884\u6d4b y=0 \u6211\u4eec\u89c2\u5bdf\u7ed3\u679c\u53d1\u73b0, \u7eb5\u5750\u68070.5\u5bf9\u5e94\u7684\u6a2a\u5750\u6807\u53f3\u8fb9\u6211\u4eec\u5c06\u9884\u6d4b\u4e3a\u6b63, \u5de6\u8fb9\u6211\u4eec\u5c06\u9884\u6d4b\u4e3a\u8d1f\u3002 \u5728\u8fd9\u4e2a\u7279\u5b9a\u7684\u4f8b\u5b50\u4e2d, \u7ebf\u6027\u56de\u5f52\u505a\u5f97\u5f88\u597d\u4e5f\u5f88\u6b63\u786e\u3002\u6211\u4eec\u5c1d\u8bd5\u6539\u53d8\u4e00\u4e0b\u8fd9\u4e2a\u95ee\u9898, \u6211\u4eec\u5c06\u6a2a\u8f74\u5ef6\u957f\u4e00\u70b9\u3002\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4f4d\u4e8e\u53f3\u8fb9\u8fdc\u5904\u3002 \u5982\u679c\u6211\u4eec\u4f7f\u7528\u539f\u6765\u7eff\u8272\u7684\u7ebf, \u4ecd\u7136\u80fd\u505a\u5b8c\u6210\u5f88\u597d\u7684\u5206\u7c7b(\u628a\u53f3\u4fa7\u6700\u8fdc\u7684\u70b9\u7b97\u8fdb\u53bb)\u3002\u4f46\u662f\u968f\u7740\u53f3\u4fa7\u6700\u8fdc\u5904\u7684\u70b9\u52a0\u5165, \u6211\u4eec\u8fd0\u884c\u6211\u4eec\u7684\u7ebf\u6027\u56de\u5f52\u7b97\u6cd5\u6211\u4eec\u4f1a\u5f97\u5230\u53e6\u4e00\u6761\u6570\u636e\u7684\u62df\u5408\u76f4\u7ebf(\u5047\u8bbe\u53d8\u4e3a\u56fe\u4e2d\u84dd\u8272\u7684\u7ebf)\u3002\u6211\u4eec\u7ee7\u7eed\u5c06\u9608\u503c\u8bbe\u4e3a0.5, \u6211\u4eec\u4f1a\u53d1\u73b0\u5de6\u53f3\u4e24\u8fb9\u7684\u9884\u6d4b\u7ed3\u679c\u6709\u95ee\u9898\u4e86\u3002 \u603b\u7ed3\u4e00\u4e0b\u5c31\u662f, \u52a0\u4e86\u6700\u8fdc\u5904\u7684\u8bad\u7ec3\u96c6\u540e, \u4f7f\u5f97\u7ebf\u6027\u56de\u5f52\u5bf9\u6570\u636e\u7684\u62df\u5408\u76f4\u7ebf\u4ece\u7eff\u8272\u7684\u7ebf\u53d8\u5230\u4e86\u84dd\u8272\u7684\u7ebf, \u4ece\u800c\u751f\u6210\u4e86\u4e00\u4e2a\u66f4\u574f\u7684\u5047\u8bbe\u3002\u6240\u4ee5\u628a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5e94\u7528\u5230\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u4e0d\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\u3002 \u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898, \\(y\\) \u53d6\u503c\u4e3a 0 \u6216\u80051\uff0c\u4f46\u5982\u679c\u4f60\u4f7f\u7528\u7684\u662f\u7ebf\u6027\u56de\u5f52\uff0c\u90a3\u4e48\u5047\u8bbe\u51fd\u6570\u7684\u8f93\u51fa\u503c\u53ef\u80fd\u8fdc\u5927\u4e8e 1\uff0c\u6216\u8005\u8fdc\u5c0f\u4e8e0\uff0c\u5373\u4f7f\u6240\u6709\u8bad\u7ec3\u6837\u672c\u7684\u6807\u7b7e \\(y\\) \u90fd\u7b49\u4e8e 0 \u6216 1\u3002\u5c3d\u7ba1\u6211\u4eec\u77e5\u9053\u6807\u7b7e\u5e94\u8be5\u53d6\u503c0 \u6216\u80051\uff0c\u4f46\u662f\u5982\u679c\u7b97\u6cd5\u5f97\u5230\u7684\u503c\u8fdc\u5927\u4e8e1\u6216\u8005\u8fdc\u5c0f\u4e8e0\u7684\u8bdd\uff0c\u5c31\u4f1a\u611f\u89c9\u5f88\u5947\u602a\u3002\u6240\u4ee5\u6211\u4eec\u5728\u63a5\u4e0b\u6765\u7684\u8981\u7814\u7a76\u7684\u7b97\u6cd5\u5c31\u53eb\u505a\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\uff0c\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6027\u8d28\u662f\uff1a\u5b83\u7684\u8f93\u51fa\u503c\u6c38\u8fdc\u57280\u5230 1 \u4e4b\u95f4\u3002 \u987a\u4fbf\u8bf4\u4e00\u4e0b\uff0c\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u662f\u5206\u7c7b\u7b97\u6cd5\uff0c\u6211\u4eec\u5c06\u5b83\u4f5c\u4e3a\u5206\u7c7b\u7b97\u6cd5\u4f7f\u7528\u3002\u6709\u65f6\u5019\u53ef\u80fd\u56e0\u4e3a\u8fd9\u4e2a\u7b97\u6cd5\u7684\u540d\u5b57\u4e2d\u51fa\u73b0\u4e86\u201c\u56de\u5f52\u201d\u4f7f\u4f60\u611f\u5230\u56f0\u60d1\uff0c\u4f46 \u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u5b9e\u9645\u4e0a\u662f \u4e00\u79cd \u5206\u7c7b\u7b97\u6cd5 \uff0c\u5b83\u9002\u7528\u4e8e\u6807\u7b7e\u53d6\u503c\u79bb\u6563\u7684\u60c5\u51b5\uff0c\u5982\uff1a0, 1\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u5f00\u59cb\u5b66\u4e60\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u7684\u7ec6\u8282\u3002 2\u3001\u5047\u8bbe\u51fd\u6570\u63cf\u8ff0 \u53c2\u8003\u89c6\u9891: 6 - 2 - Hypothesis Representation (7 min).mkv \u4e3a\u4e86\u4f7f \\(h_{\\theta}(x) \\in (0,1)\\) , \u5f15\u5165\u903b\u8f91\u56de\u5f52\u6a21\u578b, \u5b9a\u4e49\u5047\u8bbe\u51fd\u6570 \\[ h_{\\theta}(x) = g(z)=g(\\theta^Tx) \\] \u5bf9\u6bd4\u7ebf\u6027\u56de\u5f52\u51fd\u6570 \\(h_\\theta(x)=\\theta^Tx\\) , \\(g\\) \u8868\u793a\u903b\u8f91\u51fd\u6570 (logistic function), \u590d\u5408\u8d77\u6765, \u5219\u6210\u4e3a\u903b\u8f91\u56de\u5f52\u51fd\u6570\u3002 \u903b\u8f91\u51fd\u6570\u662f S \u578b\u51fd\u6570, \u4f1a\u5c06\u6240\u6709\u5b9e\u6570\u6620\u5c04\u5230(0, 1)\u8303\u56f4\u3002 sigmoid \u51fd\u6570 (\u5982\u4e0b\u56fe) \u662f\u903b\u8f91\u51fd\u6570\u7684\u7279\u6b8a\u60c5\u51b5, \u5176\u516c\u5f0f\u4e3a \\(g(z)=\\frac{1}{1+e^{-z}}\\) \u5e94\u7528sigmoid \u51fd\u6570, \u5219\u903b\u8f91\u56de\u5f52\u6a21\u578b: \\(h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}\\) \u903b\u8f91\u56de\u5f52\u6a21\u578b\u4e2d, \\(h_\\theta(x)\\) \u7684\u4f5c\u7528\u662f, \u6839\u636e\u8f93\u5165 \\(x\\) \u4ee5\u53ca\u53c2\u6570 \\(\\theta\\) , \u8ba1\u7b97\u5f97\u51fa \"\u8f93\u51fa \\(y=1\\) \" \u7684\u53ef\u80fd\u6027(estimated probability), \u6982\u7387\u5b66\u4e2d\u8868\u793a\u4e3a: \\[ h_\\theta(x) = P(y=1|x;\\theta) = 1-P(y=0|x;\\theta) \\\\\\\\ P(y=1|x;\\theta)+ P(y=0|x;\\theta) = 1 \\] \u6ce8\u610f \u4e0a\u9762\u7b2c\u4e8c\u4e2a\u5f0f\u5b50\u662f\u9488\u5bf9\u6211\u4eec\u73b0\u5728\u7814\u7a76\u7684\u4e8c\u5206\u7c7b\u95ee\u9898\u800c\u8bf4\u7684, y \u7684\u53d6\u503c\u53ea\u80fd\u662f0 \u6216 1\u3002\u6240\u4ee5\u4e24\u8005\u76f8\u52a0\u7684\u6982\u7387\u4e3a 100% \u3002 \\(P(y=1|x;\\theta)\\) \u7684\u542b\u4e49: probability that y=1, given x, parameterized by \\(\\theta\\) \u3002 \u4ee5\u80bf\u7624\u8bca\u65ad\u4e3a\u4f8b, \\(h_\\theta(x)=0.7\\) \u8868\u793a\u75c5\u4eba\u6709 \\(70%\\) \u7684\u6982\u7387\u5f97\u4e86\u6076\u6027\u80bf\u7624\u3002 3\u3001\u51b3\u7b56\u8fb9\u754c \u53c2\u8003\u89c6\u9891: 6 - 3 - Decision Boundary (15 min).mkv \u51b3\u7b56\u8fb9\u754c\u7684\u6982\u5ff5, \u53ef\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7684\u62df\u5408\u539f\u7406\u3002 \u5728\u903b\u8f91\u56de\u5f52\u4e2d, \u6709\u5047\u8bbe\u51fd\u6570 \\(h_{\\theta}(x) = g(z)=g(\\theta^Tx)\\) \u3002 \u4e3a\u4e86\u5f97\u51fa\u5206\u7c7b\u7684\u7ed3\u679c, \u8fd9\u91cc\u548c\u524d\u9762\u4e00\u6837, \u89c4\u5b9a\u4ee5 \\(0.5\\) \u4e3a\u9608\u503c: \\[ h_\\theta(x) \\geq 0.5 \\rightarrow y=1 \\\\\\\\ h_\\theta(x) < 0.5 \\rightarrow y=0 \\\\\\\\ \\] \u56de\u5fc6\u4e00\u4e0bsigmoid\u51f8\u51fd\u6570\u7684\u56fe\u50cf : \u89c2\u5bdf\u53ef\u5f97\u5f53 \\(g(z) \\geq 0.5\\) \u65f6, \u6709 \\(z \\geq 0\\) , \u5373 \\(\\theta^T x \\geq 0\\) \u3002 sigmoid \u51fd\u6570\u7684\u516c\u5f0f\u4e3a \\(g(z)=\\frac{1}{1+e^{-z}}\\) \u540c\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u540c\u5728\u4e8e\uff1a \\[ z \\rightarrow +\\infty, e^{-\\infty} \\rightarrow 0 \\Rightarrow g(z)=1 \\\\\\\\ z \\rightarrow -\\infty, e^{+\\infty} \\rightarrow +\\infty \\Rightarrow g(z)=0 \\\\\\\\ \\] \u76f4\u89c2\u4e00\u70b9 \u4e3e\u4e2a\u6817\u5b50, \\(h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)\\) \u662f\u4e0b\u56fe\u6a21\u578b\u7684\u5047\u8bbe\u51fd\u6570: \u56fe 3.3.1 \u51b3\u7b56\u8fb9\u754c1 \u6839\u636e\u4e0a\u9762\u7684\u8ba8\u8bba, \u8981\u8fdb\u884c\u5206\u7c7b, \u90a3\u4e48\u53ea\u8981 \\(\\theta_0+\\theta_1x+\\theta_2x \\geq 0\\) \u65f6, \u5c31\u9884\u6d4b \\(y=1\\) , \u5373\u9884\u6d4b\u4e3a\u6b63\u5411\u7c7b\u3002 \u5982\u679c\u53d6 \\(\\theta = [-3 \\ \\ 1\\ \\ 1]^T\\) , \u5219\u6709 \\(z=-3+x_1+x_2\\) , \u5f53 \\(z \\geq 0\\) , \u5373 \\(x_1+x_2 \\geq 3\\) \u65f6, \u6613\u7ed8\u5236\u51fa\u56fe\u4e2d\u7684\u54c1\u7ea2\u8272\u76f4\u7ebf, \u5373 \u51b3\u7b56\u8fb9\u754c , \u4e3a\u6b63\u5411\u7c7b (\u4ee5\u7ea2\u53c9\u6807\u6ce8\u7684\u6570\u636e) \u7ed9\u51fa \\(y=1\\) \u7684\u5206\u7c7b\u9884\u6d4b\u7ed3\u679c\u3002 \u4e0a\u9762\u8ba8\u8bba\u4e86\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4e2d\u7ebf\u6027\u62df\u5408\u7684\u4f8b\u5b50, \u4e0b\u9762\u5219\u662f\u4e00\u4e2a\u591a\u9879\u5f0f\u62df\u5408\u7684\u4f8b\u5b50, \u548c\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u60c5\u51b5\u4e5f\u662f\u7c7b\u4f3c\u7684\u3002 \u4e3a\u4e86\u62df\u5408\u4e0b\u56fe\u6570\u636e, \u5efa\u6a21\u591a\u9879\u5f0f\u5047\u8bbe\u51fd\u6570: \\[ h_\\theta(x) = g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2) \\] \u8fd9\u91cc\u53d6 \\(\\theta = [-1\\ \\ 0\\ \\ 0\\ \\ 1\\ \\ 1]^T\\) , \u51b3\u7b56\u8fb9\u754c\u5bf9\u5e94\u4e86\u4e00\u4e2a\u5728\u539f\u70b9\u5904\u7684\u5355\u4f4d\u5143 ( \\(x_1^2+x_2^2=1\\) ), \u5982\u6b64\u4fbf\u53ef\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c, \u5982\u56fe\u4e2d\u54c1\u7ea2\u8272\u66f2\u7ebf: \u56fe 3.3.2 \u51b3\u7b56\u8fb9\u754c2 \u5f53\u7136, \u901a\u8fc7\u4e00\u4e9b\u66f4\u4e3a\u590d\u6742\u7684\u591a\u9879\u5f0f, \u8fd8\u80fd\u62df\u5408\u90a3\u4e9b\u56fe\u50cf\u663e\u5f97\u975e\u5e38\u602a\u5f02\u7684\u6570\u636e, \u4f7f\u5f97\u51b3\u7b56\u8fb9\u754c\u5f62\u72b6\u4f3c\u7897\u88c5\u3001\u7231\u5fc3\u72b6\u7b49\u7b49\u3002 \u7b80\u5355\u6765\u8bf4, \u51b3\u7b56\u7684\u8fb9\u754c\u5c31\u662f \u5206\u7c7b\u7684\u5206\u754c\u7ebf , \u5206\u7c7b\u73b0\u5728\u5b9e\u9645\u5c31\u7531 \\(z\\) (\u4e2d\u7684 \\(\\theta\\) ) \u51b3\u5b9a\u5566\u3002 4\u3001\u4ee3\u4ef7\u51fd\u6570 \u53c2\u8003\u89c6\u9891: 6 - 4 - Cost Function (11 min).mkv \u90a3\u6211\u4eec\u600e\u4e48\u77e5\u9053\u51b3\u7b56\u8fb9\u754c\u65f6\u5565\u6837? \\(\\theta\\) \u591a\u5c11\u65f6\u80fd\u5f88\u597d\u7684\u62df\u5408\u6570\u636e? \u5f53\u7136, \u89c1\u62db\u62c6\u62db, \u603b\u8981\u6765\u4e2a \\(J(\\theta)\\) \u3002 \u5982\u679c\u76f4\u63a5\u5957\u7528\u7ebf\u6027\u56de\u5f52\u7684\u4ee3\u4ef7\u51fd\u6570: \\[ J(\\theta)=\\frac{1}{2m} \\sum_{i=1}^m{\\big(h_\\theta(x^{(i)})-y^{(i)}\\big)^2} \\] \u5176\u4e2d, \\(h_\\theta(x)=g(\\theta^Tx)\\) , \u53ef\u7ed8\u5236\u5173\u4e8e \\(J(\\theta)\\) \u7684\u56fe\u50cf, \u5982\u4e0b\u56fe \u56fe 3.4.1 \u903b\u8f91\u56de\u5f52\u4ee3\u4ef7\u51fd\u6570\u76f4\u63a5\u7528\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u4ee3\u4ef7\u51fd\u6570 \u56de\u5fc6\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u5e73\u65b9\u635f\u5931\u51fd\u6570, \u5176\u5b9e\u662f\u4e00\u4e2a\u4e8c\u6b21\u51f8\u51fd\u6570 (\u7897\u72b6) , \u4e8c\u6b21\u51f8\u51fd\u6570\u7684\u91cd\u8981\u6027\u8d28\u662f\u53ea\u6709\u4e00\u4e2a\u5c40\u90e8\u6700\u5c0f\u70b9\u5373\u5168\u5c40\u6700\u5c0f\u70b9\u3002\u4e0a\u56fe\u4e2d\u6709\u8bb8\u591a\u5c40\u90e8\u6700\u5c0f\u70b9, \u8fd9\u6837\u4f7f\u5f97\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u65e0\u6cd5\u786e\u5b9a\u54ea\u4e2a\u6536\u655b\u70b9\u662f\u5168\u5c40\u6700\u4f18\u3002 \u56fe 3.4.2 \u903b\u8f91\u56de\u5f52\u4e2d\u7406\u60f3\u7684\u4ee3\u4ef7\u51fd\u6570 \u5982\u679c\u6b64\u5904\u7684\u4ee3\u4ef7\u51fd\u6570\u4e5f\u662f\u4e00\u4e2a\u51f8\u51fd\u6570, \u662f\u5426\u4e5f\u6709\u540c\u6837\u7684\u6027\u8d28, \u4ece\u800c\u6700\u4f18\u5316? \u8fd9\u7c7b\u8ba8\u8bba\u51f8\u51fd\u6570\u6700\u4f18\u503c\u5f97\u95ee\u9898, \u88ab\u79f0\u4e3a \u51f8\u4f18\u5316\u95ee\u9898 (Convex optimization) \u3002 \u5f53\u7136, \u635f\u5931\u51fd\u6570\u4e0d\u6b62\u5e73\u65b9\u635f\u5931\u51fd\u6570\u4e00\u79cd\u3002 \u5bf9\u4e8e\u903b\u8f91\u56de\u5f52, \u66f4\u6362\u5e73\u65b9\u635f\u5931\u51fd\u6570\u4e3a\u5bf9\u6570\u635f\u5931\u51fd\u6570, \u53ef\u7531\u7edf\u8ba1\u5b66\u4e2d\u7684 \u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\u63a8\u5bfc\u51fa\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta)\\) (\u4e0b\u4e00\u8282\u82f1\u6587\u7248\u4e2d\u7ed9\u51fa\u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b) : \\[ J(\\theta)=\\frac{1}{m} \\sum_{i=1}^m{Cost\\big( h_\\theta(x^{(i)}, y^{(i)}\\big)}\\\\\\\\ Cost\\big( h_\\theta(x^{(i)}, y^{(i)}\\big)=-\\log \\big(h_\\theta(x)\\big) \\ \\ \\ \\ \\ if\\ y=1 \\\\\\\\ Cost\\big( h_\\theta(x^{(i)}, y^{(i)}\\big)=-\\log \\big(1-h_\\theta(x)\\big) \\ \\ \\ \\ if\\ y=0 \\] \u5219\u5173\u4e8e \\(J(\\theta)\\) \u7684\u56fe\u50cf\u5982\u4e0b: \u56fe 3.4.3 \u5bf9\u6570\u4ee3\u4ef7\u51fd\u6570y=1&y=0\u4e24\u79cd\u60c5\u51b5 \u5982\u5de6\u4e0a\u56fe, \u5f53\u8bad\u7ec3\u96c6\u7684\u7ed3\u679c\u4e3a \\(y=1\\) (\u6b63\u6837\u672c) \u65f6, \u968f\u7740\u5047\u8bbe\u51fd\u6570\u8d8b\u5411\u4e8e1, \u4ee3\u4ef7\u51fd\u6570\u7684\u503c\u4f1a\u8d8b\u5411\u4e8e0, \u5373\u610f\u5473\u7740\u62df\u5408\u7a0b\u5ea6\u5f88\u597d\u3002\u5982\u679c\u5047\u8bbe\u51fd\u6570\u6b64\u65f6\u8d8b\u4e8e0, \u5219\u4f1a\u7ed9\u51fa\u4e00\u4e2a \u5f88\u9ad8\u7684\u4ee3\u4ef7 , \u62df\u5408\u7a0b\u5ea6 \u5dee , \u7b97\u6cd5\u4f1a\u6839\u636e\u5176\u8fc5\u901f\u7ea0\u6b63 \\(\\theta\\) \u503c, \u53f3\u56fe \\(y=0\\) \u540c\u7406\u3002 \u533a\u522b\u4e8e\u5e73\u65b9\u635f\u5931\u51fd\u6570, \u5bf9\u6570\u635f\u5931\u51fd\u6570\u4e5f\u662f\u4e00\u4e2a\u51f8\u51fd\u6570, \u4ed6\u6ca1\u6709\u5c40\u90e8\u6700\u4f18\u503c, \u53ea\u6709\u5168\u5c40\u6700\u4f18\u503c\u3002 5\u3001Logistic regression \u91cc\u9762\u5305\u542b: \u4ee3\u4ef7\u51fd\u6570\u548c\u68af\u5ea6\u7684\u63a8\u5bfc\u3002(u1s1, \u82f1\u6587\u7248\u771f\u7684\u5199\u5f97\u5f88\u597d) Let's now talk about the classification problem. This is just like the regression problem, except that the values \\(y\\) we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which \\(y\\) can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then \\(x^{(i)}\\) may be some features of a piece of emial, and \\(y\\) may be 1 if it is a piece of spam mail, and 0 otherwise. O is also called the negative class , and 1 the positive class , and they are sometimes also denoted by the symbols \"-\" and \"+\". Given \\(x^{(i)}\\) , the corresponding \\(y^{(i)}\\) is also called the label for the training example. We could approach the classification problem ignoring the fact that \\(y\\) is discrete-valued, and use our linear regression algorithm to try to predict \\(y\\) given \\(x\\) . However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn't make sense for \\(h_\\theta(x)\\) to take values larger than 1 or smaller than 0 when we know that \\(y \\in \\{0, 1\\}\\) . \u2002 To fix this, let's change the form for our hypotheses \\(h_\\theta(x)\\) . We will choose \\[ h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}} \\] where \\[ g(z)= \\frac{1}{1+e^{-z}} \\] is called the logistic function or the sigmoid function . Here is a plot showing \\(g(z)\\) : \u56fe 3.5.1 sigmoid function Notice that \\(g(z)\\) tends towards 1 as \\(z \\rightarrow + \\infty\\) , and \\(g(z)\\) tends towards 0 as \\(z \\rightarrow - \\infty\\) . Moreover, \\(g(z)\\) , and hence also \\(h(x)\\) , is always bounded between 0 and 1. As before, we are keeping the convention of letting \\(x_0=1\\) , so that \\(\\theta^Tx=\\theta_0+\\sum_{j=1}^n \\theta_jx_j\\) . \u2002 For now, let's take the choice of \\(g\\) as given. Other function that smoothly increase from 0 to 1 can also be used, but for a couple of reasons that we'll see later, the choice of the logistic function is a fairly natural one. Before moving on, here's a useful property of the derivative of the sigmoid function, which we write a \\(g'\\) : \\[ \\begin{aligned} g'(z) &= \\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\\\\\ &= \\frac{1}{(1+e^{-z})^2}(e^{-z})\\\\\\\\ &= \\frac{1}{1+e^{-z}} \\cdot \\Big(1-\\frac{1}{1+e^{-z}}\\Big)\\\\\\\\ &= g(z)\\big(1-g(z)\\big) \\end{aligned} \\] \u2002 So, given the logistic regression model, how do we fit \\(\\theta\\) for it? Following how we saw least squares regression could be derived as the maximum likelihood estimator under a set of assumptions, let's endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likehood. \u2002 Let us assume that \\[ P(y=1|x;\\theta) = h_\\theta(x) \\\\\\\\ P(y=0|x;\\theta) = 1-h_\\theta(x) \\] Notice that this can written more compactly as \\[ p(y|x;\\theta)=\\big(h_\\theta(x)\\big)^y \\big(1-h_\\theta(x)\\big)^{1-y} \\] Assuming that the \\(m\\) training examples were generated independently, we can then write down the likelihood of the parameters as \\[ \\begin{aligned} L(\\theta) &= p(\\vec y|X;\\theta)\\\\\\\\ &= \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\\theta)\\\\\\\\ &= \\prod_{i=1}^{m} \\big(h_\\theta(x^{(i)}\\big)^{y^{(i)}}\\big(1-h_\\theta(x^{(i)}\\big)^{1-y^{(i)}} \\end{aligned} \\] As before, it will be easier to maximize the log likelihood: \\[ \\begin{aligned} l(\\theta) &= \\log L(\\theta) \\\\\\\\ &= \\sum_{i=1}^m{y^{(i)}\\log h \\big(x^{(i)}\\big)}+(1-y^{(i)})\\log \\big(1-h(x^{(i)}\\big) \\end{aligned} \\] \u5176\u5b9e\u5230\u4e86\u8fd9\u91cc, \u56e0\u4e3a\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u662f\u6c42\u4f7f \\(l(\\theta)\\) \u53d6\u6700\u5927\u503c\u65f6\u7684 \\(\\theta\\) , \u5176\u5b9e\u539f\u6587\u5230\u4e86\u8fd9\u91cc\u662f\u662f\u7528\u4e86\u68af\u5ea6\u4e0a\u5347\u6cd5\u6c42\u89e3, \u6c42\u5f97\u7684 \\(\\theta\\) \u5c31\u662f\u6211\u4eec\u8981\u6c42\u7684\u6700\u4f73\u53c2\u6570\u3002 \u4f46\u662f\u4e3a\u4e86\u8ddf\u524d\u4e00\u8282\u7684\u4e2d\u6587\u8bfe\u7a0b\u7edf\u4e00, \u6211\u4eec\u8fd8\u662f\u7528\u68af\u5ea6\u4e0b\u964d\u6765\u505a, \u8fd9\u91cc\u6211\u4eec\u505a\u4e00\u4e2a\u8f6c\u6362\u3002\u56e0\u4e3a \\(l(\\theta)\\) \u8981\u6c42\u6700\u5927\u7b49\u4ef7\u4e8e \\(-\\frac{1}{m}l(\\theta)\\) \u53d6\u6700\u5c0f\u3002\u6240\u4ee5, \u6211\u4eec\u53ef\u4ee5\u4ee4\uff1a \\[ \\begin{aligned} J(\\theta) &= -\\frac{1}{m}l(\\theta) \\\\\\\\ &= -\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[{{y}^{(i)}}\\log \\left( {h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)+\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)]} \\end{aligned} \\] \u56e0\u4e3a\u6c42\u68af\u5ea6\u8981\u6c42 \\(\\frac{\\partial }{\\partial \\theta_j}J(\\theta)\\) , \u6240\u4ee5\u8fd9\u91cc\u987a\u4fbf\u63a8\u5bfc\u4e00\u4e0b\u3002\u8001\u5957\u8def, \u4e3a\u4e86\u7b80\u5316, \u5148\u5047\u8bbe\u8bad\u7ec3\u96c6\u53ea\u6709\u4e00\u4e2a\u3002 Let's start by working with just one training example \\((x, y)\\) \u3002 \\[ \\begin{aligned} \\frac{\\partial }{\\partial \\theta_j}l(\\theta) &= \\Big(y \\frac{1}{g(\\theta^Tx)}-(1-y) \\frac{1}{1-g(\\theta^Tx)}\\Big) \\frac{\\partial }{\\partial \\theta_j}g(\\theta^Tx)\\\\\\\\ &=\\Big(y \\frac{1}{g(\\theta^Tx)}-(1-y) \\frac{1}{1-g(\\theta^Tx)}\\Big)g(\\theta^Tx)\\big(1-g(\\theta^Tx)\\big) \\frac{\\partial }{\\partial \\theta_j}\\theta^Tx\\\\\\\\ &=\\Big(y\\big(1-g(\\theta^Tx)\\big)-\\big(1-y\\big)g(\\theta^Tx)\\Big)x_j\\\\\\\\ &=\\big(y-h_\\theta(x)\\big)x_j \\end{aligned} \\] \u6240\u4ee5 \\[ \\begin{aligned} \\frac{\\partial }{\\partial \\theta_j}J(\\theta)&=-\\frac{1}{m}l(\\theta) \\\\\\\\ &=\\big(h_\\theta(x)-y\\big)x_j \\end{aligned} \\] \u5176\u4e2d, \\(m=1\\) \u3002 \u8fd9\u4e2a\u662f\u4e0d\u662f\u5f88\u719f\u6089, \u8fd9\u4e2a\u68af\u5ea6\u5176\u5b9e\u8ddf\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u5b8c\u5168\u4e00\u6837\u3002 6\u3001\u4ee3\u4ef7\u51fd\u6570\u7b80\u5316\u548c\u68af\u5ea6\u4e0b\u964d \u53c2\u8003\u89c6\u9891: 6 - 5 - Simplified Cost Function and Gradient Descent (10 min).mkv 7\u3001\u8fdb\u9636\u4f18\u5316 \u53c2\u8003\u89c6\u9891: 6 - 6 - Advanced Optimization (14 min).mkv 8\u3001\u591a\u7c7b\u522b\u5206\u7c7b: \u4e00\u5bf9\u591a \u53c2\u8003\u89c6\u9891: 6 - 7 - Multiclass Classification_ One-vs-all (6 min).mkv","title":"\u4e09. \u903b\u8f91\u56de\u5f52"},{"location":"machine%20learning/3.%20logistic%20regression/#1","text":"\u53c2\u8003\u89c6\u9891: 6 - 1 - Classification (8 min).mkv p { text-align: justify; /*\u6587\u672c\u4e24\u7aef\u5bf9\u9f50*/ } center img{ border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08); } center div{ color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px; } \u4e0b\u9762\u6211\u4eec\u5f00\u59cb\u8ba8\u8bba\u5206\u7c7b\u95ee\u9898\u3002\u5728\u5206\u7c7b\u95ee\u9898\u4e2d, \u4f60\u8981\u9884\u6d4b\u7684\u53d8\u91cf \\(y\\) \u662f\u79bb\u6563\u7684\u503c, \u6211\u4eec\u5c06\u5b66\u4e60\u4e00\u79cd\u53eb\u505a\u903b\u8f91\u56de\u5f52 (Logistic Regression) \u7684\u7b97\u6cd5, \u8fd9\u662f\u76ee\u524d\u6700\u6d41\u884c\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u4e00\u79cd\u5b66\u4e60\u7b97\u6cd5\u3002\u6240\u4ee5\u8bf4 \u903b\u8f91\u56de\u5f52\u662f\u89e3\u51b3\u5206\u7c7b\u95ee\u9898 \u3002 \u5728\u5206\u7c7b\u95ee\u9898\u4e2d, \u6211\u4eec\u5c1d\u8bd5\u9884\u6d4b\u7684\u662f\u7ed3\u679c\u662f\u5426\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u7c7b\uff08\u4f8b\u5982\u6b63\u786e\u6216\u9519\u8bef\uff09\u3002\u5206\u7c7b\u95ee\u9898\u7684\u4f8b\u5b50\u6709: \u5224\u65ad\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\u662f\u5426\u662f\u5783\u573e\u90ae\u4ef6; \u5224\u65ad\u4e00\u6b21\u91d1\u878d\u4ea4\u6613\u662f\u5426\u662f\u6b3a\u8bc8; \u4e4b\u524d\u6211\u4eec\u4e5f\u8c08\u5230\u4e86\u80bf\u7624\u5206\u7c7b\u95ee\u9898\u7684\u4f8b\u5b50, \u533a\u522b\u4e00\u4e2a\u80bf\u7624\u662f\u6076\u6027\u7684\u8fd8\u662f\u826f\u6027\u7684\u3002 \u6211\u4eec\u73b0\u5728\u8981\u4ece\u53ea\u5305\u542b\u4e24\u7c7b0\u548c1\u7684\u4e8c\u5206\u7c7b\u5f00\u59cb, \u540e\u9762\u6211\u4eec\u5c06\u8ba8\u8bba\u591a\u5206\u7c7b\u95ee\u9898, \u4f8b\u5982\u53d8\u91cfy\u53ef\u4ee5\u53d6 0, 1, 2, 3 \u8fd9\u51e0\u4e2a\u503c\u3002\u6211\u4eec\u5c06\u56e0\u53d8\u91cf (dependent variable) \u53ef\u80fd\u5c5e\u4e8e\u7684\u4e24\u4e2a\u7c7b\u5206\u522b\u79f0\u4e3a\u8d1f\u5411\u7c7b (negative class) \u548c\u6b63\u5411\u7c7b (positive class) ,\u5219\u56e0\u53d8\u91cf, \u5176\u4e2d 0 \u8868\u793a\u8d1f\u5411\u7c7b, 1 \u8868\u793a\u6b63\u5411\u7c7b\u3002 \u6211\u4eec\u5982\u4f55\u5f00\u53d1\u4e00\u4e2a\u5206\u7c7b\u7b97\u6cd5? \u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u7684\u8bad\u7ec3\u96c6\u662f\u5bf9\u80bf\u7624\u8fdb\u884c\u6076\u6027\u548c\u826f\u6027\u5206\u7c7b\u5f97\u5230\u7684\u6570\u636e, \u6ce8\u610f\u5230\u6076\u6027\u4e0e\u5426\u53ea\u6709\u4e24\u4e2a\u503c, 0 (No) \u4ee5\u53ca 1 (Yes)\u3002\u6240\u4ee5\u5bf9\u4e8e\u8fd9\u4e2a\u8bad\u7ec3\u96c6, \u6211\u4eec\u53ef\u4ee5\u505a\u7684\u662f\u628a\u6211\u4eec\u5df2\u7ecf\u5b66\u4f1a\u7684\u7ebf\u6027\u56de\u5f52\u7b97\u6cd5\u5e94\u7528\u5230\u8fd9\u4e2a\u6570\u636e\u96c6: \u7528\u76f4\u7ebf\u5bf9\u6570\u636e\u8fdb\u884c\u62df\u5408\u3002 \u5982\u679c\u6211\u4eec\u60f3\u505a\u51fa\u9884\u6d4b\uff0c\u6211\u4eec\u53ef\u4ee5\u8bbe\u7f6e\u5206\u7c7b\u5668\u7684\u9608\u503c\u4e3a0.5, \u5373\u7eb5\u5750\u6807\u4e3a0.5\u3002 \u5982\u679c\u5047\u8bbe\u51fd\u6570\u8f93\u51fa\u4e00\u4e2a\u503c >= 0.5, \u5219\u53ef\u4ee5\u9884\u6d4b y=1 \u5982\u679c\u5047\u8bbe\u51fd\u6570\u8f93\u51fa\u4e00\u4e2a\u503c < 0.5, \u5219\u53ef\u4ee5\u9884\u6d4b y=0 \u6211\u4eec\u89c2\u5bdf\u7ed3\u679c\u53d1\u73b0, \u7eb5\u5750\u68070.5\u5bf9\u5e94\u7684\u6a2a\u5750\u6807\u53f3\u8fb9\u6211\u4eec\u5c06\u9884\u6d4b\u4e3a\u6b63, \u5de6\u8fb9\u6211\u4eec\u5c06\u9884\u6d4b\u4e3a\u8d1f\u3002 \u5728\u8fd9\u4e2a\u7279\u5b9a\u7684\u4f8b\u5b50\u4e2d, \u7ebf\u6027\u56de\u5f52\u505a\u5f97\u5f88\u597d\u4e5f\u5f88\u6b63\u786e\u3002\u6211\u4eec\u5c1d\u8bd5\u6539\u53d8\u4e00\u4e0b\u8fd9\u4e2a\u95ee\u9898, \u6211\u4eec\u5c06\u6a2a\u8f74\u5ef6\u957f\u4e00\u70b9\u3002\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4f4d\u4e8e\u53f3\u8fb9\u8fdc\u5904\u3002 \u5982\u679c\u6211\u4eec\u4f7f\u7528\u539f\u6765\u7eff\u8272\u7684\u7ebf, \u4ecd\u7136\u80fd\u505a\u5b8c\u6210\u5f88\u597d\u7684\u5206\u7c7b(\u628a\u53f3\u4fa7\u6700\u8fdc\u7684\u70b9\u7b97\u8fdb\u53bb)\u3002\u4f46\u662f\u968f\u7740\u53f3\u4fa7\u6700\u8fdc\u5904\u7684\u70b9\u52a0\u5165, \u6211\u4eec\u8fd0\u884c\u6211\u4eec\u7684\u7ebf\u6027\u56de\u5f52\u7b97\u6cd5\u6211\u4eec\u4f1a\u5f97\u5230\u53e6\u4e00\u6761\u6570\u636e\u7684\u62df\u5408\u76f4\u7ebf(\u5047\u8bbe\u53d8\u4e3a\u56fe\u4e2d\u84dd\u8272\u7684\u7ebf)\u3002\u6211\u4eec\u7ee7\u7eed\u5c06\u9608\u503c\u8bbe\u4e3a0.5, \u6211\u4eec\u4f1a\u53d1\u73b0\u5de6\u53f3\u4e24\u8fb9\u7684\u9884\u6d4b\u7ed3\u679c\u6709\u95ee\u9898\u4e86\u3002 \u603b\u7ed3\u4e00\u4e0b\u5c31\u662f, \u52a0\u4e86\u6700\u8fdc\u5904\u7684\u8bad\u7ec3\u96c6\u540e, \u4f7f\u5f97\u7ebf\u6027\u56de\u5f52\u5bf9\u6570\u636e\u7684\u62df\u5408\u76f4\u7ebf\u4ece\u7eff\u8272\u7684\u7ebf\u53d8\u5230\u4e86\u84dd\u8272\u7684\u7ebf, \u4ece\u800c\u751f\u6210\u4e86\u4e00\u4e2a\u66f4\u574f\u7684\u5047\u8bbe\u3002\u6240\u4ee5\u628a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5e94\u7528\u5230\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u4e0d\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\u3002 \u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898, \\(y\\) \u53d6\u503c\u4e3a 0 \u6216\u80051\uff0c\u4f46\u5982\u679c\u4f60\u4f7f\u7528\u7684\u662f\u7ebf\u6027\u56de\u5f52\uff0c\u90a3\u4e48\u5047\u8bbe\u51fd\u6570\u7684\u8f93\u51fa\u503c\u53ef\u80fd\u8fdc\u5927\u4e8e 1\uff0c\u6216\u8005\u8fdc\u5c0f\u4e8e0\uff0c\u5373\u4f7f\u6240\u6709\u8bad\u7ec3\u6837\u672c\u7684\u6807\u7b7e \\(y\\) \u90fd\u7b49\u4e8e 0 \u6216 1\u3002\u5c3d\u7ba1\u6211\u4eec\u77e5\u9053\u6807\u7b7e\u5e94\u8be5\u53d6\u503c0 \u6216\u80051\uff0c\u4f46\u662f\u5982\u679c\u7b97\u6cd5\u5f97\u5230\u7684\u503c\u8fdc\u5927\u4e8e1\u6216\u8005\u8fdc\u5c0f\u4e8e0\u7684\u8bdd\uff0c\u5c31\u4f1a\u611f\u89c9\u5f88\u5947\u602a\u3002\u6240\u4ee5\u6211\u4eec\u5728\u63a5\u4e0b\u6765\u7684\u8981\u7814\u7a76\u7684\u7b97\u6cd5\u5c31\u53eb\u505a\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\uff0c\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6027\u8d28\u662f\uff1a\u5b83\u7684\u8f93\u51fa\u503c\u6c38\u8fdc\u57280\u5230 1 \u4e4b\u95f4\u3002 \u987a\u4fbf\u8bf4\u4e00\u4e0b\uff0c\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u662f\u5206\u7c7b\u7b97\u6cd5\uff0c\u6211\u4eec\u5c06\u5b83\u4f5c\u4e3a\u5206\u7c7b\u7b97\u6cd5\u4f7f\u7528\u3002\u6709\u65f6\u5019\u53ef\u80fd\u56e0\u4e3a\u8fd9\u4e2a\u7b97\u6cd5\u7684\u540d\u5b57\u4e2d\u51fa\u73b0\u4e86\u201c\u56de\u5f52\u201d\u4f7f\u4f60\u611f\u5230\u56f0\u60d1\uff0c\u4f46 \u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u5b9e\u9645\u4e0a\u662f \u4e00\u79cd \u5206\u7c7b\u7b97\u6cd5 \uff0c\u5b83\u9002\u7528\u4e8e\u6807\u7b7e\u53d6\u503c\u79bb\u6563\u7684\u60c5\u51b5\uff0c\u5982\uff1a0, 1\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u5f00\u59cb\u5b66\u4e60\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u7684\u7ec6\u8282\u3002","title":"1\u3001\u5206\u7c7b\u95ee\u9898"},{"location":"machine%20learning/3.%20logistic%20regression/#2","text":"\u53c2\u8003\u89c6\u9891: 6 - 2 - Hypothesis Representation (7 min).mkv \u4e3a\u4e86\u4f7f \\(h_{\\theta}(x) \\in (0,1)\\) , \u5f15\u5165\u903b\u8f91\u56de\u5f52\u6a21\u578b, \u5b9a\u4e49\u5047\u8bbe\u51fd\u6570 \\[ h_{\\theta}(x) = g(z)=g(\\theta^Tx) \\] \u5bf9\u6bd4\u7ebf\u6027\u56de\u5f52\u51fd\u6570 \\(h_\\theta(x)=\\theta^Tx\\) , \\(g\\) \u8868\u793a\u903b\u8f91\u51fd\u6570 (logistic function), \u590d\u5408\u8d77\u6765, \u5219\u6210\u4e3a\u903b\u8f91\u56de\u5f52\u51fd\u6570\u3002 \u903b\u8f91\u51fd\u6570\u662f S \u578b\u51fd\u6570, \u4f1a\u5c06\u6240\u6709\u5b9e\u6570\u6620\u5c04\u5230(0, 1)\u8303\u56f4\u3002 sigmoid \u51fd\u6570 (\u5982\u4e0b\u56fe) \u662f\u903b\u8f91\u51fd\u6570\u7684\u7279\u6b8a\u60c5\u51b5, \u5176\u516c\u5f0f\u4e3a \\(g(z)=\\frac{1}{1+e^{-z}}\\) \u5e94\u7528sigmoid \u51fd\u6570, \u5219\u903b\u8f91\u56de\u5f52\u6a21\u578b: \\(h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}\\) \u903b\u8f91\u56de\u5f52\u6a21\u578b\u4e2d, \\(h_\\theta(x)\\) \u7684\u4f5c\u7528\u662f, \u6839\u636e\u8f93\u5165 \\(x\\) \u4ee5\u53ca\u53c2\u6570 \\(\\theta\\) , \u8ba1\u7b97\u5f97\u51fa \"\u8f93\u51fa \\(y=1\\) \" \u7684\u53ef\u80fd\u6027(estimated probability), \u6982\u7387\u5b66\u4e2d\u8868\u793a\u4e3a: \\[ h_\\theta(x) = P(y=1|x;\\theta) = 1-P(y=0|x;\\theta) \\\\\\\\ P(y=1|x;\\theta)+ P(y=0|x;\\theta) = 1 \\] \u6ce8\u610f \u4e0a\u9762\u7b2c\u4e8c\u4e2a\u5f0f\u5b50\u662f\u9488\u5bf9\u6211\u4eec\u73b0\u5728\u7814\u7a76\u7684\u4e8c\u5206\u7c7b\u95ee\u9898\u800c\u8bf4\u7684, y \u7684\u53d6\u503c\u53ea\u80fd\u662f0 \u6216 1\u3002\u6240\u4ee5\u4e24\u8005\u76f8\u52a0\u7684\u6982\u7387\u4e3a 100% \u3002 \\(P(y=1|x;\\theta)\\) \u7684\u542b\u4e49: probability that y=1, given x, parameterized by \\(\\theta\\) \u3002 \u4ee5\u80bf\u7624\u8bca\u65ad\u4e3a\u4f8b, \\(h_\\theta(x)=0.7\\) \u8868\u793a\u75c5\u4eba\u6709 \\(70%\\) \u7684\u6982\u7387\u5f97\u4e86\u6076\u6027\u80bf\u7624\u3002","title":"2\u3001\u5047\u8bbe\u51fd\u6570\u63cf\u8ff0"},{"location":"machine%20learning/3.%20logistic%20regression/#3","text":"\u53c2\u8003\u89c6\u9891: 6 - 3 - Decision Boundary (15 min).mkv \u51b3\u7b56\u8fb9\u754c\u7684\u6982\u5ff5, \u53ef\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7684\u62df\u5408\u539f\u7406\u3002 \u5728\u903b\u8f91\u56de\u5f52\u4e2d, \u6709\u5047\u8bbe\u51fd\u6570 \\(h_{\\theta}(x) = g(z)=g(\\theta^Tx)\\) \u3002 \u4e3a\u4e86\u5f97\u51fa\u5206\u7c7b\u7684\u7ed3\u679c, \u8fd9\u91cc\u548c\u524d\u9762\u4e00\u6837, \u89c4\u5b9a\u4ee5 \\(0.5\\) \u4e3a\u9608\u503c: \\[ h_\\theta(x) \\geq 0.5 \\rightarrow y=1 \\\\\\\\ h_\\theta(x) < 0.5 \\rightarrow y=0 \\\\\\\\ \\] \u56de\u5fc6\u4e00\u4e0bsigmoid\u51f8\u51fd\u6570\u7684\u56fe\u50cf : \u89c2\u5bdf\u53ef\u5f97\u5f53 \\(g(z) \\geq 0.5\\) \u65f6, \u6709 \\(z \\geq 0\\) , \u5373 \\(\\theta^T x \\geq 0\\) \u3002 sigmoid \u51fd\u6570\u7684\u516c\u5f0f\u4e3a \\(g(z)=\\frac{1}{1+e^{-z}}\\) \u540c\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u540c\u5728\u4e8e\uff1a \\[ z \\rightarrow +\\infty, e^{-\\infty} \\rightarrow 0 \\Rightarrow g(z)=1 \\\\\\\\ z \\rightarrow -\\infty, e^{+\\infty} \\rightarrow +\\infty \\Rightarrow g(z)=0 \\\\\\\\ \\] \u76f4\u89c2\u4e00\u70b9 \u4e3e\u4e2a\u6817\u5b50, \\(h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2)\\) \u662f\u4e0b\u56fe\u6a21\u578b\u7684\u5047\u8bbe\u51fd\u6570: \u56fe 3.3.1 \u51b3\u7b56\u8fb9\u754c1 \u6839\u636e\u4e0a\u9762\u7684\u8ba8\u8bba, \u8981\u8fdb\u884c\u5206\u7c7b, \u90a3\u4e48\u53ea\u8981 \\(\\theta_0+\\theta_1x+\\theta_2x \\geq 0\\) \u65f6, \u5c31\u9884\u6d4b \\(y=1\\) , \u5373\u9884\u6d4b\u4e3a\u6b63\u5411\u7c7b\u3002 \u5982\u679c\u53d6 \\(\\theta = [-3 \\ \\ 1\\ \\ 1]^T\\) , \u5219\u6709 \\(z=-3+x_1+x_2\\) , \u5f53 \\(z \\geq 0\\) , \u5373 \\(x_1+x_2 \\geq 3\\) \u65f6, \u6613\u7ed8\u5236\u51fa\u56fe\u4e2d\u7684\u54c1\u7ea2\u8272\u76f4\u7ebf, \u5373 \u51b3\u7b56\u8fb9\u754c , \u4e3a\u6b63\u5411\u7c7b (\u4ee5\u7ea2\u53c9\u6807\u6ce8\u7684\u6570\u636e) \u7ed9\u51fa \\(y=1\\) \u7684\u5206\u7c7b\u9884\u6d4b\u7ed3\u679c\u3002 \u4e0a\u9762\u8ba8\u8bba\u4e86\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4e2d\u7ebf\u6027\u62df\u5408\u7684\u4f8b\u5b50, \u4e0b\u9762\u5219\u662f\u4e00\u4e2a\u591a\u9879\u5f0f\u62df\u5408\u7684\u4f8b\u5b50, \u548c\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u60c5\u51b5\u4e5f\u662f\u7c7b\u4f3c\u7684\u3002 \u4e3a\u4e86\u62df\u5408\u4e0b\u56fe\u6570\u636e, \u5efa\u6a21\u591a\u9879\u5f0f\u5047\u8bbe\u51fd\u6570: \\[ h_\\theta(x) = g(\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1^2+\\theta_4x_2^2) \\] \u8fd9\u91cc\u53d6 \\(\\theta = [-1\\ \\ 0\\ \\ 0\\ \\ 1\\ \\ 1]^T\\) , \u51b3\u7b56\u8fb9\u754c\u5bf9\u5e94\u4e86\u4e00\u4e2a\u5728\u539f\u70b9\u5904\u7684\u5355\u4f4d\u5143 ( \\(x_1^2+x_2^2=1\\) ), \u5982\u6b64\u4fbf\u53ef\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c, \u5982\u56fe\u4e2d\u54c1\u7ea2\u8272\u66f2\u7ebf: \u56fe 3.3.2 \u51b3\u7b56\u8fb9\u754c2 \u5f53\u7136, \u901a\u8fc7\u4e00\u4e9b\u66f4\u4e3a\u590d\u6742\u7684\u591a\u9879\u5f0f, \u8fd8\u80fd\u62df\u5408\u90a3\u4e9b\u56fe\u50cf\u663e\u5f97\u975e\u5e38\u602a\u5f02\u7684\u6570\u636e, \u4f7f\u5f97\u51b3\u7b56\u8fb9\u754c\u5f62\u72b6\u4f3c\u7897\u88c5\u3001\u7231\u5fc3\u72b6\u7b49\u7b49\u3002 \u7b80\u5355\u6765\u8bf4, \u51b3\u7b56\u7684\u8fb9\u754c\u5c31\u662f \u5206\u7c7b\u7684\u5206\u754c\u7ebf , \u5206\u7c7b\u73b0\u5728\u5b9e\u9645\u5c31\u7531 \\(z\\) (\u4e2d\u7684 \\(\\theta\\) ) \u51b3\u5b9a\u5566\u3002","title":"3\u3001\u51b3\u7b56\u8fb9\u754c"},{"location":"machine%20learning/3.%20logistic%20regression/#4","text":"\u53c2\u8003\u89c6\u9891: 6 - 4 - Cost Function (11 min).mkv \u90a3\u6211\u4eec\u600e\u4e48\u77e5\u9053\u51b3\u7b56\u8fb9\u754c\u65f6\u5565\u6837? \\(\\theta\\) \u591a\u5c11\u65f6\u80fd\u5f88\u597d\u7684\u62df\u5408\u6570\u636e? \u5f53\u7136, \u89c1\u62db\u62c6\u62db, \u603b\u8981\u6765\u4e2a \\(J(\\theta)\\) \u3002 \u5982\u679c\u76f4\u63a5\u5957\u7528\u7ebf\u6027\u56de\u5f52\u7684\u4ee3\u4ef7\u51fd\u6570: \\[ J(\\theta)=\\frac{1}{2m} \\sum_{i=1}^m{\\big(h_\\theta(x^{(i)})-y^{(i)}\\big)^2} \\] \u5176\u4e2d, \\(h_\\theta(x)=g(\\theta^Tx)\\) , \u53ef\u7ed8\u5236\u5173\u4e8e \\(J(\\theta)\\) \u7684\u56fe\u50cf, \u5982\u4e0b\u56fe \u56fe 3.4.1 \u903b\u8f91\u56de\u5f52\u4ee3\u4ef7\u51fd\u6570\u76f4\u63a5\u7528\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u4ee3\u4ef7\u51fd\u6570 \u56de\u5fc6\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u5e73\u65b9\u635f\u5931\u51fd\u6570, \u5176\u5b9e\u662f\u4e00\u4e2a\u4e8c\u6b21\u51f8\u51fd\u6570 (\u7897\u72b6) , \u4e8c\u6b21\u51f8\u51fd\u6570\u7684\u91cd\u8981\u6027\u8d28\u662f\u53ea\u6709\u4e00\u4e2a\u5c40\u90e8\u6700\u5c0f\u70b9\u5373\u5168\u5c40\u6700\u5c0f\u70b9\u3002\u4e0a\u56fe\u4e2d\u6709\u8bb8\u591a\u5c40\u90e8\u6700\u5c0f\u70b9, \u8fd9\u6837\u4f7f\u5f97\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u65e0\u6cd5\u786e\u5b9a\u54ea\u4e2a\u6536\u655b\u70b9\u662f\u5168\u5c40\u6700\u4f18\u3002 \u56fe 3.4.2 \u903b\u8f91\u56de\u5f52\u4e2d\u7406\u60f3\u7684\u4ee3\u4ef7\u51fd\u6570 \u5982\u679c\u6b64\u5904\u7684\u4ee3\u4ef7\u51fd\u6570\u4e5f\u662f\u4e00\u4e2a\u51f8\u51fd\u6570, \u662f\u5426\u4e5f\u6709\u540c\u6837\u7684\u6027\u8d28, \u4ece\u800c\u6700\u4f18\u5316? \u8fd9\u7c7b\u8ba8\u8bba\u51f8\u51fd\u6570\u6700\u4f18\u503c\u5f97\u95ee\u9898, \u88ab\u79f0\u4e3a \u51f8\u4f18\u5316\u95ee\u9898 (Convex optimization) \u3002 \u5f53\u7136, \u635f\u5931\u51fd\u6570\u4e0d\u6b62\u5e73\u65b9\u635f\u5931\u51fd\u6570\u4e00\u79cd\u3002 \u5bf9\u4e8e\u903b\u8f91\u56de\u5f52, \u66f4\u6362\u5e73\u65b9\u635f\u5931\u51fd\u6570\u4e3a\u5bf9\u6570\u635f\u5931\u51fd\u6570, \u53ef\u7531\u7edf\u8ba1\u5b66\u4e2d\u7684 \u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\u63a8\u5bfc\u51fa\u4ee3\u4ef7\u51fd\u6570 \\(J(\\theta)\\) (\u4e0b\u4e00\u8282\u82f1\u6587\u7248\u4e2d\u7ed9\u51fa\u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b) : \\[ J(\\theta)=\\frac{1}{m} \\sum_{i=1}^m{Cost\\big( h_\\theta(x^{(i)}, y^{(i)}\\big)}\\\\\\\\ Cost\\big( h_\\theta(x^{(i)}, y^{(i)}\\big)=-\\log \\big(h_\\theta(x)\\big) \\ \\ \\ \\ \\ if\\ y=1 \\\\\\\\ Cost\\big( h_\\theta(x^{(i)}, y^{(i)}\\big)=-\\log \\big(1-h_\\theta(x)\\big) \\ \\ \\ \\ if\\ y=0 \\] \u5219\u5173\u4e8e \\(J(\\theta)\\) \u7684\u56fe\u50cf\u5982\u4e0b: \u56fe 3.4.3 \u5bf9\u6570\u4ee3\u4ef7\u51fd\u6570y=1&y=0\u4e24\u79cd\u60c5\u51b5 \u5982\u5de6\u4e0a\u56fe, \u5f53\u8bad\u7ec3\u96c6\u7684\u7ed3\u679c\u4e3a \\(y=1\\) (\u6b63\u6837\u672c) \u65f6, \u968f\u7740\u5047\u8bbe\u51fd\u6570\u8d8b\u5411\u4e8e1, \u4ee3\u4ef7\u51fd\u6570\u7684\u503c\u4f1a\u8d8b\u5411\u4e8e0, \u5373\u610f\u5473\u7740\u62df\u5408\u7a0b\u5ea6\u5f88\u597d\u3002\u5982\u679c\u5047\u8bbe\u51fd\u6570\u6b64\u65f6\u8d8b\u4e8e0, \u5219\u4f1a\u7ed9\u51fa\u4e00\u4e2a \u5f88\u9ad8\u7684\u4ee3\u4ef7 , \u62df\u5408\u7a0b\u5ea6 \u5dee , \u7b97\u6cd5\u4f1a\u6839\u636e\u5176\u8fc5\u901f\u7ea0\u6b63 \\(\\theta\\) \u503c, \u53f3\u56fe \\(y=0\\) \u540c\u7406\u3002 \u533a\u522b\u4e8e\u5e73\u65b9\u635f\u5931\u51fd\u6570, \u5bf9\u6570\u635f\u5931\u51fd\u6570\u4e5f\u662f\u4e00\u4e2a\u51f8\u51fd\u6570, \u4ed6\u6ca1\u6709\u5c40\u90e8\u6700\u4f18\u503c, \u53ea\u6709\u5168\u5c40\u6700\u4f18\u503c\u3002","title":"4\u3001\u4ee3\u4ef7\u51fd\u6570"},{"location":"machine%20learning/3.%20logistic%20regression/#5logistic-regression","text":"\u91cc\u9762\u5305\u542b: \u4ee3\u4ef7\u51fd\u6570\u548c\u68af\u5ea6\u7684\u63a8\u5bfc\u3002(u1s1, \u82f1\u6587\u7248\u771f\u7684\u5199\u5f97\u5f88\u597d) Let's now talk about the classification problem. This is just like the regression problem, except that the values \\(y\\) we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which \\(y\\) can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then \\(x^{(i)}\\) may be some features of a piece of emial, and \\(y\\) may be 1 if it is a piece of spam mail, and 0 otherwise. O is also called the negative class , and 1 the positive class , and they are sometimes also denoted by the symbols \"-\" and \"+\". Given \\(x^{(i)}\\) , the corresponding \\(y^{(i)}\\) is also called the label for the training example. We could approach the classification problem ignoring the fact that \\(y\\) is discrete-valued, and use our linear regression algorithm to try to predict \\(y\\) given \\(x\\) . However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn't make sense for \\(h_\\theta(x)\\) to take values larger than 1 or smaller than 0 when we know that \\(y \\in \\{0, 1\\}\\) . \u2002 To fix this, let's change the form for our hypotheses \\(h_\\theta(x)\\) . We will choose \\[ h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}} \\] where \\[ g(z)= \\frac{1}{1+e^{-z}} \\] is called the logistic function or the sigmoid function . Here is a plot showing \\(g(z)\\) : \u56fe 3.5.1 sigmoid function Notice that \\(g(z)\\) tends towards 1 as \\(z \\rightarrow + \\infty\\) , and \\(g(z)\\) tends towards 0 as \\(z \\rightarrow - \\infty\\) . Moreover, \\(g(z)\\) , and hence also \\(h(x)\\) , is always bounded between 0 and 1. As before, we are keeping the convention of letting \\(x_0=1\\) , so that \\(\\theta^Tx=\\theta_0+\\sum_{j=1}^n \\theta_jx_j\\) . \u2002 For now, let's take the choice of \\(g\\) as given. Other function that smoothly increase from 0 to 1 can also be used, but for a couple of reasons that we'll see later, the choice of the logistic function is a fairly natural one. Before moving on, here's a useful property of the derivative of the sigmoid function, which we write a \\(g'\\) : \\[ \\begin{aligned} g'(z) &= \\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\\\\\ &= \\frac{1}{(1+e^{-z})^2}(e^{-z})\\\\\\\\ &= \\frac{1}{1+e^{-z}} \\cdot \\Big(1-\\frac{1}{1+e^{-z}}\\Big)\\\\\\\\ &= g(z)\\big(1-g(z)\\big) \\end{aligned} \\] \u2002 So, given the logistic regression model, how do we fit \\(\\theta\\) for it? Following how we saw least squares regression could be derived as the maximum likelihood estimator under a set of assumptions, let's endow our classification model with a set of probabilistic assumptions, and then fit the parameters via maximum likehood. \u2002 Let us assume that \\[ P(y=1|x;\\theta) = h_\\theta(x) \\\\\\\\ P(y=0|x;\\theta) = 1-h_\\theta(x) \\] Notice that this can written more compactly as \\[ p(y|x;\\theta)=\\big(h_\\theta(x)\\big)^y \\big(1-h_\\theta(x)\\big)^{1-y} \\] Assuming that the \\(m\\) training examples were generated independently, we can then write down the likelihood of the parameters as \\[ \\begin{aligned} L(\\theta) &= p(\\vec y|X;\\theta)\\\\\\\\ &= \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\\theta)\\\\\\\\ &= \\prod_{i=1}^{m} \\big(h_\\theta(x^{(i)}\\big)^{y^{(i)}}\\big(1-h_\\theta(x^{(i)}\\big)^{1-y^{(i)}} \\end{aligned} \\] As before, it will be easier to maximize the log likelihood: \\[ \\begin{aligned} l(\\theta) &= \\log L(\\theta) \\\\\\\\ &= \\sum_{i=1}^m{y^{(i)}\\log h \\big(x^{(i)}\\big)}+(1-y^{(i)})\\log \\big(1-h(x^{(i)}\\big) \\end{aligned} \\] \u5176\u5b9e\u5230\u4e86\u8fd9\u91cc, \u56e0\u4e3a\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u662f\u6c42\u4f7f \\(l(\\theta)\\) \u53d6\u6700\u5927\u503c\u65f6\u7684 \\(\\theta\\) , \u5176\u5b9e\u539f\u6587\u5230\u4e86\u8fd9\u91cc\u662f\u662f\u7528\u4e86\u68af\u5ea6\u4e0a\u5347\u6cd5\u6c42\u89e3, \u6c42\u5f97\u7684 \\(\\theta\\) \u5c31\u662f\u6211\u4eec\u8981\u6c42\u7684\u6700\u4f73\u53c2\u6570\u3002 \u4f46\u662f\u4e3a\u4e86\u8ddf\u524d\u4e00\u8282\u7684\u4e2d\u6587\u8bfe\u7a0b\u7edf\u4e00, \u6211\u4eec\u8fd8\u662f\u7528\u68af\u5ea6\u4e0b\u964d\u6765\u505a, \u8fd9\u91cc\u6211\u4eec\u505a\u4e00\u4e2a\u8f6c\u6362\u3002\u56e0\u4e3a \\(l(\\theta)\\) \u8981\u6c42\u6700\u5927\u7b49\u4ef7\u4e8e \\(-\\frac{1}{m}l(\\theta)\\) \u53d6\u6700\u5c0f\u3002\u6240\u4ee5, \u6211\u4eec\u53ef\u4ee5\u4ee4\uff1a \\[ \\begin{aligned} J(\\theta) &= -\\frac{1}{m}l(\\theta) \\\\\\\\ &= -\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[{{y}^{(i)}}\\log \\left( {h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)+\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)]} \\end{aligned} \\] \u56e0\u4e3a\u6c42\u68af\u5ea6\u8981\u6c42 \\(\\frac{\\partial }{\\partial \\theta_j}J(\\theta)\\) , \u6240\u4ee5\u8fd9\u91cc\u987a\u4fbf\u63a8\u5bfc\u4e00\u4e0b\u3002\u8001\u5957\u8def, \u4e3a\u4e86\u7b80\u5316, \u5148\u5047\u8bbe\u8bad\u7ec3\u96c6\u53ea\u6709\u4e00\u4e2a\u3002 Let's start by working with just one training example \\((x, y)\\) \u3002 \\[ \\begin{aligned} \\frac{\\partial }{\\partial \\theta_j}l(\\theta) &= \\Big(y \\frac{1}{g(\\theta^Tx)}-(1-y) \\frac{1}{1-g(\\theta^Tx)}\\Big) \\frac{\\partial }{\\partial \\theta_j}g(\\theta^Tx)\\\\\\\\ &=\\Big(y \\frac{1}{g(\\theta^Tx)}-(1-y) \\frac{1}{1-g(\\theta^Tx)}\\Big)g(\\theta^Tx)\\big(1-g(\\theta^Tx)\\big) \\frac{\\partial }{\\partial \\theta_j}\\theta^Tx\\\\\\\\ &=\\Big(y\\big(1-g(\\theta^Tx)\\big)-\\big(1-y\\big)g(\\theta^Tx)\\Big)x_j\\\\\\\\ &=\\big(y-h_\\theta(x)\\big)x_j \\end{aligned} \\] \u6240\u4ee5 \\[ \\begin{aligned} \\frac{\\partial }{\\partial \\theta_j}J(\\theta)&=-\\frac{1}{m}l(\\theta) \\\\\\\\ &=\\big(h_\\theta(x)-y\\big)x_j \\end{aligned} \\] \u5176\u4e2d, \\(m=1\\) \u3002 \u8fd9\u4e2a\u662f\u4e0d\u662f\u5f88\u719f\u6089, \u8fd9\u4e2a\u68af\u5ea6\u5176\u5b9e\u8ddf\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u5b8c\u5168\u4e00\u6837\u3002","title":"5\u3001Logistic regression"},{"location":"machine%20learning/3.%20logistic%20regression/#6","text":"\u53c2\u8003\u89c6\u9891: 6 - 5 - Simplified Cost Function and Gradient Descent (10 min).mkv","title":"6\u3001\u4ee3\u4ef7\u51fd\u6570\u7b80\u5316\u548c\u68af\u5ea6\u4e0b\u964d"},{"location":"machine%20learning/3.%20logistic%20regression/#7","text":"\u53c2\u8003\u89c6\u9891: 6 - 6 - Advanced Optimization (14 min).mkv","title":"7\u3001\u8fdb\u9636\u4f18\u5316"},{"location":"machine%20learning/3.%20logistic%20regression/#8","text":"\u53c2\u8003\u89c6\u9891: 6 - 7 - Multiclass Classification_ One-vs-all (6 min).mkv","title":"8\u3001\u591a\u7c7b\u522b\u5206\u7c7b: \u4e00\u5bf9\u591a"},{"location":"math%20in%20ML/","text":"\u53c2\u8003 \u8fd9\u90e8\u5206\u5185\u5bb9\u4e3b\u8981\u53c2\u8003\u4e86\u795e\u79d8\u5927\u725bIterator\u5728\u77e5\u4e4e\u53d1\u8868\u7684\u5185\u5bb9\u3002 \u53c2\u8003\u94fe\u63a5\uff1a Iterator\u7684\u4e13\u680f\u300c\u5de5\u79d1\u6570\u5b66\u300d","title":"\u524d\u8a00\u548c\u76ee\u5f55"},{"location":"math%20in%20ML/#_1","text":"\u8fd9\u90e8\u5206\u5185\u5bb9\u4e3b\u8981\u53c2\u8003\u4e86\u795e\u79d8\u5927\u725bIterator\u5728\u77e5\u4e4e\u53d1\u8868\u7684\u5185\u5bb9\u3002 \u53c2\u8003\u94fe\u63a5\uff1a Iterator\u7684\u4e13\u680f\u300c\u5de5\u79d1\u6570\u5b66\u300d","title":"\u53c2\u8003"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/","text":"\u53c2\u8003\u94fe\u63a5: \u77e9\u9635\u6c42\u5bfc\u7684\u672c\u8d28\u4e0e\u5206\u5b50\u5e03\u5c40\u3001\u5206\u6bcd\u5e03\u5c40\u7684\u672c\u8d28\uff08\u77e9\u9635\u6c42\u5bfc\u2014\u2014\u672c\u8d28\u7bc7\uff09 \u4e00. \u51fd\u6570\u4e0e\u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635 \u8003\u8651\u4e00\u4e2a\u51fd\u6570 \\[ \\text{function(input)} \\] \u9488\u5bf9 \\(\\text function\\) \u7684\u7c7b\u578b\u3001 \\(\\text input\\) \u7684\u7c7b\u578b, \u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u51fd\u6570 \\(\\text function\\) \u5206\u4e3a\u4e0d\u540c\u7684\u79cd\u7c7b\u3002 1\u3001function \u662f\u4e00\u4e2a\u6807\u91cf \u6211\u4eec\u79f0 \\(\\text function\\) \u662f\u4ee5\u4e00\u4e2a \u5b9e\u503c\u6807\u91cf\u7684\u51fd\u6570 \u3002\u7528\u7ec6\u4f53\u5c0f\u5199\u5b57\u6bcd \\(f\\) \u8868\u793a\u3002 1.1 input \u662f\u4e00\u4e2a\u6807\u91cf \u6211\u4eec\u79f0 \\(\\text function\\) \u7684 \u53d8\u5143 \u662f \u6807\u91cf \u3002\u7528\u7ec6\u4f53\u5c0f\u5199\u5b57\u6bcd \\(x\\) \u8868\u793a\u3002 \u4f8b1: \\[ f(x)=x+2\\\\\\\\ \\tag{e.g.1} \\] 1.2 input \u662f\u4e00\u4e2a\u5411\u91cf \u6211\u4eec\u79f0 \\(\\text function\\) \u7684 \u53d8\u5143 \u662f \u5411\u91cf \u3002\u7528\u7c97\u4f53\u5c0f\u5199\u5b57\u6bcd \\(\\pmb{x}\\) \u8868\u793a\u3002 \u4f8b2: \u8bbe \\(\\pmb{x}=[x_1,x_2,x_3]^T\\) \\[ f(\\pmb{x}) = a_1x_1^2+a_2x_2^2+a_3x_3^2+a_4x_1x_2\\\\\\\\ \\tag{e.g.2} \\] 1.3 input \u662f\u4e00\u4e2a\u77e9\u9635 \u6211\u4eec\u79f0 \\(\\text function\\) \u7684 \u53d8\u5143 \u662f \u77e9\u9635 \u3002\u7528\u7c97\u4f53\u5927\u5199\u5b57\u6bcd \\(\\pmb{X}\\) \u8868\u793a\u3002 \u4f8b3: \u8bbe \\(\\pmb{X}_{3 \\times 2}=(x_{ij})_{i=1,j=1}^{3,2}\\) \\[ f(\\pmb{X})=a_1x_{11}^2+a_2x_{12}^2+a_3x_{21}^2+a_4x_{22}^2+a_5x_{31}^2+a_6x_{32}^2\\\\\\\\ \\tag{e.g.3} \\] 2\u3001function \u662f\u4e00\u4e2a\u5411\u91cf \u6211\u4eec\u79f0 \\(\\text function\\) \u662f\u4e00\u4e2a \u5b9e\u5411\u91cf\u51fd\u6570 \u3002\u7528 \u7c97\u4f53 \u5c0f\u5199\u5b57\u6bcd \\(\\pmb{f}\\) \u8868\u793a\u3002 \u542b\u4e49: \\(\\pmb{f}\\) \u662f\u7531\u82e5\u5e72\u4e2a \\(f\\) \u7ec4\u6210\u7684 \u5411\u91cf \u3002 \u540c\u6837\u5730, \u53d8\u5143\u5206\u4e3a\u4e09\u79cd: \u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635 \u3002\u8fd9\u91cc\u7684\u7b26\u53f7\u4ecd\u548c\u4e0a\u9762\u7684\u76f8\u540c\u3002 2.1 \u6807\u91cf\u53d8\u5143 \u4f8b\u56db: \\[ \\pmb{f}_{3 \\times 1}(x)= \\begin{bmatrix} f_1(x) \\\\\\\\ f_2(x) \\\\\\\\ f_3(x) \\end{bmatrix}= \\begin{bmatrix} x+1 \\\\\\\\ 2x+1 \\\\\\\\ 3x^2+1 \\end{bmatrix} \\\\\\\\ \\tag{e.g.4} \\] 2.2 \u5411\u91cf\u53d8\u5143 \u4f8b5: \u8bbe \\(\\pmb{x}=[x_1,x_2,x_3]^T\\) \\[ f_{3 \\times 1}(\\pmb{x})= \\begin{bmatrix} f_1(\\pmb{x})\\\\\\\\ f_2(\\pmb{x})\\\\\\\\ f_3(\\pmb{x}) \\end{bmatrix}= \\begin{bmatrix} x_1+x_2+x_3\\\\\\\\ x_1^2+2x_2+2x_3\\\\\\\\ x_1x_2+x_2+x_3 \\end{bmatrix}\\\\\\\\ \\tag{e.g.5} \\] 2.3 \u77e9\u9635\u53d8\u5143 \u4f8b6\uff1a \u8bbe \\(\\pmb{X}_{3 \\times 2}=(x_{ij})_{i=1,j=1}^{3,2}\\) \\[ \\pmb{f}_{3\\times1}(\\pmb{X})= \\begin{bmatrix} f_1(\\pmb{X})\\\\\\\\ f_2(\\pmb{X})\\\\\\\\ f_3(\\pmb{X}) \\end{bmatrix} = \\begin{bmatrix} x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\\\\\ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}+x_{11}x_{12}\\\\\\\\ 2x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}+x_{11}x_{12} \\end{bmatrix} \\\\\\\\ \\tag{e.g.6} \\] 3\u3001function \u662f\u4e00\u4e2a\u77e9\u9635 \u6211\u4eec\u79f0 \\(\\text function\\) \u662f\u4e00\u4e2a \u5b9e\u77e9\u9635\u51fd\u6570 \u3002\u7528 \u7c97\u4f53 \u5927\u5199\u5b57\u6bcd \\(\\pmb{F}\\) \u8868\u793a\u3002 \u542b\u4e49: \\(\\pmb{F}\\) \u662f\u7531\u82e5\u5e72\u4e2a \\(f\\) \u7ec4\u6210\u7684\u4e00\u4e2a \u77e9\u9635 \u3002 \u540c\u6837\u5730, \u53d8\u5143\u5206\u4e09\u4e2d: \u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635 \u3002\u8fd9\u91cc\u7684\u7b26\u53f7\u4ecd\u4e0e\u4e0a\u9762\u7684\u76f8\u540c\u3002 3.1 \u6807\u91cf\u53d8\u5143 \u4f8b7: \\[ \\pmb{F}_{3 \\times 2}(x)= \\begin{bmatrix} f_{11}(x) & f_{12}(x)\\\\\\\\ f_{21}(x) & f_{22}(x)\\\\\\\\ f_{31}(x) & f_{32}(x) \\end{bmatrix}= \\begin{bmatrix} x+1 & 2x+2\\\\\\\\ x^2+1 & 2x^2+1\\\\\\\\ x^3+1 & 2x^3+1 \\end{bmatrix}\\\\\\\\ \\tag{e.g.7} \\] 3.2 \u5411\u91cf\u53d8\u5143 \u4f8b8: \u8bbe \\(\\pmb{x}=[x_1,x_2,x_3]^T\\) \\[ \\pmb{F}_{3\\times2}(\\pmb{x})= \\left[ \\matrix{ f_{11}(\\pmb{x}) & f_{12}(\\pmb{x})\\\\ f_{21}(\\pmb{x}) & f_{22}(\\pmb{x})\\\\ f_{31}(\\pmb{x}) & f_{32}(\\pmb{x})\\\\ } \\right] = \\left[ \\matrix{ 2x_{1}+x_{2}+x_{3} & 2x_{1}+2x_{2}+x_{3} \\\\ 2x_{1}+2x_{2}+x_{3} & x_{1}+2x_{2}+x_{3} & \\\\ 2x_{1}+x_{2}+2x_{3} & x_{1}+2x_{2}+2x_{3} & } \\right] \\\\\\\\ \\tag{e.g.8} \\] 3.3 \u77e9\u9635\u53d8\u5143 \u4f8b9\uff1a \u8bbe \\(\\pmb{X}_{3 \\times 2}=(x_{ij})_{i=1,j=1}^{3,2}\\) \\[ \\begin{align*} \\pmb{F}_{3\\times2}(\\pmb{X})&= \\left[ \\matrix{ f_{11}(\\pmb{X}) & f_{12}(\\pmb{X})\\\\ f_{21}(\\pmb{X}) & f_{22}(\\pmb{X})\\\\ f_{31}(\\pmb{X}) & f_{32}(\\pmb{X})\\\\ } \\right]\\\\\\\\ &= \\left[ \\matrix{ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 2x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ 3x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 4x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ 5x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 6x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} } \\right] \\end{align*} \\\\\\\\ \\tag{e.g.9} \\] 4\u3001\u603b\u7ed3 \\(funtion/input\\) \u6807\u91cf\u53d8\u5143 \u5411\u91cf\u53d8\u5143 \u77e9\u9635\u53d8\u5143 \u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f(x)\\) \\(f(\\pmb{x})\\) \\(f(\\pmb{X})\\) \u5b9e\u5411\u91cf\u51fd\u6570 \\(\\pmb{f}(x)\\) \\(\\pmb{f(x)}\\) \\(\\pmb{f(X)}\\) \u5b9e\u77e9\u9635\u51fd\u6570 \\(\\pmb{F}(x)\\) \\(\\pmb{F(x)}\\) \\(\\pmb{F(X)}\\) \u4e8c. \u77e9\u9635\u6c42\u5bfc\u7684\u672c\u8d28 \u6211\u4eec\u5728\u9ad8\u7b49\u6570\u5b66\u4e2d\u5b66\u8fc7, \u5bf9\u4e8e\u4e00\u4e2a\u591a\u5143\u51fd\u6570 \u4f8b10: \\[ f(x_1,x_2,x_3)=x_1^2+x_1x_2+x_2x_3\\\\\\\\ \\tag{e.g.10} \\] \u6211\u4eec\u53ef\u4ee5\u5c06 \\(\\pmb{f}\\) \u5bf9 \\(x_1,x_2,x_3\\) \u7684 \u504f\u5bfc \u5206\u522b\u6c42\u51fa\u6765, \u5373: \\[ \\left\\{ \\begin{align*} \\frac{\\partial f}{\\partial x_1} & = 2x_1+x_2 \\\\\\\\ \\frac{\\partial f}{\\partial x_2} & = x_1+x_3 \\\\\\\\ \\frac{\\partial f}{\\partial x_3} & = x_2 \\end{align*} \\right. \\\\\\\\ \\] \u77e9\u9635\u6c42\u5bfc\u4e5f\u662f\u4e00\u6837\uff0c \u672c\u8d28\u5c31\u662f \\(function\\) \u4e2d\u7684\u6bcf\u4e2a \\(f\\) \u5206\u522b\u5bf9\u53d8\u5143\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u9010\u4e2a\u6c42\u504f\u5bfc, \u53ea\u4e0d\u8fc7\u5199\u6210\u4e86\u5411\u91cf\u3001\u77e9\u9635\u5f62\u5f0f\u800c\u5df2\u3002 \u5bf9\u4e8e \\((e.g.10)\\) , \u6211\u4eec\u628a\u5f97\u51fa\u7684\u4e09\u4e2a\u7ed3\u679c\u5199\u6210\u5217\u5411\u91cf\u5f62\u5f0f: \\[ \\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}_{3 \\times 1}}= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}\\\\\\\\ \\frac{\\partial f}{\\partial x_2}\\\\\\\\ \\frac{\\partial f}{\\partial x_3}\\\\\\end{bmatrix}= \\begin{bmatrix} 2x_1+x_2\\\\\\\\ x_1+x_3\\\\\\\\ x_2 \\\\\\end{bmatrix}\\\\\\\\ \\tag{1} \\] \u4e00\u4e2a\u77e9\u9635\u6c42\u5bfc \u4ee5\u5217\u5411\u91cf\u5f62\u5f0f\u5c55\u5f00 \u7684\u96cf\u5f62\u5c31\u51fa\u73b0\u4e86\u3002 \u5f53\u7136\u6211\u4eec\u4e5f\u53ef\u4ee5 \u4ee5\u884c\u5411\u91cf\u5f62\u5f0f\u5c55\u5f00: \\[ \\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}_{3 \\times 1}^T}=\\Big[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}\\Big]=[2x_1+x_2, x_1+x_3,x_2]\\\\\\\\ \\tag{2} \\] \u6240\u4ee5, \u5982\u679c \\(function\\) \u4e2d\u6709 \\(m\\) \u4e2a \\(f\\) , \u53d8\u5143\u4e2d\u6709 \\(n\\) \u4e2a\u5143\u7d20, \u90a3\u4e48\u6bcf\u4e2a \\(f\\) \u5bf9\u53d8\u5143\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u9010\u4e2a\u6c42\u504f\u5bfc\u540e\uff0c\u6211\u4eec\u5c31\u4f1a\u4ea7\u751f \\(m \\times n\\) \u4e2a\u7ed3\u679c\u3002 \u8fd9\u5c31\u662f\u77e9\u9635\u6c42\u5bfc\u7684\u672c\u8d28\u3002 \u81f3\u4e8e\u8fd9 \\(m \\times n\\) \u4e2a\u7ed3\u679c\u7684\u5e03\u5c40, \u662f\u5199\u6210\u884c\u5411\u91cf, \u8fd8\u662f\u5217\u5411\u91cf, \u8fd8\u662f\u5199\u6210\u77e9\u9635, \u5c31\u662f\u6211\u4eec\u63a5\u4e0b\u6765\u8981\u8ba8\u8bba\u7684\u4e8b\u60c5\u3002 \u4e09. \u77e9\u9635\u6c42\u5bfc\u7ed3\u679c\u7684\u5e03\u5c40 \u4e0d\u4e25\u8c28 \u5730\u8bf4\uff0c\u4ece \u76f4\u89c2 \u4e0a\u770b: \u5206\u5b50\u5e03\u5c40 , \u5c31\u662f\u5206\u5b50\u662f \u5217\u5411\u91cf \u5f62\u5f0f, \u5206\u6bcd\u662f \u884c\u5411\u91cf \u5f62\u5f0f, \u5982 \\((2)\\) \u5f0f\u3002\u5982\u679c\u8fd9\u91cc\u7684 \\(function\\) \u662f\u5b9e\u5411\u91cf\u51fd\u6570 \\(\\pmb f_{21}\\) \u7684\u8bdd, \u7ed3\u679c\u5c31\u662f \\(2 \\times 3\\) \u7684\u77e9\u9635\u4e86\uff1a \\[ \\frac{\\partial \\pmb{f}_{2 \\times 1}(\\pmb{x})} {\\partial \\pmb{x}_{3 \\times 1}^T}= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_1}{\\partial x_3} \\\\\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_3} \\\\\\end{bmatrix}_{2 \\times 3}\\\\\\\\ \\tag{3} \\] \u5206\u6bcd\u5e03\u5c40 , \u5c31\u662f\u5206\u6bcd\u662f \u5217\u5411\u91cf \u5f62\u5f0f, \u5206\u5b50\u662f \u884c\u5411\u91cf \u5f62\u5f0f, \u5982 \\((1)\\) \u5f0f\u3002\u5982\u679c\u8fd9\u91cc\u7684 \\(function\\) \u662f\u5b9e\u5411\u91cf\u51fd\u6570 \\(\\pmb f_{21}\\) \u7684\u8bdd, \u7ed3\u679c\u5c31\u662f \\(3 \\times 2\\) \u7684\u77e9\u9635\u4e86\uff1a \\[ \\frac{\\partial \\pmb{f}_{2 \\times 1}^T(\\pmb{x})} {\\partial \\pmb{x}_{3 \\times 1}}= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_1} \\\\\\\\ \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_2} \\\\\\\\ \\frac{\\partial f_1}{\\partial x_3} & \\frac{\\partial f_2}{\\partial x_3} \\end{bmatrix}_{3 \\times 2}\\\\\\\\ \\tag{4} \\] \u76f4\u89c2\u4e0a\u7406\u89e3\u4e86\u4e4b\u540e\uff0c\u6211\u4eec\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684 \\(function\\) \uff0c\u4e0d\u540c\u7c7b\u578b\u7684 \u53d8\u5143 \uff0c\u7ed9\u51fa \u4e25\u8c28 \u7684\u5e03\u5c40\u8bf4\u660e\u3002\uff08\u8fd9\u91cc\u4e0d\u8ba8\u8bba\u6807\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f(x)\\) \uff0c\u56e0\u4e3a\u7ed3\u679c\u5c31\u662f\u4e00\u4e2a\u5143\u7d20\u561b~\uff09 1\u3001\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u51fd\u6570 \\[ f(\\pmb{x}), \\pmb{x}=[x_1,x_2,\\cdots , x_n]^T \\] 1.1 \u884c\u5411\u91cf\u504f\u5bfc\u5f62\u5f0f(\u884c\u504f\u5bfc\u5411\u91cf) \\[ D_{\\pmb{x}}f(\\pmb{x})=\\frac{\\partial f(\\pmb x)}{\\partial \\pmb{x}^T}=\\Big[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots , \\frac{\\partial f}{\\partial x_n}\\Big]\\\\\\\\ \\tag{5} \\] 1.2 \u68af\u5ea6\u5411\u91cf\u5f62\u5f0f(\u5217\u5411\u91cf\u504f\u5bfc \u5217\u504f\u5bfc\u5411\u91cf) \\[ \\nabla_{\\pmb {x}}f(\\pmb {x})=\\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}}=\\Big[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots , \\frac{\\partial f}{\\partial x_n}\\Big]^T\\\\\\\\ \\tag{6} \\] \u8fd9\u4e24\u79cd\u5f62\u5f0f \u4e92\u4e3a\u8f6c\u7f6e \u3002 2\u3001\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\[ f(\\pmb {X}), \\pmb {X}_{m \\times n}=(x_{ij})_{i=1,j=1}^{m,n} \\] \u5148\u4ecb\u7ecd\u4e00\u4e2a\u7b26\u53f7 \\(\\text{vec}(\\pmb {X})\\) , \u4f5c\u7528\u662f\u5c06\u77e9\u9635 \\(\\pmb {X}\\) \u6309\u5217\u5806\u6808\u6765\u5411\u91cf\u5316\u3002 \u89e3\u91ca\u4e00\u4e0b, \\(\\text{vec}(\\pmb {X})\\) \u5c31\u662f\u628a\u77e9\u9635 \\(\\pmb {X}\\) \u7684\u7b2c 1 \u5217, \u7b2c 2 \u5217, \u76f4\u5230\u7b2c n \u5217\u53d6\u51fa\u6765\uff0c\u7136\u540e\u6309\u987a\u5e8f\u7ec4\u6210\u4e00\u4e2a\u5217\u5411\u91cf\uff0c\u5373\uff1a \\[ \\text{vec}(\\pmb {X})=[x_{11},x_{21},\\cdots ,x_{m1},x_{12},x_{22},\\cdots ,x_{m2},\\cdots , x_{1n},x_{2n},\\cdots ,x_{mn}]^T\\\\\\\\ \\tag{7} \\] 2.1 \u884c\u5411\u91cf\u504f\u5bfc\u5f62\u5f0f(\u884c\u504f\u5bfc\u5411\u91cf) \u5373\u5148\u628a \u77e9\u9635\u53d8\u5143 \\(\\pmb {X}\\) \u6309 \\(vec\\) \u5411\u91cf\u5316 \uff0c\u8f6c\u6362\u6210 \u5411\u91cf \u53d8\u5143\uff0c\u518d\u5bf9\u8be5 \u5411\u91cf\u53d8\u5143 \u4f7f\u7528 \\((5)\\) \u5f0f\uff1a \\[ \\begin{align*} \\text{D}_{\\text{vec}\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\text{vec}^T(\\pmb{X})} \\\\\\\\ &= \\left[ \\frac{\\partial f}{\\partial x_{11}},\\frac{\\partial f}{\\partial x_{21}},\\cdots,\\frac{\\partial f}{\\partial x_{m1}},\\frac{\\partial f}{\\partial x_{12}},\\frac{\\partial f}{\\partial x_{22}},\\cdots,\\frac{\\partial f}{\\partial x_{m2}},\\cdots,\\frac{\\partial f} {\\partial x_{1n}},\\frac{\\partial f}{\\partial x_{2n}},\\cdots,\\frac{\\partial f}{\\partial x_{mn}} \\right] \\end{align*} \\\\\\\\ \\tag{8} \\] 2.2 Jacobian\u77e9\u9635\u5f62\u5f0f \u5373\u5148\u628a \u77e9\u9635\u53d8\u5143 \\(\\pmb {X}\\) \u8fdb\u884c\u8f6c\u7f6e, \u518d\u5bf9\u8f6c\u7f6e\u540e\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5143\u7d20\u9010\u4e2a\u6c42\u504f\u5bfc\uff0c\u7ed3\u679c\u5e03\u5c40\u548c\u8f6c\u7f6e\u5e03\u5c40\u4e00\u6837\u3002 \\[ \\begin{align*} \\text{D}_{\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T_{m\\times n}} \\\\\\\\ &= \\left[ \\matrix{ \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} } \\right]_{n\\times m} \\end{align*} \\\\\\\\ \\tag{9} \\] 2.3 \u68af\u5ea6\u5411\u91cf\u5f62\u5f0f(\u5217\u504f\u5bfc\u5411\u91cf, \u5217\u5411\u91cf\u504f\u5bfc) \u5373\u7ebf\u628a \u77e9\u9635 \u53d8\u5143 \\(\\pmb{X}\\) \u6309 \\(\\text{vec}\\) \u5411\u91cf\u5316 , \u8f6c\u6362\u6210 \u5411\u91cf \u53d8\u5143, \u518d\u5bf9\u8be5 \u53d8\u5143 \u4f7f\u7528 \\((6)\\) \u5f0f: \\[ \\begin{align*} \\nabla_{\\text{vec}\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\text{vec}\\pmb{X}} \\\\\\\\ &= \\left[ \\frac{\\partial f}{\\partial x_{11}},\\frac{\\partial f}{\\partial x_{21}},\\cdots,\\frac{\\partial f}{\\partial x_{m1}},\\frac{\\partial f}{\\partial x_{12}},\\frac{\\partial f}{\\partial x_{22}},\\cdots,\\frac{\\partial f}{\\partial x_{m2}},\\cdots,\\frac{\\partial f} {\\partial x_{1n}},\\frac{\\partial f}{\\partial x_{2n}},\\cdots,\\frac{\\partial f}{\\partial x_{mn}} \\right]^T \\end{align*} \\\\\\\\ \\tag{10} \\] 2.4 \u68af\u5ea6\u77e9\u9635\u5f62\u5f0f \u76f4\u63a5\u5bf9\u539f\u77e9\u9635\u53d8\u5143 \\(\\pmb{X}\\) \u7684\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5143\u7d20\u9010\u4e2a\u6c42\u5bfc, \u7ed3\u679c\u5e03\u5c40\u548c\u539f\u77e9\u9635\u5e03\u5c40\u4e00\u6837\u3002 \\[ \\begin{align*} \\nabla_{\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}_{m\\times n}} \\\\\\\\ &= \\left[ \\matrix{ \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{12}}&\\cdots&\\frac{\\partial f}{\\partial x_{1n}} \\\\ \\frac{\\partial f}{\\partial x_{21}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{2n}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{m1}}&\\frac{\\partial f}{\\partial x_{m2}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} } \\right]_{m\\times n} \\end{align*} \\\\\\\\ \\tag{11} \\] 2.5 \u4e00\u4e9b\u53d1\u73b0 2.5.1 \u8f6c\u7f6e \\((9)\\) \u5f0f\u4e0e \\((11)\\) \u5f0f\u4e92\u4e3a\u8f6c\u7f6e \uff1b \\((8)\\) \u5f0f\u4e0e \\((10)\\) \u5f0f\u4e92\u4e3a\u8f6c\u7f6e\u3002 2.5.2 \u76f8\u7b49 \u5f53 \u77e9\u9635 \u53d8\u5143 \\(\\pmb{X}\\) \u672c\u8eab\u5c31\u662f\u4e00\u4e2a \u5217\u5411\u91cf \\(\\pmb{x}=[x_1,x_2,\\cdots ,x_n]^T\\) \u65f6, \\((5)\\) \u5f0f\u3001 \\((8)\\) \u5f0f\u3001 \\((9)\\) \u5f0f \u76f8\u7b49 ; \\((6)\\) \u5f0f\u3001 \\((10)\\) \u5f0f\u3001 \\((11)\\) \u5f0f \u76f8\u7b49 ; \u5f53\u7136, \u524d\u4e09\u4e2a\u5f0f\u5b50\u4e0e\u540e\u4e09\u4e2a\u5f0f\u5b50 \u4e92\u4e3a\u8f6c\u7f6e \u3002 \u8fd9\u4e00\u53d1\u73b0\u8bf4\u660e\uff0c\u5bf9\u4e8e \u5411\u91cf \u53d8\u5143\u7684 \u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f(\\pmb {x})\\) , \\(\\pmb{x}=[x_1,x_2,\\cdots ,x_n]^T\\) , \u7ed3\u679c\u5e03\u5c40\u672c\u8d28\u4e0a\u6709\u4e24\u79cd\u5f62\u5f0f\uff0c\u4e00\u79cd\u662f \\(\\text{Jacobian}\\) \u77e9\u9635\uff08 \u5df2\u7ecf\u6210\u884c\u5411\u91cf\u4e86 \uff09\u5f62\u5f0f\uff0c\u4e00\u79cd\u662f \u68af\u5ea6\u77e9\u9635\uff08\u5df2\u7ecf\u6210\u5217\u5411\u91cf\u4e86\uff09 \u5f62\u5f0f\u3002\u4e24\u79cd\u5f62\u5f0f \u4e92\u4e3a\u8f6c\u7f6e \u3002 \u540e\u9762\u7b2c\u4e09\u5927\u5757 \u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570, \u6682\u65f6\u7528\u4e0d\u5230 \uff0c\u6240\u4ee5\u5c31\u4e0d\u8bb0\u4e86\u3002\u8981\u7528\u7684\u65f6\u5019\u518d\u8bf4\u3002 \u56db. \u5206\u5b50\u5e03\u5c40\u3001\u5206\u6bcd\u5e03\u5c40\u7684\u672c\u8d28 \u770b\u5230\u8fd9\u91cc\uff0c\u76f8\u4fe1\u540c\u5b66\u4eec\u5bf9\u77e9\u9635\u6c42\u5bfc\u7ed3\u679c\u7684\u5e03\u5c40\u6709\u4e86\u5f88\u5168\u9762\u7684\u4e86\u89e3\u4e86\uff0c\u65e0\u975e\u5c31\u662f \u5206\u5b50\u7684\u8f6c\u7f6e\u3001\u5411\u91cf\u5316\uff0c\u5206\u6bcd\u7684\u8f6c\u7f6e\u3001\u5411\u91cf\u5316 \uff0c\u5b83\u4eec\u7684 \u5404\u79cd\u7ec4\u5408 \u800c\u5df2\u3002 \u7ed3\u5408\u4e0a\u8ff0\u77e5\u8bc6\uff0c\u6211\u4eec\u603b\u7ed3\uff1a 1\u3001 \u5206\u5b50\u5e03\u5c40\u7684\u672c\u8d28 \uff1a\u5206\u5b50\u662f \u6807\u91cf\u3001\u5217\u5411\u91cf \u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684 \u5217\u5411\u91cf \uff1b\u5206\u6bcd\u662f \u6807\u91cf \u3001\u5217\u5411\u91cf\u8f6c\u7f6e\u540e\u7684 \u884c\u5411\u91cf \u3001\u77e9\u9635\u7684 \u8f6c\u7f6e \u77e9\u9635\u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684\u5217\u5411\u91cf\u8f6c\u7f6e\u540e\u7684 \u884c\u5411\u91cf \u3002 2\u3001 \u5206\u6bcd\u5e03\u5c40\u7684\u672c\u8d28 \uff1a\u5206\u5b50\u662f \u6807\u91cf \u3001\u5217\u5411\u91cf \u8f6c\u7f6e \u540e\u7684 \u884c\u5411\u91cf \u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684\u5217\u5411\u91cf \u8f6c\u7f6e \u540e\u7684 \u884c\u5411\u91cf \uff1b\u5206\u6bcd\u662f \u6807\u91cf\u3001\u5217\u5411\u91cf\u3001\u77e9\u9635\u81ea\u5df1 \u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684 \u5217\u5411\u91cf \u3002 \u601d\u8003\u4e00\u4e0b\uff0c\u5176\u5b9e\u6211\u4eec\u53ef\u4ee5\u518d\u7b80\u6d01\u4e00\u4e9b\uff1a \u8c01\u8f6c\u7f6e\u4e86\uff0c\u5c31\u662f\u53e6\u4e00\u65b9\u7684\u5e03\u5c40 \u3002\u5206\u5b50\u8f6c\u7f6e\u4e86\uff0c\u5c31\u662f\u5206\u6bcd\u5e03\u5c40\uff1b\u5206\u6bcd\u8f6c\u7f6e\u4e86\uff0c\u5c31\u662f\u5206\u5b50\u5e03\u5c40\u3002","title":"\u4e00. \u77e9\u9635\u6c42\u5bfc\u672c\u8d28"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#_1","text":"\u8003\u8651\u4e00\u4e2a\u51fd\u6570 \\[ \\text{function(input)} \\] \u9488\u5bf9 \\(\\text function\\) \u7684\u7c7b\u578b\u3001 \\(\\text input\\) \u7684\u7c7b\u578b, \u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u51fd\u6570 \\(\\text function\\) \u5206\u4e3a\u4e0d\u540c\u7684\u79cd\u7c7b\u3002","title":"\u4e00. \u51fd\u6570\u4e0e\u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#1function","text":"\u6211\u4eec\u79f0 \\(\\text function\\) \u662f\u4ee5\u4e00\u4e2a \u5b9e\u503c\u6807\u91cf\u7684\u51fd\u6570 \u3002\u7528\u7ec6\u4f53\u5c0f\u5199\u5b57\u6bcd \\(f\\) \u8868\u793a\u3002","title":"1\u3001function \u662f\u4e00\u4e2a\u6807\u91cf"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#11-input","text":"\u6211\u4eec\u79f0 \\(\\text function\\) \u7684 \u53d8\u5143 \u662f \u6807\u91cf \u3002\u7528\u7ec6\u4f53\u5c0f\u5199\u5b57\u6bcd \\(x\\) \u8868\u793a\u3002 \u4f8b1: \\[ f(x)=x+2\\\\\\\\ \\tag{e.g.1} \\]","title":"1.1 input \u662f\u4e00\u4e2a\u6807\u91cf"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#12-input","text":"\u6211\u4eec\u79f0 \\(\\text function\\) \u7684 \u53d8\u5143 \u662f \u5411\u91cf \u3002\u7528\u7c97\u4f53\u5c0f\u5199\u5b57\u6bcd \\(\\pmb{x}\\) \u8868\u793a\u3002 \u4f8b2: \u8bbe \\(\\pmb{x}=[x_1,x_2,x_3]^T\\) \\[ f(\\pmb{x}) = a_1x_1^2+a_2x_2^2+a_3x_3^2+a_4x_1x_2\\\\\\\\ \\tag{e.g.2} \\]","title":"1.2 input \u662f\u4e00\u4e2a\u5411\u91cf"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#13-input","text":"\u6211\u4eec\u79f0 \\(\\text function\\) \u7684 \u53d8\u5143 \u662f \u77e9\u9635 \u3002\u7528\u7c97\u4f53\u5927\u5199\u5b57\u6bcd \\(\\pmb{X}\\) \u8868\u793a\u3002 \u4f8b3: \u8bbe \\(\\pmb{X}_{3 \\times 2}=(x_{ij})_{i=1,j=1}^{3,2}\\) \\[ f(\\pmb{X})=a_1x_{11}^2+a_2x_{12}^2+a_3x_{21}^2+a_4x_{22}^2+a_5x_{31}^2+a_6x_{32}^2\\\\\\\\ \\tag{e.g.3} \\]","title":"1.3 input \u662f\u4e00\u4e2a\u77e9\u9635"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#2function","text":"\u6211\u4eec\u79f0 \\(\\text function\\) \u662f\u4e00\u4e2a \u5b9e\u5411\u91cf\u51fd\u6570 \u3002\u7528 \u7c97\u4f53 \u5c0f\u5199\u5b57\u6bcd \\(\\pmb{f}\\) \u8868\u793a\u3002 \u542b\u4e49: \\(\\pmb{f}\\) \u662f\u7531\u82e5\u5e72\u4e2a \\(f\\) \u7ec4\u6210\u7684 \u5411\u91cf \u3002 \u540c\u6837\u5730, \u53d8\u5143\u5206\u4e3a\u4e09\u79cd: \u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635 \u3002\u8fd9\u91cc\u7684\u7b26\u53f7\u4ecd\u548c\u4e0a\u9762\u7684\u76f8\u540c\u3002","title":"2\u3001function \u662f\u4e00\u4e2a\u5411\u91cf"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#21","text":"\u4f8b\u56db: \\[ \\pmb{f}_{3 \\times 1}(x)= \\begin{bmatrix} f_1(x) \\\\\\\\ f_2(x) \\\\\\\\ f_3(x) \\end{bmatrix}= \\begin{bmatrix} x+1 \\\\\\\\ 2x+1 \\\\\\\\ 3x^2+1 \\end{bmatrix} \\\\\\\\ \\tag{e.g.4} \\]","title":"2.1 \u6807\u91cf\u53d8\u5143"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#22","text":"\u4f8b5: \u8bbe \\(\\pmb{x}=[x_1,x_2,x_3]^T\\) \\[ f_{3 \\times 1}(\\pmb{x})= \\begin{bmatrix} f_1(\\pmb{x})\\\\\\\\ f_2(\\pmb{x})\\\\\\\\ f_3(\\pmb{x}) \\end{bmatrix}= \\begin{bmatrix} x_1+x_2+x_3\\\\\\\\ x_1^2+2x_2+2x_3\\\\\\\\ x_1x_2+x_2+x_3 \\end{bmatrix}\\\\\\\\ \\tag{e.g.5} \\]","title":"2.2 \u5411\u91cf\u53d8\u5143"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#23","text":"\u4f8b6\uff1a \u8bbe \\(\\pmb{X}_{3 \\times 2}=(x_{ij})_{i=1,j=1}^{3,2}\\) \\[ \\pmb{f}_{3\\times1}(\\pmb{X})= \\begin{bmatrix} f_1(\\pmb{X})\\\\\\\\ f_2(\\pmb{X})\\\\\\\\ f_3(\\pmb{X}) \\end{bmatrix} = \\begin{bmatrix} x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\\\\\ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}+x_{11}x_{12}\\\\\\\\ 2x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}+x_{11}x_{12} \\end{bmatrix} \\\\\\\\ \\tag{e.g.6} \\]","title":"2.3 \u77e9\u9635\u53d8\u5143"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#3function","text":"\u6211\u4eec\u79f0 \\(\\text function\\) \u662f\u4e00\u4e2a \u5b9e\u77e9\u9635\u51fd\u6570 \u3002\u7528 \u7c97\u4f53 \u5927\u5199\u5b57\u6bcd \\(\\pmb{F}\\) \u8868\u793a\u3002 \u542b\u4e49: \\(\\pmb{F}\\) \u662f\u7531\u82e5\u5e72\u4e2a \\(f\\) \u7ec4\u6210\u7684\u4e00\u4e2a \u77e9\u9635 \u3002 \u540c\u6837\u5730, \u53d8\u5143\u5206\u4e09\u4e2d: \u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635 \u3002\u8fd9\u91cc\u7684\u7b26\u53f7\u4ecd\u4e0e\u4e0a\u9762\u7684\u76f8\u540c\u3002","title":"3\u3001function \u662f\u4e00\u4e2a\u77e9\u9635"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#31","text":"\u4f8b7: \\[ \\pmb{F}_{3 \\times 2}(x)= \\begin{bmatrix} f_{11}(x) & f_{12}(x)\\\\\\\\ f_{21}(x) & f_{22}(x)\\\\\\\\ f_{31}(x) & f_{32}(x) \\end{bmatrix}= \\begin{bmatrix} x+1 & 2x+2\\\\\\\\ x^2+1 & 2x^2+1\\\\\\\\ x^3+1 & 2x^3+1 \\end{bmatrix}\\\\\\\\ \\tag{e.g.7} \\]","title":"3.1 \u6807\u91cf\u53d8\u5143"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#32","text":"\u4f8b8: \u8bbe \\(\\pmb{x}=[x_1,x_2,x_3]^T\\) \\[ \\pmb{F}_{3\\times2}(\\pmb{x})= \\left[ \\matrix{ f_{11}(\\pmb{x}) & f_{12}(\\pmb{x})\\\\ f_{21}(\\pmb{x}) & f_{22}(\\pmb{x})\\\\ f_{31}(\\pmb{x}) & f_{32}(\\pmb{x})\\\\ } \\right] = \\left[ \\matrix{ 2x_{1}+x_{2}+x_{3} & 2x_{1}+2x_{2}+x_{3} \\\\ 2x_{1}+2x_{2}+x_{3} & x_{1}+2x_{2}+x_{3} & \\\\ 2x_{1}+x_{2}+2x_{3} & x_{1}+2x_{2}+2x_{3} & } \\right] \\\\\\\\ \\tag{e.g.8} \\]","title":"3.2 \u5411\u91cf\u53d8\u5143"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#33","text":"\u4f8b9\uff1a \u8bbe \\(\\pmb{X}_{3 \\times 2}=(x_{ij})_{i=1,j=1}^{3,2}\\) \\[ \\begin{align*} \\pmb{F}_{3\\times2}(\\pmb{X})&= \\left[ \\matrix{ f_{11}(\\pmb{X}) & f_{12}(\\pmb{X})\\\\ f_{21}(\\pmb{X}) & f_{22}(\\pmb{X})\\\\ f_{31}(\\pmb{X}) & f_{32}(\\pmb{X})\\\\ } \\right]\\\\\\\\ &= \\left[ \\matrix{ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 2x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ 3x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 4x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ 5x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 6x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} } \\right] \\end{align*} \\\\\\\\ \\tag{e.g.9} \\]","title":"3.3 \u77e9\u9635\u53d8\u5143"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#4","text":"\\(funtion/input\\) \u6807\u91cf\u53d8\u5143 \u5411\u91cf\u53d8\u5143 \u77e9\u9635\u53d8\u5143 \u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f(x)\\) \\(f(\\pmb{x})\\) \\(f(\\pmb{X})\\) \u5b9e\u5411\u91cf\u51fd\u6570 \\(\\pmb{f}(x)\\) \\(\\pmb{f(x)}\\) \\(\\pmb{f(X)}\\) \u5b9e\u77e9\u9635\u51fd\u6570 \\(\\pmb{F}(x)\\) \\(\\pmb{F(x)}\\) \\(\\pmb{F(X)}\\)","title":"4\u3001\u603b\u7ed3"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#_2","text":"\u6211\u4eec\u5728\u9ad8\u7b49\u6570\u5b66\u4e2d\u5b66\u8fc7, \u5bf9\u4e8e\u4e00\u4e2a\u591a\u5143\u51fd\u6570 \u4f8b10: \\[ f(x_1,x_2,x_3)=x_1^2+x_1x_2+x_2x_3\\\\\\\\ \\tag{e.g.10} \\] \u6211\u4eec\u53ef\u4ee5\u5c06 \\(\\pmb{f}\\) \u5bf9 \\(x_1,x_2,x_3\\) \u7684 \u504f\u5bfc \u5206\u522b\u6c42\u51fa\u6765, \u5373: \\[ \\left\\{ \\begin{align*} \\frac{\\partial f}{\\partial x_1} & = 2x_1+x_2 \\\\\\\\ \\frac{\\partial f}{\\partial x_2} & = x_1+x_3 \\\\\\\\ \\frac{\\partial f}{\\partial x_3} & = x_2 \\end{align*} \\right. \\\\\\\\ \\] \u77e9\u9635\u6c42\u5bfc\u4e5f\u662f\u4e00\u6837\uff0c \u672c\u8d28\u5c31\u662f \\(function\\) \u4e2d\u7684\u6bcf\u4e2a \\(f\\) \u5206\u522b\u5bf9\u53d8\u5143\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u9010\u4e2a\u6c42\u504f\u5bfc, \u53ea\u4e0d\u8fc7\u5199\u6210\u4e86\u5411\u91cf\u3001\u77e9\u9635\u5f62\u5f0f\u800c\u5df2\u3002 \u5bf9\u4e8e \\((e.g.10)\\) , \u6211\u4eec\u628a\u5f97\u51fa\u7684\u4e09\u4e2a\u7ed3\u679c\u5199\u6210\u5217\u5411\u91cf\u5f62\u5f0f: \\[ \\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}_{3 \\times 1}}= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}\\\\\\\\ \\frac{\\partial f}{\\partial x_2}\\\\\\\\ \\frac{\\partial f}{\\partial x_3}\\\\\\end{bmatrix}= \\begin{bmatrix} 2x_1+x_2\\\\\\\\ x_1+x_3\\\\\\\\ x_2 \\\\\\end{bmatrix}\\\\\\\\ \\tag{1} \\] \u4e00\u4e2a\u77e9\u9635\u6c42\u5bfc \u4ee5\u5217\u5411\u91cf\u5f62\u5f0f\u5c55\u5f00 \u7684\u96cf\u5f62\u5c31\u51fa\u73b0\u4e86\u3002 \u5f53\u7136\u6211\u4eec\u4e5f\u53ef\u4ee5 \u4ee5\u884c\u5411\u91cf\u5f62\u5f0f\u5c55\u5f00: \\[ \\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}_{3 \\times 1}^T}=\\Big[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}\\Big]=[2x_1+x_2, x_1+x_3,x_2]\\\\\\\\ \\tag{2} \\] \u6240\u4ee5, \u5982\u679c \\(function\\) \u4e2d\u6709 \\(m\\) \u4e2a \\(f\\) , \u53d8\u5143\u4e2d\u6709 \\(n\\) \u4e2a\u5143\u7d20, \u90a3\u4e48\u6bcf\u4e2a \\(f\\) \u5bf9\u53d8\u5143\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u9010\u4e2a\u6c42\u504f\u5bfc\u540e\uff0c\u6211\u4eec\u5c31\u4f1a\u4ea7\u751f \\(m \\times n\\) \u4e2a\u7ed3\u679c\u3002 \u8fd9\u5c31\u662f\u77e9\u9635\u6c42\u5bfc\u7684\u672c\u8d28\u3002 \u81f3\u4e8e\u8fd9 \\(m \\times n\\) \u4e2a\u7ed3\u679c\u7684\u5e03\u5c40, \u662f\u5199\u6210\u884c\u5411\u91cf, \u8fd8\u662f\u5217\u5411\u91cf, \u8fd8\u662f\u5199\u6210\u77e9\u9635, \u5c31\u662f\u6211\u4eec\u63a5\u4e0b\u6765\u8981\u8ba8\u8bba\u7684\u4e8b\u60c5\u3002","title":"\u4e8c. \u77e9\u9635\u6c42\u5bfc\u7684\u672c\u8d28"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#_3","text":"\u4e0d\u4e25\u8c28 \u5730\u8bf4\uff0c\u4ece \u76f4\u89c2 \u4e0a\u770b: \u5206\u5b50\u5e03\u5c40 , \u5c31\u662f\u5206\u5b50\u662f \u5217\u5411\u91cf \u5f62\u5f0f, \u5206\u6bcd\u662f \u884c\u5411\u91cf \u5f62\u5f0f, \u5982 \\((2)\\) \u5f0f\u3002\u5982\u679c\u8fd9\u91cc\u7684 \\(function\\) \u662f\u5b9e\u5411\u91cf\u51fd\u6570 \\(\\pmb f_{21}\\) \u7684\u8bdd, \u7ed3\u679c\u5c31\u662f \\(2 \\times 3\\) \u7684\u77e9\u9635\u4e86\uff1a \\[ \\frac{\\partial \\pmb{f}_{2 \\times 1}(\\pmb{x})} {\\partial \\pmb{x}_{3 \\times 1}^T}= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_1}{\\partial x_3} \\\\\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_3} \\\\\\end{bmatrix}_{2 \\times 3}\\\\\\\\ \\tag{3} \\] \u5206\u6bcd\u5e03\u5c40 , \u5c31\u662f\u5206\u6bcd\u662f \u5217\u5411\u91cf \u5f62\u5f0f, \u5206\u5b50\u662f \u884c\u5411\u91cf \u5f62\u5f0f, \u5982 \\((1)\\) \u5f0f\u3002\u5982\u679c\u8fd9\u91cc\u7684 \\(function\\) \u662f\u5b9e\u5411\u91cf\u51fd\u6570 \\(\\pmb f_{21}\\) \u7684\u8bdd, \u7ed3\u679c\u5c31\u662f \\(3 \\times 2\\) \u7684\u77e9\u9635\u4e86\uff1a \\[ \\frac{\\partial \\pmb{f}_{2 \\times 1}^T(\\pmb{x})} {\\partial \\pmb{x}_{3 \\times 1}}= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_1} \\\\\\\\ \\frac{\\partial f_1}{\\partial x_2} & \\frac{\\partial f_2}{\\partial x_2} \\\\\\\\ \\frac{\\partial f_1}{\\partial x_3} & \\frac{\\partial f_2}{\\partial x_3} \\end{bmatrix}_{3 \\times 2}\\\\\\\\ \\tag{4} \\] \u76f4\u89c2\u4e0a\u7406\u89e3\u4e86\u4e4b\u540e\uff0c\u6211\u4eec\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684 \\(function\\) \uff0c\u4e0d\u540c\u7c7b\u578b\u7684 \u53d8\u5143 \uff0c\u7ed9\u51fa \u4e25\u8c28 \u7684\u5e03\u5c40\u8bf4\u660e\u3002\uff08\u8fd9\u91cc\u4e0d\u8ba8\u8bba\u6807\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f(x)\\) \uff0c\u56e0\u4e3a\u7ed3\u679c\u5c31\u662f\u4e00\u4e2a\u5143\u7d20\u561b~\uff09","title":"\u4e09. \u77e9\u9635\u6c42\u5bfc\u7ed3\u679c\u7684\u5e03\u5c40"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#1","text":"\\[ f(\\pmb{x}), \\pmb{x}=[x_1,x_2,\\cdots , x_n]^T \\]","title":"1\u3001\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u51fd\u6570"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#11","text":"\\[ D_{\\pmb{x}}f(\\pmb{x})=\\frac{\\partial f(\\pmb x)}{\\partial \\pmb{x}^T}=\\Big[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots , \\frac{\\partial f}{\\partial x_n}\\Big]\\\\\\\\ \\tag{5} \\]","title":"1.1 \u884c\u5411\u91cf\u504f\u5bfc\u5f62\u5f0f(\u884c\u504f\u5bfc\u5411\u91cf)"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#12","text":"\\[ \\nabla_{\\pmb {x}}f(\\pmb {x})=\\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}}=\\Big[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots , \\frac{\\partial f}{\\partial x_n}\\Big]^T\\\\\\\\ \\tag{6} \\] \u8fd9\u4e24\u79cd\u5f62\u5f0f \u4e92\u4e3a\u8f6c\u7f6e \u3002","title":"1.2 \u68af\u5ea6\u5411\u91cf\u5f62\u5f0f(\u5217\u5411\u91cf\u504f\u5bfc \u5217\u504f\u5bfc\u5411\u91cf)"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#2","text":"\\[ f(\\pmb {X}), \\pmb {X}_{m \\times n}=(x_{ij})_{i=1,j=1}^{m,n} \\] \u5148\u4ecb\u7ecd\u4e00\u4e2a\u7b26\u53f7 \\(\\text{vec}(\\pmb {X})\\) , \u4f5c\u7528\u662f\u5c06\u77e9\u9635 \\(\\pmb {X}\\) \u6309\u5217\u5806\u6808\u6765\u5411\u91cf\u5316\u3002 \u89e3\u91ca\u4e00\u4e0b, \\(\\text{vec}(\\pmb {X})\\) \u5c31\u662f\u628a\u77e9\u9635 \\(\\pmb {X}\\) \u7684\u7b2c 1 \u5217, \u7b2c 2 \u5217, \u76f4\u5230\u7b2c n \u5217\u53d6\u51fa\u6765\uff0c\u7136\u540e\u6309\u987a\u5e8f\u7ec4\u6210\u4e00\u4e2a\u5217\u5411\u91cf\uff0c\u5373\uff1a \\[ \\text{vec}(\\pmb {X})=[x_{11},x_{21},\\cdots ,x_{m1},x_{12},x_{22},\\cdots ,x_{m2},\\cdots , x_{1n},x_{2n},\\cdots ,x_{mn}]^T\\\\\\\\ \\tag{7} \\]","title":"2\u3001\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#21_1","text":"\u5373\u5148\u628a \u77e9\u9635\u53d8\u5143 \\(\\pmb {X}\\) \u6309 \\(vec\\) \u5411\u91cf\u5316 \uff0c\u8f6c\u6362\u6210 \u5411\u91cf \u53d8\u5143\uff0c\u518d\u5bf9\u8be5 \u5411\u91cf\u53d8\u5143 \u4f7f\u7528 \\((5)\\) \u5f0f\uff1a \\[ \\begin{align*} \\text{D}_{\\text{vec}\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\text{vec}^T(\\pmb{X})} \\\\\\\\ &= \\left[ \\frac{\\partial f}{\\partial x_{11}},\\frac{\\partial f}{\\partial x_{21}},\\cdots,\\frac{\\partial f}{\\partial x_{m1}},\\frac{\\partial f}{\\partial x_{12}},\\frac{\\partial f}{\\partial x_{22}},\\cdots,\\frac{\\partial f}{\\partial x_{m2}},\\cdots,\\frac{\\partial f} {\\partial x_{1n}},\\frac{\\partial f}{\\partial x_{2n}},\\cdots,\\frac{\\partial f}{\\partial x_{mn}} \\right] \\end{align*} \\\\\\\\ \\tag{8} \\]","title":"2.1 \u884c\u5411\u91cf\u504f\u5bfc\u5f62\u5f0f(\u884c\u504f\u5bfc\u5411\u91cf)"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#22-jacobian","text":"\u5373\u5148\u628a \u77e9\u9635\u53d8\u5143 \\(\\pmb {X}\\) \u8fdb\u884c\u8f6c\u7f6e, \u518d\u5bf9\u8f6c\u7f6e\u540e\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5143\u7d20\u9010\u4e2a\u6c42\u504f\u5bfc\uff0c\u7ed3\u679c\u5e03\u5c40\u548c\u8f6c\u7f6e\u5e03\u5c40\u4e00\u6837\u3002 \\[ \\begin{align*} \\text{D}_{\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T_{m\\times n}} \\\\\\\\ &= \\left[ \\matrix{ \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} } \\right]_{n\\times m} \\end{align*} \\\\\\\\ \\tag{9} \\]","title":"2.2 Jacobian\u77e9\u9635\u5f62\u5f0f"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#23_1","text":"\u5373\u7ebf\u628a \u77e9\u9635 \u53d8\u5143 \\(\\pmb{X}\\) \u6309 \\(\\text{vec}\\) \u5411\u91cf\u5316 , \u8f6c\u6362\u6210 \u5411\u91cf \u53d8\u5143, \u518d\u5bf9\u8be5 \u53d8\u5143 \u4f7f\u7528 \\((6)\\) \u5f0f: \\[ \\begin{align*} \\nabla_{\\text{vec}\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\text{vec}\\pmb{X}} \\\\\\\\ &= \\left[ \\frac{\\partial f}{\\partial x_{11}},\\frac{\\partial f}{\\partial x_{21}},\\cdots,\\frac{\\partial f}{\\partial x_{m1}},\\frac{\\partial f}{\\partial x_{12}},\\frac{\\partial f}{\\partial x_{22}},\\cdots,\\frac{\\partial f}{\\partial x_{m2}},\\cdots,\\frac{\\partial f} {\\partial x_{1n}},\\frac{\\partial f}{\\partial x_{2n}},\\cdots,\\frac{\\partial f}{\\partial x_{mn}} \\right]^T \\end{align*} \\\\\\\\ \\tag{10} \\]","title":"2.3 \u68af\u5ea6\u5411\u91cf\u5f62\u5f0f(\u5217\u504f\u5bfc\u5411\u91cf, \u5217\u5411\u91cf\u504f\u5bfc)"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#24","text":"\u76f4\u63a5\u5bf9\u539f\u77e9\u9635\u53d8\u5143 \\(\\pmb{X}\\) \u7684\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5143\u7d20\u9010\u4e2a\u6c42\u5bfc, \u7ed3\u679c\u5e03\u5c40\u548c\u539f\u77e9\u9635\u5e03\u5c40\u4e00\u6837\u3002 \\[ \\begin{align*} \\nabla_{\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}_{m\\times n}} \\\\\\\\ &= \\left[ \\matrix{ \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{12}}&\\cdots&\\frac{\\partial f}{\\partial x_{1n}} \\\\ \\frac{\\partial f}{\\partial x_{21}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{2n}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{m1}}&\\frac{\\partial f}{\\partial x_{m2}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} } \\right]_{m\\times n} \\end{align*} \\\\\\\\ \\tag{11} \\]","title":"2.4 \u68af\u5ea6\u77e9\u9635\u5f62\u5f0f"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#25","text":"","title":"2.5 \u4e00\u4e9b\u53d1\u73b0"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#251","text":"\\((9)\\) \u5f0f\u4e0e \\((11)\\) \u5f0f\u4e92\u4e3a\u8f6c\u7f6e \uff1b \\((8)\\) \u5f0f\u4e0e \\((10)\\) \u5f0f\u4e92\u4e3a\u8f6c\u7f6e\u3002","title":"2.5.1 \u8f6c\u7f6e"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#252","text":"\u5f53 \u77e9\u9635 \u53d8\u5143 \\(\\pmb{X}\\) \u672c\u8eab\u5c31\u662f\u4e00\u4e2a \u5217\u5411\u91cf \\(\\pmb{x}=[x_1,x_2,\\cdots ,x_n]^T\\) \u65f6, \\((5)\\) \u5f0f\u3001 \\((8)\\) \u5f0f\u3001 \\((9)\\) \u5f0f \u76f8\u7b49 ; \\((6)\\) \u5f0f\u3001 \\((10)\\) \u5f0f\u3001 \\((11)\\) \u5f0f \u76f8\u7b49 ; \u5f53\u7136, \u524d\u4e09\u4e2a\u5f0f\u5b50\u4e0e\u540e\u4e09\u4e2a\u5f0f\u5b50 \u4e92\u4e3a\u8f6c\u7f6e \u3002 \u8fd9\u4e00\u53d1\u73b0\u8bf4\u660e\uff0c\u5bf9\u4e8e \u5411\u91cf \u53d8\u5143\u7684 \u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f(\\pmb {x})\\) , \\(\\pmb{x}=[x_1,x_2,\\cdots ,x_n]^T\\) , \u7ed3\u679c\u5e03\u5c40\u672c\u8d28\u4e0a\u6709\u4e24\u79cd\u5f62\u5f0f\uff0c\u4e00\u79cd\u662f \\(\\text{Jacobian}\\) \u77e9\u9635\uff08 \u5df2\u7ecf\u6210\u884c\u5411\u91cf\u4e86 \uff09\u5f62\u5f0f\uff0c\u4e00\u79cd\u662f \u68af\u5ea6\u77e9\u9635\uff08\u5df2\u7ecf\u6210\u5217\u5411\u91cf\u4e86\uff09 \u5f62\u5f0f\u3002\u4e24\u79cd\u5f62\u5f0f \u4e92\u4e3a\u8f6c\u7f6e \u3002 \u540e\u9762\u7b2c\u4e09\u5927\u5757 \u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570, \u6682\u65f6\u7528\u4e0d\u5230 \uff0c\u6240\u4ee5\u5c31\u4e0d\u8bb0\u4e86\u3002\u8981\u7528\u7684\u65f6\u5019\u518d\u8bf4\u3002","title":"2.5.2 \u76f8\u7b49"},{"location":"math%20in%20ML/1.%20M%20derivation%20essence/#_4","text":"\u770b\u5230\u8fd9\u91cc\uff0c\u76f8\u4fe1\u540c\u5b66\u4eec\u5bf9\u77e9\u9635\u6c42\u5bfc\u7ed3\u679c\u7684\u5e03\u5c40\u6709\u4e86\u5f88\u5168\u9762\u7684\u4e86\u89e3\u4e86\uff0c\u65e0\u975e\u5c31\u662f \u5206\u5b50\u7684\u8f6c\u7f6e\u3001\u5411\u91cf\u5316\uff0c\u5206\u6bcd\u7684\u8f6c\u7f6e\u3001\u5411\u91cf\u5316 \uff0c\u5b83\u4eec\u7684 \u5404\u79cd\u7ec4\u5408 \u800c\u5df2\u3002 \u7ed3\u5408\u4e0a\u8ff0\u77e5\u8bc6\uff0c\u6211\u4eec\u603b\u7ed3\uff1a 1\u3001 \u5206\u5b50\u5e03\u5c40\u7684\u672c\u8d28 \uff1a\u5206\u5b50\u662f \u6807\u91cf\u3001\u5217\u5411\u91cf \u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684 \u5217\u5411\u91cf \uff1b\u5206\u6bcd\u662f \u6807\u91cf \u3001\u5217\u5411\u91cf\u8f6c\u7f6e\u540e\u7684 \u884c\u5411\u91cf \u3001\u77e9\u9635\u7684 \u8f6c\u7f6e \u77e9\u9635\u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684\u5217\u5411\u91cf\u8f6c\u7f6e\u540e\u7684 \u884c\u5411\u91cf \u3002 2\u3001 \u5206\u6bcd\u5e03\u5c40\u7684\u672c\u8d28 \uff1a\u5206\u5b50\u662f \u6807\u91cf \u3001\u5217\u5411\u91cf \u8f6c\u7f6e \u540e\u7684 \u884c\u5411\u91cf \u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684\u5217\u5411\u91cf \u8f6c\u7f6e \u540e\u7684 \u884c\u5411\u91cf \uff1b\u5206\u6bcd\u662f \u6807\u91cf\u3001\u5217\u5411\u91cf\u3001\u77e9\u9635\u81ea\u5df1 \u3001\u77e9\u9635\u5411\u91cf\u5316\u540e\u7684 \u5217\u5411\u91cf \u3002 \u601d\u8003\u4e00\u4e0b\uff0c\u5176\u5b9e\u6211\u4eec\u53ef\u4ee5\u518d\u7b80\u6d01\u4e00\u4e9b\uff1a \u8c01\u8f6c\u7f6e\u4e86\uff0c\u5c31\u662f\u53e6\u4e00\u65b9\u7684\u5e03\u5c40 \u3002\u5206\u5b50\u8f6c\u7f6e\u4e86\uff0c\u5c31\u662f\u5206\u6bcd\u5e03\u5c40\uff1b\u5206\u6bcd\u8f6c\u7f6e\u4e86\uff0c\u5c31\u662f\u5206\u5b50\u5e03\u5c40\u3002","title":"\u56db. \u5206\u5b50\u5e03\u5c40\u3001\u5206\u6bcd\u5e03\u5c40\u7684\u672c\u8d28"},{"location":"math%20in%20ML/2.%20M%20derivation/","text":"\u53c2\u8003\u94fe\u63a5: \u77e9\u9635\u6c42\u5bfc\u516c\u5f0f\u7684\u6570\u5b66\u63a8\u5bfc\uff08\u77e9\u9635\u6c42\u5bfc\u2014\u2014\u8fdb\u9636\u7bc7\uff09 \u4e00. \u77e9\u9635\u7684\u8ff9 1\u3001\u5b9a\u4e49 \\(n \\times n\\) \u7684 \u65b9\u9635 \\(A_{n \\times n}\\) \u7684\u4e3b\u5bf9\u89d2\u7ebf\u5143\u7d20\u4e4b\u548c\u5c31\u53eb\u505a\u77e9\u9635 \\(A\\) \u7684\u8ff9(trace), \u8bb0\u4f5c \\(tr(A)\\) , \u5373: \\(\\pmb{A}_{n \\times n} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nn} \\\\ \\end{bmatrix}_{n \\times n}\\\\\\\\\\) \\(A\\) \u7684\u8ff9\u4e3a: \\[tr(\\pmb{A})=a_{11} + a_{22} + \\cdots + a_{nn} = \\sum_{i=1}^n{a_{ii}} \\tag{1}\\] \u6ce8\u610f\uff1a\u53ea\u6709 \u65b9\u9635 \u624d\u6709\u8ff9\u3002 2\u3001\u4e00\u4e9b\u6027\u8d28(\u5efa\u8bae\u719f\u8bb0) 2.1 \u6807\u91cf\u7684\u8ff9 \u5bf9\u4e8e\u4e00\u4e2a\u6807\u91cf \\(x\\) ,\u53ef\u4ee5\u770b\u6210\u662f \\(1 \\times 1\\) \u7684\u77e9\u9635\uff0c\u5b83\u7684\u8ff9\u5c31\u662f\u5b83\u81ea\u5df1\u3002 \\[x=tr(x)\\\\\\\\ \\tag{2}\\] 2.2 \u7ebf\u6027\u6cd5\u5219 \u76f8\u52a0\u518d\u6c42\u8ff9\u7b49\u4e8e\u6c42\u8ff9\u518d\u76f8\u52a0\uff0c\u6807\u91cf\u63d0\u5916\u9762 \\[tr(c_1A+c_2B)=c_1tr(A)+c_2tr(B)\\\\\\\\ \\tag{3}\\] \u5176\u4e2d, \\(c_1,c_2\\) \u4e3a\u6807\u91cf\u3002 \u8bc1\u660e\uff1a \\[ \\begin{align} tr(c_1\\pmb{A}+c_2\\pmb{B}) &= tr \\begin{bmatrix} c_1a_{11}+c_2b_{11} & c_1a_{12}+c_2b_{12} & \\cdots & c_1a_{1n}+c_2b_{1n} \\\\ c_1a_{21}+c_2b_{21} & c_1a_{22}+c_2b_{22} & \\cdots & c_1a_{2n}+c_2b_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_1a_{n1}+c_2b_{n1} & c_1a_{n2}+c_2b_{n2} & \\cdots & c_1a_{nn}+c_2b_{nn} \\\\ \\end{bmatrix} \\\\\\\\ &= (c_1a_{11}+c_2b_{11})+(c_1a_{22}+c_2b_{22})+\\cdots + (c_1a_{nn}+c_2b_{nn}) \\\\\\\\ &= c_1(a_{11}+a_{22}+\\cdots+a_{nn}) + c_2(b_{11}+b_{22}+\\cdots+b_{nn}) \\\\\\\\ &= c_1tr(\\pmb{A})+c_2tr(\\pmb{B}) \\end{align} \\\\\\\\ \\tag{4} \\] \u8bc1\u6bd5\u3002 2.3 \u8f6c\u7f6e \u8f6c\u7f6e\u7684\u8ff9\u7b49\u4e8e\u539f\u77e9\u9635\u7684\u8ff9 \\[tr(A^T)=tr(A)\\\\\\\\ \\tag{5}\\] \u8bc1\u660e\uff1a \u56e0\u4e3a\u8f6c\u7f6e\u4e0d\u4f1a\u6539\u53d8\u4e3b\u5bf9\u89d2\u7ebf\u7684\u5143\u7d20\uff0c\u6545\u6210\u7acb\u3002 \u8bc1\u6bd5\u3002 2.4 \u4e58\u79ef\u7684\u8ff9\u7684\u672c\u8d28 \u5bf9\u4e8e\u4e24\u4e2a\u9636\u6570\u90fd\u662f \\(m \\times n\\) \u7684\u77e9\u9635 \\(A_{m \\times n}, B_{m \\times n}\\) \u5176\u4e2d\u4e00\u4e2a\u77e9\u9635\u4e58\u4ee5\uff08\u5de6\u4e58\u53f3\u4e58\u90fd\u53ef\u4ee5\uff09\u53e6\u4e00\u4e2a\u77e9\u9635\u7684\u8f6c\u7f6e\u7684\u8ff9\uff0c\u672c\u8d28\u662f \\(A_{m \\times n}, B_{m \\times n}\\) \u4e24\u4e2a\u77e9\u9635\u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u5411\u91cf\u7684\u70b9\u79ef\u5728\u77e9\u9635\u4e0a\u7684\u63a8\u5e7f\uff0c\u5373\uff1a \\[\\begin{align} tr(\\pmb{A}\\pmb{B}^T) &= a_{11}b_{11}+a_{12}b_{12}+\\cdots+a_{1n}b_{1n}\\\\ & +a_{21}b_{21}+a_{22}b_{22}+ \\cdots +a_{2n}b_{2n}\\\\ & + \\cdots \\\\ & + a_{m1}b_{m1} + a_{m2}b_{m2} + \\cdots a_{mn}b_{mn} \\end{align} \\\\\\\\ \\tag{6}\\] \u8bc1\u660e \\[ \\begin{align} tr(\\pmb{A}\\pmb{B}^T) &= tr( \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\begin{bmatrix} b_{11} & b_{21} & \\cdots & b_{m1} \\\\ b_{12} & b_{22} & \\cdots & b_{m2} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ b_{1n} & b_{2n} & \\cdots & b_{mn} \\\\ \\end{bmatrix} ) \\\\\\\\ &= tr \\begin{bmatrix} a_{11}b_{11} + a_{12}b_{12} + \\cdots + a_{1n}b_{1n} & \u4e0d\u7528\u7ba1 & \\cdots & \u4e0d\u7528\u7ba1 \\\\ \u4e0d\u7528\u7ba1 & a_{21}b_{21} + a_{22}b_{22} + \\cdots + a_{2n}b_{2n} & \\cdots & \u4e0d\u7528\u7ba1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \u4e0d\u7528\u7ba1 & \u4e0d\u7528\u7ba1 & \\cdots & a_{m1}b_{m1} + a_{m2}b_{m2} + \\cdots + a_{mn}b_{mn} \\\\ \\end{bmatrix}_{m \\times m} \\\\\\\\ &= a_{11}b_{11} + a_{12}b_{12} + \\cdots + a_{1n}b_{1n}\\\\ & + a_{21}b_{21} + a_{22}b_{22} + \\cdots + a_{2n}b_{2n}\\\\ & + \\cdots \\\\ & + a_{m1}b_{m1} + a_{m2}b_{m2} + \\cdots + a_{mn}b_{mn} \\end{align} \\\\\\\\ \\tag{7} \\] \u8bc1\u6bd5\u3002 2.5 \u4ea4\u6362\u5f8b \u77e9\u9635\u4e58\u79ef\u4f4d\u7f6e\u4e92\u6362\uff0c\u8ff9\u4e0d\u53d8 \\[tr(AB)=tr(BA)\\\\\\\\ \\tag{8}\\] \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times m}\\) \u3002 \u8bc1\u660e\uff1a \u628a \\(B_{n \\times m}\\) \u770b\u505a\u662f \\((B^T)_{m \\times n}\\) \u7684\u8f6c\u7f6e\u3002\u7531\u4e58\u79ef\u7684\u8ff9\u7684\u672c\u8d28\uff0c\u5373 \\((6)\\) \u5f0f\u53ef\u77e5\uff0c\u65e0\u8bba\u4e58\u79ef\u600e\u4e48\u4ea4\u6362\u987a\u5e8f\uff0c \\(A_{m \\times n}\\) \u4e0e \\((B^T)_{m \\times n}\\) \u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0\uff0c\u6c38\u8fdc\u662f\u4e0d\u53d8\u7684\u3002 \u8bc1\u6bd5\u3002 2.6 \u66f4\u591a\u7684\u4ea4\u6362\u5f8b \\[tr(ABC)=tr(CAB)=tr(BCA)\\\\\\\\ \\tag{9}\\] \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times p}, C_{p \\times m}\\) \u3002 \u8bc1\u660e\uff1a \u628a\u4e24\u4e2a\u77e9\u9635\u7684\u4e58\u79ef\u770b\u505a\u4e00\u4e2a\u77e9\u9635\uff0c\u548c\u53e6\u5916\u7684\u4e00\u4e2a\u77e9\u9635\u5e94\u7528\u4ea4\u6362\u5f8b\u5373\u53ef\u3002 \u8bc1\u6bd5\u3002 2.7 \u719f\u7ec3\u4f7f\u7528 \\[tr(AB^T)=tr(B^TA)=tr(A^TB)=tr(BA^T)\\\\\\\\ \\tag{10}\\] \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times m}\\) \u3002 \u8bc1\u660e\uff1a \u7b2c\u4e00\u4e2a\u548c\u7b2c\u4e8c\u4e2a\u662f\u4ea4\u6362\u5f8b\uff0c\u7b2c\u4e8c\u4e2a\u548c\u4e09\u4e2a\u662f\u8f6c\u7f6e\uff0c\u7b2c\u4e09\u4e2a\u548c\u7b2c\u56db\u4e2a\u662f\u4ea4\u6362\u5f8b\u3002 \u8bc1\u6bd5\u3002 \u4e8c. \u5fae\u5206\u4e0e\u5168\u5fae\u5206 \u6211\u4eec\u5148\u6765\u590d\u4e60\u4e00\u4e0b\u672c\u79d1\u9636\u6bb5\u6240\u5b66\u7684\u9ad8\u7b49\u6570\u5b66\u4e2d\u7684\u5fae\u5206\u4e0e\u5168\u5fae\u5206\u3002 1\u3001\u4e00\u5143\u51fd\u6570\u7684\u5fae\u5206 1.1 \u666e\u901a\u51fd\u6570\u7684\u5fae\u5206 \u8bbe \\(y=f(x)\\) , \\(y\\) \u53ef\u5bfc, \u5219\u5176\u5fae\u5206\u4e3a\uff1a \\[\\mathbb{d}y=\\mathbb{d}f(x)=f'(x)\\mathbb{d}x\\\\\\\\ \\tag{11}\\] 1.2 \u590d\u5408\u51fd\u6570\u7684\u5fae\u5206 \u8bbe \\(y=f(u), u=g(x)\\) , \u5747\u53ef\u5bfc, \u5219 \\(y\\) \u5fae\u5206\u4e3a\uff1a \\[\\mathbb{d}y=\\mathbb{d}f(u)=f'(u)\\mathbb{d}g(x)=f'(u)g'(x)\\mathbb{d}x\\\\\\\\ \\tag{12}\\] \u4e4d\u4e00\u770b\u5f88\u590d\u6742\uff0c\u5176\u5b9e\u4e3e\u4e2a\u4f8b\u5b50\u5c31\u5f88\u7b80\u5355\uff1a \u8bbe \\(y=sin(2x+1), u=2x+1\\) , \u5219 \\(y\\) \u7684\u5fae\u5206\u4e3a\uff1a \\[\\begin{align} \\mathbb{d}y&=\\mathbb{d}(\\sin{u})=\\cos{u}\\mathbb{d}u=\\cos(2x+1)\\mathbb{d}(2x+1) \\\\\\\\ &=\\cos(2x+1) \\cdot 2 \\mathbb{d}x=2\\cos(2x+1) \\mathbb{d}x \\end{align} \\\\\\\\ \\tag{13}\\] 2\u3001\u591a\u5143\u51fd\u6570\u7684\u5168\u5fae\u5206 2.1 \u666e\u901a\u51fd\u6570\u7684\u5168\u5fae\u5206 \u8bbe \\(z=f(x,y), z\\) \u53ef\u5fae, \u5219\u5176\u5168\u5fae\u5206\u4e3a\uff1a \\[\\mathbb{d}z=\\frac{\\partial z}{\\partial x}\\mathbb{d}x+\\frac{\\partial z}{\\partial y}\\mathbb{d}y\\\\\\\\ \\tag{14}\\] 2.2 \u590d\u5408\u51fd\u6570\u7684\u5168\u5fae\u5206 \u8bbe \\(z=f(u), u=\\varphi(x,y), z\\) \u53ef\u5bfc, \\(u\\) \u53ef\u5fae, \u5219\u5176\u5168\u5fae\u5206\u4e3a\uff1a \\[ \\begin{align} dz & = df(u)-f'(u)du=f'(u)(\\frac{\\partial u}{\\partial x}dx+\\frac{\\partial u}{\\partial y}dy)\\\\ &=f'(u)\\frac{\\partial u}{\\partial x}dx+f'(u)\\frac{\\partial u}{\\partial y}dy \\end{align}\\\\\\ \\tag{15} \\] \u4e3e\u4e2a\u4f8b\u5b50\uff1a \u8bbe \\(z=sin(2x+y^2), u=2x+y^2\\) , \u5219 \\(z\\) \u7684\u5168\u5fae\u5206\u4e3a\uff1a \\[ \\begin{align} dz & = d(sin(u)) = cosudu=cos(2x+y^2)d(2x+y^2)\\\\ & = cos(2x+y^2)(2dx+2ydy)\\\\ & = 2cos(2x+y^2)dx+2ycos(2x+y^2)dy\\\\\\ \\end{align} \\] 3\u3001\u5fae\u5206/\u5168\u5fae\u5206\u7684\u6cd5\u5219 3.1 \u5e38\u6570\u7684\u5fae\u5206 \\[dc=0\\\\\\\\ \\tag{16.1}\\] \u5176\u4e2d, \\(c\\) \u4e3a\u5e38\u6570\u3002 3.2 \u7ebf\u6027\u6cd5\u5219 \u76f8\u52a0\u518d\u5fae\u5206\u7b49\u4e8e\u5fae\u5206\u518d\u76f8\u52a0\uff0c\u5e38\u6570\u63d0\u5916\u9762 \\[d(c_1u+c_2v)=c_1du+c_2dv\\\\\\\\ \\tag{16.2}\\] \u5176\u4e2d, \u4e00\u5143\u51fd\u6570 \\(u=u(x), v=v(x)\\) \u6216\u591a\u5143\u51fd\u6570 \\(u=u(x,y, v=v(x,y), c_1,c_2\\) \u4e3a\u5e38\u6570\u3002 3.3 \u4e58\u6cd5\u6cd5\u5219 \u524d\u5fae\u540e\u4e0d\u5fae + \u524d\u4e0d\u5fae\u540e\u5fae \\[d(uv)=d(u)v+ud(v)\\\\ \\tag{16.3}\\] 3.4 \u5546\u6cd5\u5219 (\u4e0a\u5fae\u4e0b\u4e0d\u5fae \u51cf \u4e0a\u4e0d\u5fae\u4e0b\u5fae\uff09 \u9664\u4ee5 \uff08\u4e0b\u7684\u5e73\u65b9\uff09 \\[d(\\frac{u}{v})=\\frac{1}{v^2}(d(u)v-ud(v))\\] \u5176\u4e2d, \u4e00\u5143\u51fd\u6570 \\(v=v(x)\\neq0, u=u(x)\\) \u6216\u591a\u5143\u51fd\u6570 \\(v=v(x,y)\\neq0, u=u(x,y)\\) \u3002 \u4e09. \u77e9\u9635\u7684\u5fae\u5206 1\u3001\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\[f(\\pmb{x}),\\pmb{x}=[x_1,x_2,\\cdots,x_n]^T\\\\\\\\\\] \u4ed6\u5176\u5b9e\u5c31\u662f\u591a\u5143\u51fd\u6570, \u8bbe\u5176\u53ef\u5fae, \u5219\u4ed6\u7684\u5168\u5fae\u5206, \u5373 \\((14)\\) \u5f0f\uff1a \\[ \\begin{align} df(\\pmb{x}) &=\\frac{\\partial f}{\\partial x_1}dx_1+\\frac{\\partial f}{\\partial x_2}dx_2 + \\cdots+\\frac{\\partial f}{\\partial x_n}dx_n\\\\\\\\ &= (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\cdots,\\frac{\\partial f}{\\partial x_n}) \\begin{bmatrix} dx_1 \\\\ dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix} \\end{align} \\\\\\ \\tag{17} \\] \u7ed3\u679c\u662f\u6807\u91cf, \u7531 \\((2)\\) \u5f0f\u53ef\u77e5, \\((17)\\) \u5f0f\u53ef\u5199\u6210\u8ff9\u7684\u5f62\u5f0f, \u5373: \\[ \\begin{align} df(\\pmb{x}) &= (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\cdots,\\frac{\\partial f}{\\partial x_n}) \\begin{bmatrix} dx_1 \\\\ dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix} \\\\\\\\ &=\\mathbb{tr}\\Big((\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\cdots,\\frac{\\partial f}{\\partial x_n}) \\begin{bmatrix} dx_1 \\\\ dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix}\\Big) \\end{align} \\\\\\ \\tag{18} \\] 2\u3001\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\[ f(X), X_{m \\times n}=(x_{ij})_{i=1,j=1}^{m,n} \\] \u5b83\u4e5f\u662f\u591a\u5143\u51fd\u6570\uff0c\u8bbe\u5176\u53ef\u5fae\uff0c\u5219\u5b83\u7684\u5168\u5fae\u5206\uff0c\u4ecd\u662f \\((14)\\) \u5f0f: \\[\\begin{align} df(\\pmb{X}) &=\\frac{\\partial f}{\\partial x_{11}}dx_{11}+\\frac{\\partial f}{\\partial x_{12}}dx_{12} + \\cdots+\\frac{\\partial f}{\\partial x_{1n}}dx_{1n}\\\\ &+\\frac{\\partial f}{\\partial x_{21}}dx_{21}+\\frac{\\partial f}{\\partial x_{22}}dx_{22} + \\cdots+\\frac{\\partial f}{\\partial x_{2n}}dx_{2n}\\\\ &+\\cdots\\\\ &+\\frac{\\partial f}{\\partial x_{m1}}dx_{m1}+\\frac{\\partial f}{\\partial x_{m2}}dx_{m2} + \\cdots+\\frac{\\partial f}{\\partial x_{mn}}dx_{mn} \\end{align} \\\\\\ \\tag{19}\\] \u6211\u4eec\u4ece\u8fd9\u4e2a\u7ed3\u679c\u4e2d\u53d1\u73b0\uff0c\u5b83\u5176\u5b9e\u5c31\u662f\u77e9\u9635 \\((\\frac{\\partial f}{\\partial x_{ij}})_{i=1,\\ j=1}^{m,\\ n}\\) \u4e0e\u77e9\u9635 \\((dx_{ij})_{i=1,j=1}^{m,n}\\) \u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0\uff0c\u7531 \\((6)\\) \u5f0f\u53ef\u77e5\uff0c \\((19)\\) \u5f0f\u4e5f\u53ef\u4ee5\u5199\u6210\u8ff9\u7684\u5f62\u5f0f\uff0c\u5373\uff1a \\[\\begin{align} df(\\pmb{X}) &=\\frac{\\partial f}{\\partial x_{11}}dx_{11}+\\frac{\\partial f}{\\partial x_{12}}dx_{12} + \\cdots+\\frac{\\partial f}{\\partial x_{1n}}dx_{1n}\\\\ &+\\frac{\\partial f}{\\partial x_{21}}dx_{21}+\\frac{\\partial f}{\\partial x_{22}}dx_{22} + \\cdots+\\frac{\\partial f}{\\partial x_{2n}}dx_{2n}\\\\ &+\\cdots\\\\ &+\\frac{\\partial f}{\\partial x_{m1}}dx_{m1}+\\frac{\\partial f}{\\partial x_{m2}}dx_{m2} + \\cdots+\\frac{\\partial f}{\\partial x_{mn}}dx_{mn} \\\\\\\\ &=tr( \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} \\end{bmatrix}_{n\\times m} \\begin{bmatrix} dx_{11} & dx_{12} & \\cdots & dx_{1n} \\\\ dx_{21} & dx_{22} & \\cdots & dx_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ dx_{m1} & dx_{m2} & \\cdots & dx_{mn} \\end{bmatrix}_{m \\times n} ) \\end{align} \\\\\\ \\tag{20}\\] 3\u3001\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570 \\[ F(X), F_{p \\times q}=(f_{ij})_{i=1, j=1}^{p,q}, X_{m \\times n}=(x_{ij})_{i=1,j=1}^{m,n} \\] \u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570\uff0c\u5b83\u7684\u6bcf\u4e2a\u5143\u7d20\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f_{i,j}(X)\\) \u3002 \u6211\u4eec\u5b9a\u4e49\uff1a\u8bbe \\(f_{i,j}(X)\\) \u53ef\u5fae, \u5219\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570\u7684\u77e9\u9635\u5fae\u5206\uff0c\u5c31\u662f\u5bf9\u5e94\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5143\u7d20 \\(f_{i,j}(X)\\) \u6c42\u5168\u5fae\u5206\uff0c\u6392\u5217\u5e03\u5c40 \u4e0d\u53d8 , \u5373\uff1a \\[ \\begin{align} d\\pmb{F}_{p \\times q}(\\pmb{X}) & = \\begin{bmatrix} df_{11}(\\pmb{X})& df_{12}(\\pmb{X}) & \\cdots & df_{1q}(\\pmb{X}) \\\\ df_{21}(\\pmb{X})& df_{22}(\\pmb{X}) & \\cdots & df_{2q}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ df_{p1}(\\pmb{X})& df_{p2}(\\pmb{X}) & \\cdots & df_{pq}(\\pmb{X}) \\end{bmatrix}_{p \\times q} \\end{align} \\\\\\ \\tag{21} \\] 3.1 \u56db\u4e2a\u6cd5\u5219 a. \u5e38\u6570\u77e9\u9635\u7684\u77e9\u9635\u5fae\u5206 \\[ dA_{m \\times n}=0_{m \\times n}\\\\\\ \\tag{22.1} \\] \u8bc1\u660e: \\(A\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u5e38\u6570, \u7531 \\((16.1)\\) \u5f97, \u6bcf\u4e2a\u5143\u7d20\u7684\u5fae\u5206\u90fd\u662f 0\u3002 \u8bc1\u6bd5\u3002 b. \u7ebf\u6027\u6cd5\u5219 \u76f8\u52a0\u518d\u5fae\u5206\u7b49\u4e8e\u5fae\u5206\u518d\u76f8\u52a0, \u5e38\u6570\u63d0\u5916\u9762 \\[ d \\big(c_1F(X)+c_2G(X)\\big)=c_1dF(x)+c_2dG(x)\\\\\\ \\tag{22.2} \\] \u5176\u4e2d, \\(c_1,c_2\\) \u662f\u5e38\u6570\u3002 \u8bc1\u660e: \\(c_1F(X)+c_2G(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f \\(c_1f_{ij}(X)+c_2g_{ij}(X)\\) , \u7531 \\((16.2)\\) \u5f0f\u53ef\u77e5, \u6bcf\u4e2a\u5143\u7d20\u7684\u5168\u5fae\u5206\u662f \\(c_1df_{ij}(X)+c_2dg_{ij}(X)\\) \u3002 \u8bc1\u6bd5\u3002 c. \u4e58\u79ef\u6cd5\u5219 \u524d\u5fae\u540e\u4e0d\u5fae + \u524d\u4e0d\u5fae\u540e\u5fae \\[ d \\big(F(X)G(X) \\big)=d\\big(F(X)\\big)G(X)+F(X)dG(X)\\\\\\ \\tag{22.3.1} \\] \u5176\u4e2d, \\(F_{p \\times q}(X),\\ G_{p \\times q}(X)\\) \u3002 \u6ce8\u610f\uff1a \u6b64\u65f6\u7684\u5fae\u5206\u662f\u77e9\u9635, \u4e0d\u80fd \u4ea4\u6362\u4e58\u673a\u7684\u5de6\u53f3\u987a\u5e8f\u3002 \u8bc1\u660e\uff1a \\(F(X)G(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f , \u7531 \\((16.2)\\) \u5f0f\u3001 \\((16.3)\\) \u5f0f\u53ef\u77e5, \u6bcf\u4e2a\u5143\u7d20\u7684\u5168\u5fae\u5206\u662f \\[ \\begin{align} d\\left( \\sum_{k=1}^q[f_{ik}(\\pmb{X})g_{kj}(\\pmb{X})] \\right) & =\\sum_{k=1}^q d(f_{ik}(\\pmb{X})g_{kj}(\\pmb{X})) \\\\\\\\ & = \\sum_{k=1}^q[d(f_{ik}(\\pmb{X}))g_{kj}(\\pmb{X})+f_{ik}(\\pmb{X})dg_{kj}(\\pmb{X})] \\\\\\\\ & = \\sum_{k=1}^q[d(f_{ik}(\\pmb{X}))g_{kj}(\\pmb{X})]+ \\sum_{k=1}^q[f_{ik}(\\pmb{X})dg_{kj}(\\pmb{X})] \\end{align} \\\\\\ \\tag{22.3.1a} \\] \u7ed3\u679c\u5de6\u8fb9\u7684\u6c42\u548c\u5f0f\uff0c\u5c31\u662f \\(d\\big(F(X)\\big)G(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u7ed3\u679c\u53f3\u8fb9\u7684\u6c42\u548c\u5f0f\uff0c\u5c31\u662f \\(F(X)dG(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u3002 \u8bc1\u6bd5\u3002 \u7531\u6b64\uff0c\u5f88\u5bb9\u6613\u5f97\u5230\u66f4\u591a\u4e2a\u4e58\u79ef\u7684\u6cd5\u5219\uff1a \\[ \\mathbb{d}(\\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}))=\\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}) + \\pmb{F}(\\pmb{X})\\mathbb{d}(\\pmb{G}(\\pmb{X}))\\pmb{H}(\\pmb{X})+ \\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\mathbb{d}\\pmb{H}(\\pmb{X}) \\\\\\\\ \\tag{22.3.2} \\] \u8bc1\u660e\uff1a \\[ \\begin{align} \\mathbb{d}(\\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X})) &= \\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X})+\\pmb{F}(\\pmb{X})\\mathbb{d}(\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X})) \\\\\\\\ &= \\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}) +\\pmb{F}(\\pmb{X})[\\mathbb{d}(\\pmb{G}(\\pmb{X}))\\pmb{H}(\\pmb{X}) + \\pmb{G}(\\pmb{X})\\mathbb{d}\\pmb{H}(\\pmb{X})] \\\\\\\\ &=\\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}) + \\pmb{F}(\\pmb{X})\\mathbb{d}(\\pmb{G}(\\pmb{X}))\\pmb{H}(\\pmb{X})+ \\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\mathbb{d}\\pmb{H}(\\pmb{X}) \\end{align} \\\\\\\\ \\tag{22.3.2a} \\] d. \u8f6c\u7f6e\u6cd5\u5219 \u8f6c\u7f6e\u7684\u77e9\u9635\u5fae\u5206\u7b49\u4e8e\u77e9\u9635\u5fae\u5206\u7684\u8f6c\u7f6e \\[ \\mathbb{d}\\pmb{F}_{p \\times q}^T(\\pmb{X})=(\\mathbb{d}\\pmb{F}_{p \\times q})^T\\\\\\ \\tag{22.4.1} \\] \u8bc1\u660e\uff1a \\[ \\begin{align} \\mathbb{d}\\pmb{F}^T_{p \\times q}(\\pmb{X}) &= \\mathbb{d} \\begin{bmatrix} f_{11}(\\pmb{X})& f_{21}(\\pmb{X}) & \\cdots & f_{p1}(\\pmb{X}) \\\\ f_{12}(\\pmb{X})& f_{22}(\\pmb{X}) & \\cdots & f_{p2}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ f_{1q}(\\pmb{X})&f_{2q}(\\pmb{X}) & \\cdots & f_{pq}(\\pmb{X}) \\end{bmatrix}_{q \\times p} \\\\\\\\ &= \\begin{bmatrix} \\mathbb{d}f_{11}(\\pmb{X})& \\mathbb{d}f_{21}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{p1}(\\pmb{X}) \\\\ \\mathbb{d}f_{12}(\\pmb{X})& \\mathbb{d}f_{22}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{p2}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}f_{1q}(\\pmb{X})&\\mathbb{d}f_{2q}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{pq}(\\pmb{X}) \\end{bmatrix}_{q \\times p} \\\\\\\\ &= \\begin{bmatrix} \\mathbb{d}f_{11}(\\pmb{X})& \\mathbb{d}f_{12}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{1q}(\\pmb{X}) \\\\ \\mathbb{d}f_{21}(\\pmb{X})& \\mathbb{d}f_{22}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{2q}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}f_{p1}(\\pmb{X})& \\mathbb{d}f_{p2}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{pq}(\\pmb{X}) \\end{bmatrix}_{p \\times q}^T \\\\\\\\ &= (\\mathbb{d}\\pmb{F}_{p \\times q}(\\pmb{X}))^T \\end{align} \\\\\\ \\tag{22.4.2} \\] \u8bc1\u6bd5\u3002 3.2 \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u77e9\u9635\u5fae\u5206\u6c42\u5bfc \\(\\pmb{X}_{m \\times n}\\) \u81ea\u5df1\u5c31\u662f\u77e9\u9635\u53d8\u5143 \\(\\pmb{X}_{m \\times n}\\) \u7684\u5b9e\u77e9\u9635\u51fd\u6570\uff0c\u4ed6\u7684\u6bcf\u4e2a\u5143\u7d20\u662f \\(x_{ij}\\) , \u6bcf\u4e2a\u5143\u7d20\u7684\u5168\u5fae\u5206\u662f \\(\\pmb{d}x_{ij}\\) \u3002 \u56e0\u6b64, \\(\\pmb{X}_{m \\times n}\\) \u7684\u77e9\u9635\u5fae\u5206\u662f: \\[ \\begin{align} \\mathbb{d}\\pmb{X}_{m \\times n} &= \\begin{bmatrix} \\mathbb{d}x_{11}& \\mathbb{d}x_{12} & \\cdots & \\mathbb{d}x_{1n} \\\\ \\mathbb{d}x_{21}& \\mathbb{d}x_{22} & \\cdots & \\mathbb{d}x_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}x_{m1}& \\mathbb{d}x_{m2} & \\cdots & \\mathbb{d}x_{mn} \\\\ \\end{bmatrix}_{m \\times n} \\end{align} \\\\\\ \\tag{23.1} \\] \u5411\u91cf \\(\\pmb{x}=[x_1,x_2,\\cdots,x_n]^T\\) \u7684\u77e9\u9635\u5fae\u5206\u662f: \\[ \\begin{align} \\mathbb{d}\\pmb{x} &= \\begin{bmatrix} \\mathbb{d}x_{1}\\\\ \\mathbb{d}x_{2}\\\\ \\vdots \\\\ \\mathbb{d}x_{n} \\\\ \\end{bmatrix}_{n \\times 1} \\end{align} \\\\\\ \\tag{23.2} \\] \u4e8e\u662f\uff0c\u6211\u4eec\u521a\u521a\u8bb2\u5230\u7684\u77e9\u9635\u5fae\u5206\u56db\u4e2a\u6cd5\u5219\uff0c\u5bf9\u4e8e \\(\\pmb{dX}_{m \\times n},\\pmb{dx}\\) \u4e5f\u662f\u9002\u7528\u7684\u3002 \u6211\u4eec\u73b0\u5728\u56de\u5230\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\uff0c\u5373 \\((20)\\) \u5f0f\uff1a \\[\\begin{align} df(\\pmb{X}) &=\\frac{\\partial f}{\\partial x_{11}}dx_{11}+\\frac{\\partial f}{\\partial x_{12}}dx_{12} + \\cdots+\\frac{\\partial f}{\\partial x_{1n}}dx_{1n}\\\\ &+\\frac{\\partial f}{\\partial x_{21}}dx_{21}+\\frac{\\partial f}{\\partial x_{22}}dx_{22} + \\cdots+\\frac{\\partial f}{\\partial x_{2n}}dx_{2n}\\\\ &+\\cdots\\\\ &+\\frac{\\partial f}{\\partial x_{m1}}dx_{m1}+\\frac{\\partial f}{\\partial x_{m2}}dx_{m2} + \\cdots+\\frac{\\partial f}{\\partial x_{mn}}dx_{mn} \\\\\\\\ &=tr( \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} \\end{bmatrix}_{n\\times m} \\begin{bmatrix} dx_{11} & dx_{12} & \\cdots & dx_{1n} \\\\ dx_{21} & dx_{22} & \\cdots & dx_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ dx_{m1} & dx_{m2} & \\cdots & dx_{mn} \\end{bmatrix}_{m \\times n} ) \\end{align} \\\\\\ \\tag{20}\\] \u89c2\u5bdf \\((20)\\) \u5f0f\u7684\u7ed3\u679c, \u53d1\u73b0\u5728 \\(tr\\) \u4e2d, \u5de6\u8fb9\u7684\u77e9\u9635\u5176\u5b9e\u5c31\u662f\u4e0a\u4e00\u7bc7 (\u672c\u8d28\u7bc7_9) \u5f0f: \\[ \\begin{align} \\text{D}_{\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T_{m\\times n}} \\\\\\\\ &= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} \\end{bmatrix}_{n\\times m} \\end{align} \\\\\\\\ \\tag{\u672c\u8d28\u7bc7_9} \\] \u800c\u53f3\u8fb9\u7684\u77e9\u9635\uff0c\u5176\u5b9e\u5c31\u662f \\((23.1)\\) \u5f0f: \\[ \\begin{align} \\mathbb{d}\\pmb{X}_{m \\times n} &= \\begin{bmatrix} \\mathbb{d}x_{11}& \\mathbb{d}x_{12} & \\cdots & \\mathbb{d}x_{1n} \\\\ \\mathbb{d}x_{21}& \\mathbb{d}x_{22} & \\cdots & \\mathbb{d}x_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}x_{m1}& \\mathbb{d}x_{m2} & \\cdots & \\mathbb{d}x_{mn} \\\\ \\end{bmatrix}_{m \\times n} \\end{align} \\\\\\ \\tag{23.1} \\] \u56e0\u6b64, \u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\uff0c\u5373 \\((20)\\) \u5f0f\uff0c\u53ef\u4ee5\u5199\u6210\uff1a \\[ \\pmb{d}f(\\pmb{X})= \\operatorname{tr}(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\pmb{dX})\\\\\\\\ \\tag{24} \\] \u522b\u5fd8\u4e86\u6211\u4eec\u7684\u76ee\u6807\u662f\u4ec0\u4e48\uff0c\u5176\u5b9e\u5c31\u662f\u8981\u6c42 \\(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\) \u3002\u6240\u4ee5\uff0c\u53ea\u8981\u6211\u4eec\u53ef\u4ee5\u628a\u4e00\u4e2a\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\u5199\u6210 \\((24)\\) \u5f0f\uff0c\u6211\u4eec\u5c31\u627e\u5230\u4e86\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c\u3002\uff08\u5df2\u7ecf\u6709\u4eba\u8bc1\u660e\uff0c\u8fd9\u6837\u7684\u7ed3\u679c\u662f\u552f\u4e00\u7684\u3002\u5373\u82e5 \\(\\pmb{d}f(\\pmb{X})=\\pmb{tr}(A_1\\pmb{dX})=\\pmb{tr}(A_2\\pmb{dX})\\) \uff0c\u5219 \\(A_1=A_2\\) ) \u5bf9\u4e8e\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\uff0c\u5373 \\((18)\\) \u5f0f\uff0c\u540c\u6837\u53ef\u4ee5\u5199\u6210\uff1a \\[ \\mathbb{d}f (\\pmb{x}) = \\operatorname{tr}(\\frac{\\partial f (\\pmb{x})}{\\partial \\pmb {x}^T} \\mathbb{d}\\pmb{x})\\\\\\\\ \\tag{25} \\] \u5f53\u77e9\u9635\u53d8\u5143 \\(X\\) \u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u5217\u5411\u91cf\u65f6 \\(\\pmb{x}\\) \u65f6 \\[ \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}= \\frac{\\partial f (\\pmb{x})}{\\partial \\pmb {x}^T} \\] \u540c\u65f6\uff0c\u7531 \\((23.1)\\) \u5f0f\u3001 \\((23.2)\\) \u5f0f\uff0c\u5f53\u77e9\u9635 $\\pmb X $ \u672c\u8eab\u662f\u5217\u5411\u91cf \u3001 \\(\\pmb x\\) \u65f6\uff0c\u4e5f\u6709 \\[ \\mathbb{d} \\pmb X = \\mathbb{d} \\pmb x \\\\\\\\ \\tag{27} \\] \u6240\u4ee5\uff0c\u77e9\u9635\u53d8\u5143\u6216\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c\uff0c\u90fd\u53ef\u4ee5\u901a\u8fc7 \\((24)\\) \u5f0f\u5f97\u5230\uff1a \\[ \\pmb{d}f(\\pmb{X})= \\operatorname{tr}(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\pmb{dX})\\\\\\\\ \\tag{24} \\] \u90a3\u4e48\uff0c\u6211\u4eec\u8be5\u5982\u4f55\u5199\u6210\u5f62\u5982 \\((24)\\) \u5f0f\u7684\u7ed3\u679c\u5462\uff0c\u522b\u6025\uff0c\u8ba9\u6211\u4eec\u5148\u7ed9\u51fa \\(3\\times 2=6\\) \u4e2a\u4f60\u5e94\u8be5\u8bb0\u4f4f\u7684\u516c\u5f0f\uff08\u4ee5\u540e\u5c31\u76f4\u63a5\u7528\u4e86\uff09\u3002 3.2.1 \u5939\u5c42\u997c \\[ \\mathbb{d} (\\pmb {AXB})=\\pmb A \\mathbb{d}(\\pmb X)\\pmb B \\\\\\\\ \\tag{25.1.1} \\] \u5176\u4e2d, \\(\\pmb {A}_{p \\times m},\\ \\pmb B_{n \\times q}\\) \u662f\u5e38\u6570\u77e9\u9635\u3002 \u8bc1\u660e: \u7531\u4e58\u79ef\u6cd5\u5219 \\((22.3.2)\\) \u5f0f\u5f97: \\[ \\mathbb{d}(\\pmb {AXB})=\\mathbb{d}(\\pmb A)\\pmb {XB}+\\pmb A \\mathbb{d}(\\pmb X)\\pmb B+\\pmb {AX}\\mathbb{d}\\pmb B\\\\\\\\ \\tag{25.1.a} \\] \u7531\u5e38\u6570\u77e9\u9635\u5fae\u5206 \\((22.1)\\) \u5f0f\u5f97: \\[ \\mathbb{d} \\pmb A = \\pmb 0_{p \\times m}, \\mathbb{d} \\pmb B =\\pmb 0_{n \\times q}\\\\\\\\ \\tag{25.1.b} \\] \u8bc1\u6bd5\u3002 \\(\\pmb X_{m\\times n}\\) \u53ef\u4ee5\u4ee3\u5165\u5176\u4ed6\u4efb\u610f\u7684\u77e9\u9635\u51fd\u6570: \\[ \\mathbb{d}\\big( \\pmb{AF}(\\pmb X)\\pmb B \\big)=\\pmb A \\mathbb{d}\\big( \\pmb F(\\pmb X) \\big)\\pmb B\\\\\\\\ \\tag{25.1.2} \\] 3.2.2 \u884c\u5217\u5f0f \\[ \\mathbb{d} |\\pmb X| = |\\pmb X|\\operatorname{tr}(\\pmb X^{-1}\\mathbb{d}\\pmb X) = \\operatorname{tr}(|\\pmb X|\\pmb X^{-1}\\mathbb{d}\\pmb X)\\\\\\\\ \\tag{25.2.1} \\] \u5176\u4e2d, \\(\\pmb X_{n \\times n}\\) \u3002 \u8bc1\u660e: \u9996\u5148\u660e\u786e, \u884c\u5217\u5f0f\u662f\u4e00\u4e2a\u5b9e\u503c\u6807\u91cf\u51fd\u6570, \u6545\u53ef\u4ee5\u4f7f\u7528 \\((24)\\) \u5f0f\u3002 \u6211\u4eec\u77e5\u9053, \u884c\u5217\u5f0f\u53ef\u4ee5\u6309\u7167\u4e00\u884c\u5c55\u5f00, \u5373\u4e00\u884c\u4e2d\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u4ed6\u7684 \u4ee3\u6570\u4f59\u5b50\u5f0f \u7136\u540e\u6c42\u548c\u3002 \u6211\u4eec\u6309\u7167\u5143\u7d20 \\(x_{ij}\\) \u6240\u5728\u7684\u7b2c \\(i\\) \u884c\u5c55\u5f00: \\[ |\\pmb X|=x_{i1}A_{i1}+x_{i2}A_{i2}+\\cdots +x_{in}A_{in}\\\\\\\\ \\tag{25.2.a} \\] \u56e0\u6b64\uff0c\u884c\u5217\u5f0f\u5bf9\u5143\u7d20 \\(x_{ij}\\) \u7684\u504f\u5bfc\uff0c\u5373\u4e3a\u8be5\u5143\u7d20\u5bf9\u5e94\u7684\u4ee3\u6570\u4f59\u5b50\u5f0f\u3002 \\[ \\frac{\\partial |\\pmb X|}{\\partial x_{ij}}=A_{ij}\\\\\\\\ \\tag{25.2.b} \\] \u56e0\u6b64, \u884c\u5217\u5f0f\u5bf9\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c\u4e3a: \\[ \\begin{align} \\frac{\\partial |\\pmb{X}|}{\\partial \\pmb{X}^T} &= \\begin{bmatrix} A_{11} & A_{21} & \\cdots & A_{n1} \\\\ A_{12} & A_{22} & \\cdots & A_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{1n} & A_{2n} & \\cdots & A_{nn} \\\\ \\end{bmatrix} \\end{align} \\\\\\\\ \\tag{25.2.c} \\] \u8fd9\u4e2a\u7ed3\u679c\u5176\u5b9e\u5c31\u662f\u4f34\u968f\u77e9\u9635 \\(\\pmb X^*\\) \u3002 \u53c8\u56e0\u4e3a\u4f34\u968f\u77e9\u9635\u548c\u9006\u77e9\u9635\u7684\u5173\u7cfb: \\[ \\pmb X^{-1} = \\frac{\\pmb X^*}{|\\pmb X|}\\\\\\\\ \\tag{25.2.d} \\] \u4ee3\u5165 \\((24)\\) : \\[ \\begin{aligned} \\mathbb{d} |\\pmb X| & = \\operatorname{tr}(\\frac{\\partial |\\pmb X|}{\\partial \\pmb X^T})\\\\\\\\ & = \\operatorname{tr}(|\\pmb X|\\pmb X^{-1}\\mathbb{d}\\pmb X) \\end{aligned} \\] \u53c8\u56e0\u4e3a\u884c\u5217\u5f0f\u662f\u6807\u91cf\uff0c\u7531 \\((3)\\) \u5f0f\uff0c\u53ef\u4ee5\u63d0\u5230\u8ff9\u7684\u5916\u9762\uff0c\u5f97: \\[ \\mathbb{d} |\\pmb X| = |\\pmb X| \\operatorname{tr}(\\pmb X^{-1}\\mathbb{d}\\pmb X) = \\operatorname{tr}(|\\pmb X|\\pmb X^{-1}\\mathbb{d}\\pmb X)\\\\\\\\ \\tag{25.2.1} \\] \u8bc1\u6bd5\u3002 \\(\\pmb X_{n\\times n}\\) \u53ef\u4ee5\u4ee3\u5165\u5176\u4ed6\u4efb\u610f\u7684\u77e9\u9635\u51fd\u6570: \\[ \\mathbb{d} |\\pmb F (\\pmb X)| = |\\pmb F (\\pmb X)| \\operatorname{tr}(\\pmb F (\\pmb X)^{-1}\\mathbb{d}\\pmb F (\\pmb X)) = \\operatorname{tr}(|\\pmb F (\\pmb X)|\\pmb F (\\pmb X)^{-1}\\mathbb{d}\\pmb F (\\pmb X))\\\\\\\\ \\tag{25.2.2} \\] 3.3 \u5982\u4f55\u4f7f\u7528\u77e9\u9635\u5fae\u5206\u6c42\u5bfc \u5bf9\u4e8e\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f (\\pmb{X})\\) , \\(\\operatorname{tr}\\big( f(\\pmb X) \\big)=f(\\pmb X)\\) , \\(\\mathbb{d}f(\\pmb X)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb X) \\big)\\) \u6240\u4ee5\u6709 \\(\\mathbb{d}f(\\pmb X)=\\mathbb{d}\\big(\\operatorname{tr}f(\\pmb X)\\big)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb X) \\big)\\)","title":"\u4e8c. \u77e9\u9635\u6c42\u5bfc"},{"location":"math%20in%20ML/2.%20M%20derivation/#_1","text":"","title":"\u4e00. \u77e9\u9635\u7684\u8ff9"},{"location":"math%20in%20ML/2.%20M%20derivation/#1","text":"\\(n \\times n\\) \u7684 \u65b9\u9635 \\(A_{n \\times n}\\) \u7684\u4e3b\u5bf9\u89d2\u7ebf\u5143\u7d20\u4e4b\u548c\u5c31\u53eb\u505a\u77e9\u9635 \\(A\\) \u7684\u8ff9(trace), \u8bb0\u4f5c \\(tr(A)\\) , \u5373: \\(\\pmb{A}_{n \\times n} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nn} \\\\ \\end{bmatrix}_{n \\times n}\\\\\\\\\\) \\(A\\) \u7684\u8ff9\u4e3a: \\[tr(\\pmb{A})=a_{11} + a_{22} + \\cdots + a_{nn} = \\sum_{i=1}^n{a_{ii}} \\tag{1}\\] \u6ce8\u610f\uff1a\u53ea\u6709 \u65b9\u9635 \u624d\u6709\u8ff9\u3002","title":"1\u3001\u5b9a\u4e49"},{"location":"math%20in%20ML/2.%20M%20derivation/#2","text":"","title":"2\u3001\u4e00\u4e9b\u6027\u8d28(\u5efa\u8bae\u719f\u8bb0)"},{"location":"math%20in%20ML/2.%20M%20derivation/#21","text":"\u5bf9\u4e8e\u4e00\u4e2a\u6807\u91cf \\(x\\) ,\u53ef\u4ee5\u770b\u6210\u662f \\(1 \\times 1\\) \u7684\u77e9\u9635\uff0c\u5b83\u7684\u8ff9\u5c31\u662f\u5b83\u81ea\u5df1\u3002 \\[x=tr(x)\\\\\\\\ \\tag{2}\\]","title":"2.1 \u6807\u91cf\u7684\u8ff9"},{"location":"math%20in%20ML/2.%20M%20derivation/#22","text":"\u76f8\u52a0\u518d\u6c42\u8ff9\u7b49\u4e8e\u6c42\u8ff9\u518d\u76f8\u52a0\uff0c\u6807\u91cf\u63d0\u5916\u9762 \\[tr(c_1A+c_2B)=c_1tr(A)+c_2tr(B)\\\\\\\\ \\tag{3}\\] \u5176\u4e2d, \\(c_1,c_2\\) \u4e3a\u6807\u91cf\u3002 \u8bc1\u660e\uff1a \\[ \\begin{align} tr(c_1\\pmb{A}+c_2\\pmb{B}) &= tr \\begin{bmatrix} c_1a_{11}+c_2b_{11} & c_1a_{12}+c_2b_{12} & \\cdots & c_1a_{1n}+c_2b_{1n} \\\\ c_1a_{21}+c_2b_{21} & c_1a_{22}+c_2b_{22} & \\cdots & c_1a_{2n}+c_2b_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_1a_{n1}+c_2b_{n1} & c_1a_{n2}+c_2b_{n2} & \\cdots & c_1a_{nn}+c_2b_{nn} \\\\ \\end{bmatrix} \\\\\\\\ &= (c_1a_{11}+c_2b_{11})+(c_1a_{22}+c_2b_{22})+\\cdots + (c_1a_{nn}+c_2b_{nn}) \\\\\\\\ &= c_1(a_{11}+a_{22}+\\cdots+a_{nn}) + c_2(b_{11}+b_{22}+\\cdots+b_{nn}) \\\\\\\\ &= c_1tr(\\pmb{A})+c_2tr(\\pmb{B}) \\end{align} \\\\\\\\ \\tag{4} \\] \u8bc1\u6bd5\u3002","title":"2.2 \u7ebf\u6027\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#23","text":"\u8f6c\u7f6e\u7684\u8ff9\u7b49\u4e8e\u539f\u77e9\u9635\u7684\u8ff9 \\[tr(A^T)=tr(A)\\\\\\\\ \\tag{5}\\] \u8bc1\u660e\uff1a \u56e0\u4e3a\u8f6c\u7f6e\u4e0d\u4f1a\u6539\u53d8\u4e3b\u5bf9\u89d2\u7ebf\u7684\u5143\u7d20\uff0c\u6545\u6210\u7acb\u3002 \u8bc1\u6bd5\u3002","title":"2.3 \u8f6c\u7f6e"},{"location":"math%20in%20ML/2.%20M%20derivation/#24","text":"\u5bf9\u4e8e\u4e24\u4e2a\u9636\u6570\u90fd\u662f \\(m \\times n\\) \u7684\u77e9\u9635 \\(A_{m \\times n}, B_{m \\times n}\\) \u5176\u4e2d\u4e00\u4e2a\u77e9\u9635\u4e58\u4ee5\uff08\u5de6\u4e58\u53f3\u4e58\u90fd\u53ef\u4ee5\uff09\u53e6\u4e00\u4e2a\u77e9\u9635\u7684\u8f6c\u7f6e\u7684\u8ff9\uff0c\u672c\u8d28\u662f \\(A_{m \\times n}, B_{m \\times n}\\) \u4e24\u4e2a\u77e9\u9635\u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u5411\u91cf\u7684\u70b9\u79ef\u5728\u77e9\u9635\u4e0a\u7684\u63a8\u5e7f\uff0c\u5373\uff1a \\[\\begin{align} tr(\\pmb{A}\\pmb{B}^T) &= a_{11}b_{11}+a_{12}b_{12}+\\cdots+a_{1n}b_{1n}\\\\ & +a_{21}b_{21}+a_{22}b_{22}+ \\cdots +a_{2n}b_{2n}\\\\ & + \\cdots \\\\ & + a_{m1}b_{m1} + a_{m2}b_{m2} + \\cdots a_{mn}b_{mn} \\end{align} \\\\\\\\ \\tag{6}\\] \u8bc1\u660e \\[ \\begin{align} tr(\\pmb{A}\\pmb{B}^T) &= tr( \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\begin{bmatrix} b_{11} & b_{21} & \\cdots & b_{m1} \\\\ b_{12} & b_{22} & \\cdots & b_{m2} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ b_{1n} & b_{2n} & \\cdots & b_{mn} \\\\ \\end{bmatrix} ) \\\\\\\\ &= tr \\begin{bmatrix} a_{11}b_{11} + a_{12}b_{12} + \\cdots + a_{1n}b_{1n} & \u4e0d\u7528\u7ba1 & \\cdots & \u4e0d\u7528\u7ba1 \\\\ \u4e0d\u7528\u7ba1 & a_{21}b_{21} + a_{22}b_{22} + \\cdots + a_{2n}b_{2n} & \\cdots & \u4e0d\u7528\u7ba1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \u4e0d\u7528\u7ba1 & \u4e0d\u7528\u7ba1 & \\cdots & a_{m1}b_{m1} + a_{m2}b_{m2} + \\cdots + a_{mn}b_{mn} \\\\ \\end{bmatrix}_{m \\times m} \\\\\\\\ &= a_{11}b_{11} + a_{12}b_{12} + \\cdots + a_{1n}b_{1n}\\\\ & + a_{21}b_{21} + a_{22}b_{22} + \\cdots + a_{2n}b_{2n}\\\\ & + \\cdots \\\\ & + a_{m1}b_{m1} + a_{m2}b_{m2} + \\cdots + a_{mn}b_{mn} \\end{align} \\\\\\\\ \\tag{7} \\] \u8bc1\u6bd5\u3002","title":"2.4 \u4e58\u79ef\u7684\u8ff9\u7684\u672c\u8d28"},{"location":"math%20in%20ML/2.%20M%20derivation/#25","text":"\u77e9\u9635\u4e58\u79ef\u4f4d\u7f6e\u4e92\u6362\uff0c\u8ff9\u4e0d\u53d8 \\[tr(AB)=tr(BA)\\\\\\\\ \\tag{8}\\] \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times m}\\) \u3002 \u8bc1\u660e\uff1a \u628a \\(B_{n \\times m}\\) \u770b\u505a\u662f \\((B^T)_{m \\times n}\\) \u7684\u8f6c\u7f6e\u3002\u7531\u4e58\u79ef\u7684\u8ff9\u7684\u672c\u8d28\uff0c\u5373 \\((6)\\) \u5f0f\u53ef\u77e5\uff0c\u65e0\u8bba\u4e58\u79ef\u600e\u4e48\u4ea4\u6362\u987a\u5e8f\uff0c \\(A_{m \\times n}\\) \u4e0e \\((B^T)_{m \\times n}\\) \u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0\uff0c\u6c38\u8fdc\u662f\u4e0d\u53d8\u7684\u3002 \u8bc1\u6bd5\u3002","title":"2.5 \u4ea4\u6362\u5f8b"},{"location":"math%20in%20ML/2.%20M%20derivation/#26","text":"\\[tr(ABC)=tr(CAB)=tr(BCA)\\\\\\\\ \\tag{9}\\] \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times p}, C_{p \\times m}\\) \u3002 \u8bc1\u660e\uff1a \u628a\u4e24\u4e2a\u77e9\u9635\u7684\u4e58\u79ef\u770b\u505a\u4e00\u4e2a\u77e9\u9635\uff0c\u548c\u53e6\u5916\u7684\u4e00\u4e2a\u77e9\u9635\u5e94\u7528\u4ea4\u6362\u5f8b\u5373\u53ef\u3002 \u8bc1\u6bd5\u3002","title":"2.6 \u66f4\u591a\u7684\u4ea4\u6362\u5f8b"},{"location":"math%20in%20ML/2.%20M%20derivation/#27","text":"\\[tr(AB^T)=tr(B^TA)=tr(A^TB)=tr(BA^T)\\\\\\\\ \\tag{10}\\] \u5176\u4e2d, \\(A_{m \\times n}, B_{n \\times m}\\) \u3002 \u8bc1\u660e\uff1a \u7b2c\u4e00\u4e2a\u548c\u7b2c\u4e8c\u4e2a\u662f\u4ea4\u6362\u5f8b\uff0c\u7b2c\u4e8c\u4e2a\u548c\u4e09\u4e2a\u662f\u8f6c\u7f6e\uff0c\u7b2c\u4e09\u4e2a\u548c\u7b2c\u56db\u4e2a\u662f\u4ea4\u6362\u5f8b\u3002 \u8bc1\u6bd5\u3002","title":"2.7 \u719f\u7ec3\u4f7f\u7528"},{"location":"math%20in%20ML/2.%20M%20derivation/#_2","text":"\u6211\u4eec\u5148\u6765\u590d\u4e60\u4e00\u4e0b\u672c\u79d1\u9636\u6bb5\u6240\u5b66\u7684\u9ad8\u7b49\u6570\u5b66\u4e2d\u7684\u5fae\u5206\u4e0e\u5168\u5fae\u5206\u3002","title":"\u4e8c. \u5fae\u5206\u4e0e\u5168\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#1_1","text":"","title":"1\u3001\u4e00\u5143\u51fd\u6570\u7684\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#11","text":"\u8bbe \\(y=f(x)\\) , \\(y\\) \u53ef\u5bfc, \u5219\u5176\u5fae\u5206\u4e3a\uff1a \\[\\mathbb{d}y=\\mathbb{d}f(x)=f'(x)\\mathbb{d}x\\\\\\\\ \\tag{11}\\]","title":"1.1 \u666e\u901a\u51fd\u6570\u7684\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#12","text":"\u8bbe \\(y=f(u), u=g(x)\\) , \u5747\u53ef\u5bfc, \u5219 \\(y\\) \u5fae\u5206\u4e3a\uff1a \\[\\mathbb{d}y=\\mathbb{d}f(u)=f'(u)\\mathbb{d}g(x)=f'(u)g'(x)\\mathbb{d}x\\\\\\\\ \\tag{12}\\] \u4e4d\u4e00\u770b\u5f88\u590d\u6742\uff0c\u5176\u5b9e\u4e3e\u4e2a\u4f8b\u5b50\u5c31\u5f88\u7b80\u5355\uff1a \u8bbe \\(y=sin(2x+1), u=2x+1\\) , \u5219 \\(y\\) \u7684\u5fae\u5206\u4e3a\uff1a \\[\\begin{align} \\mathbb{d}y&=\\mathbb{d}(\\sin{u})=\\cos{u}\\mathbb{d}u=\\cos(2x+1)\\mathbb{d}(2x+1) \\\\\\\\ &=\\cos(2x+1) \\cdot 2 \\mathbb{d}x=2\\cos(2x+1) \\mathbb{d}x \\end{align} \\\\\\\\ \\tag{13}\\]","title":"1.2 \u590d\u5408\u51fd\u6570\u7684\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#2_1","text":"","title":"2\u3001\u591a\u5143\u51fd\u6570\u7684\u5168\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#21_1","text":"\u8bbe \\(z=f(x,y), z\\) \u53ef\u5fae, \u5219\u5176\u5168\u5fae\u5206\u4e3a\uff1a \\[\\mathbb{d}z=\\frac{\\partial z}{\\partial x}\\mathbb{d}x+\\frac{\\partial z}{\\partial y}\\mathbb{d}y\\\\\\\\ \\tag{14}\\]","title":"2.1 \u666e\u901a\u51fd\u6570\u7684\u5168\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#22_1","text":"\u8bbe \\(z=f(u), u=\\varphi(x,y), z\\) \u53ef\u5bfc, \\(u\\) \u53ef\u5fae, \u5219\u5176\u5168\u5fae\u5206\u4e3a\uff1a \\[ \\begin{align} dz & = df(u)-f'(u)du=f'(u)(\\frac{\\partial u}{\\partial x}dx+\\frac{\\partial u}{\\partial y}dy)\\\\ &=f'(u)\\frac{\\partial u}{\\partial x}dx+f'(u)\\frac{\\partial u}{\\partial y}dy \\end{align}\\\\\\ \\tag{15} \\] \u4e3e\u4e2a\u4f8b\u5b50\uff1a \u8bbe \\(z=sin(2x+y^2), u=2x+y^2\\) , \u5219 \\(z\\) \u7684\u5168\u5fae\u5206\u4e3a\uff1a \\[ \\begin{align} dz & = d(sin(u)) = cosudu=cos(2x+y^2)d(2x+y^2)\\\\ & = cos(2x+y^2)(2dx+2ydy)\\\\ & = 2cos(2x+y^2)dx+2ycos(2x+y^2)dy\\\\\\ \\end{align} \\]","title":"2.2 \u590d\u5408\u51fd\u6570\u7684\u5168\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#3","text":"","title":"3\u3001\u5fae\u5206/\u5168\u5fae\u5206\u7684\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#31","text":"\\[dc=0\\\\\\\\ \\tag{16.1}\\] \u5176\u4e2d, \\(c\\) \u4e3a\u5e38\u6570\u3002","title":"3.1 \u5e38\u6570\u7684\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#32","text":"\u76f8\u52a0\u518d\u5fae\u5206\u7b49\u4e8e\u5fae\u5206\u518d\u76f8\u52a0\uff0c\u5e38\u6570\u63d0\u5916\u9762 \\[d(c_1u+c_2v)=c_1du+c_2dv\\\\\\\\ \\tag{16.2}\\] \u5176\u4e2d, \u4e00\u5143\u51fd\u6570 \\(u=u(x), v=v(x)\\) \u6216\u591a\u5143\u51fd\u6570 \\(u=u(x,y, v=v(x,y), c_1,c_2\\) \u4e3a\u5e38\u6570\u3002","title":"3.2 \u7ebf\u6027\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#33","text":"\u524d\u5fae\u540e\u4e0d\u5fae + \u524d\u4e0d\u5fae\u540e\u5fae \\[d(uv)=d(u)v+ud(v)\\\\ \\tag{16.3}\\]","title":"3.3 \u4e58\u6cd5\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#34","text":"(\u4e0a\u5fae\u4e0b\u4e0d\u5fae \u51cf \u4e0a\u4e0d\u5fae\u4e0b\u5fae\uff09 \u9664\u4ee5 \uff08\u4e0b\u7684\u5e73\u65b9\uff09 \\[d(\\frac{u}{v})=\\frac{1}{v^2}(d(u)v-ud(v))\\] \u5176\u4e2d, \u4e00\u5143\u51fd\u6570 \\(v=v(x)\\neq0, u=u(x)\\) \u6216\u591a\u5143\u51fd\u6570 \\(v=v(x,y)\\neq0, u=u(x,y)\\) \u3002","title":"3.4 \u5546\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#_3","text":"","title":"\u4e09. \u77e9\u9635\u7684\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#1_2","text":"\\[f(\\pmb{x}),\\pmb{x}=[x_1,x_2,\\cdots,x_n]^T\\\\\\\\\\] \u4ed6\u5176\u5b9e\u5c31\u662f\u591a\u5143\u51fd\u6570, \u8bbe\u5176\u53ef\u5fae, \u5219\u4ed6\u7684\u5168\u5fae\u5206, \u5373 \\((14)\\) \u5f0f\uff1a \\[ \\begin{align} df(\\pmb{x}) &=\\frac{\\partial f}{\\partial x_1}dx_1+\\frac{\\partial f}{\\partial x_2}dx_2 + \\cdots+\\frac{\\partial f}{\\partial x_n}dx_n\\\\\\\\ &= (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\cdots,\\frac{\\partial f}{\\partial x_n}) \\begin{bmatrix} dx_1 \\\\ dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix} \\end{align} \\\\\\ \\tag{17} \\] \u7ed3\u679c\u662f\u6807\u91cf, \u7531 \\((2)\\) \u5f0f\u53ef\u77e5, \\((17)\\) \u5f0f\u53ef\u5199\u6210\u8ff9\u7684\u5f62\u5f0f, \u5373: \\[ \\begin{align} df(\\pmb{x}) &= (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\cdots,\\frac{\\partial f}{\\partial x_n}) \\begin{bmatrix} dx_1 \\\\ dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix} \\\\\\\\ &=\\mathbb{tr}\\Big((\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\cdots,\\frac{\\partial f}{\\partial x_n}) \\begin{bmatrix} dx_1 \\\\ dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix}\\Big) \\end{align} \\\\\\ \\tag{18} \\]","title":"1\u3001\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570"},{"location":"math%20in%20ML/2.%20M%20derivation/#2_2","text":"\\[ f(X), X_{m \\times n}=(x_{ij})_{i=1,j=1}^{m,n} \\] \u5b83\u4e5f\u662f\u591a\u5143\u51fd\u6570\uff0c\u8bbe\u5176\u53ef\u5fae\uff0c\u5219\u5b83\u7684\u5168\u5fae\u5206\uff0c\u4ecd\u662f \\((14)\\) \u5f0f: \\[\\begin{align} df(\\pmb{X}) &=\\frac{\\partial f}{\\partial x_{11}}dx_{11}+\\frac{\\partial f}{\\partial x_{12}}dx_{12} + \\cdots+\\frac{\\partial f}{\\partial x_{1n}}dx_{1n}\\\\ &+\\frac{\\partial f}{\\partial x_{21}}dx_{21}+\\frac{\\partial f}{\\partial x_{22}}dx_{22} + \\cdots+\\frac{\\partial f}{\\partial x_{2n}}dx_{2n}\\\\ &+\\cdots\\\\ &+\\frac{\\partial f}{\\partial x_{m1}}dx_{m1}+\\frac{\\partial f}{\\partial x_{m2}}dx_{m2} + \\cdots+\\frac{\\partial f}{\\partial x_{mn}}dx_{mn} \\end{align} \\\\\\ \\tag{19}\\] \u6211\u4eec\u4ece\u8fd9\u4e2a\u7ed3\u679c\u4e2d\u53d1\u73b0\uff0c\u5b83\u5176\u5b9e\u5c31\u662f\u77e9\u9635 \\((\\frac{\\partial f}{\\partial x_{ij}})_{i=1,\\ j=1}^{m,\\ n}\\) \u4e0e\u77e9\u9635 \\((dx_{ij})_{i=1,j=1}^{m,n}\\) \u5bf9\u5e94\u4f4d\u7f6e\u7684\u5143\u7d20\u76f8\u4e58\u5e76\u76f8\u52a0\uff0c\u7531 \\((6)\\) \u5f0f\u53ef\u77e5\uff0c \\((19)\\) \u5f0f\u4e5f\u53ef\u4ee5\u5199\u6210\u8ff9\u7684\u5f62\u5f0f\uff0c\u5373\uff1a \\[\\begin{align} df(\\pmb{X}) &=\\frac{\\partial f}{\\partial x_{11}}dx_{11}+\\frac{\\partial f}{\\partial x_{12}}dx_{12} + \\cdots+\\frac{\\partial f}{\\partial x_{1n}}dx_{1n}\\\\ &+\\frac{\\partial f}{\\partial x_{21}}dx_{21}+\\frac{\\partial f}{\\partial x_{22}}dx_{22} + \\cdots+\\frac{\\partial f}{\\partial x_{2n}}dx_{2n}\\\\ &+\\cdots\\\\ &+\\frac{\\partial f}{\\partial x_{m1}}dx_{m1}+\\frac{\\partial f}{\\partial x_{m2}}dx_{m2} + \\cdots+\\frac{\\partial f}{\\partial x_{mn}}dx_{mn} \\\\\\\\ &=tr( \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} \\end{bmatrix}_{n\\times m} \\begin{bmatrix} dx_{11} & dx_{12} & \\cdots & dx_{1n} \\\\ dx_{21} & dx_{22} & \\cdots & dx_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ dx_{m1} & dx_{m2} & \\cdots & dx_{mn} \\end{bmatrix}_{m \\times n} ) \\end{align} \\\\\\ \\tag{20}\\]","title":"2\u3001\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570"},{"location":"math%20in%20ML/2.%20M%20derivation/#3_1","text":"\\[ F(X), F_{p \\times q}=(f_{ij})_{i=1, j=1}^{p,q}, X_{m \\times n}=(x_{ij})_{i=1,j=1}^{m,n} \\] \u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570\uff0c\u5b83\u7684\u6bcf\u4e2a\u5143\u7d20\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f_{i,j}(X)\\) \u3002 \u6211\u4eec\u5b9a\u4e49\uff1a\u8bbe \\(f_{i,j}(X)\\) \u53ef\u5fae, \u5219\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570\u7684\u77e9\u9635\u5fae\u5206\uff0c\u5c31\u662f\u5bf9\u5e94\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5143\u7d20 \\(f_{i,j}(X)\\) \u6c42\u5168\u5fae\u5206\uff0c\u6392\u5217\u5e03\u5c40 \u4e0d\u53d8 , \u5373\uff1a \\[ \\begin{align} d\\pmb{F}_{p \\times q}(\\pmb{X}) & = \\begin{bmatrix} df_{11}(\\pmb{X})& df_{12}(\\pmb{X}) & \\cdots & df_{1q}(\\pmb{X}) \\\\ df_{21}(\\pmb{X})& df_{22}(\\pmb{X}) & \\cdots & df_{2q}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ df_{p1}(\\pmb{X})& df_{p2}(\\pmb{X}) & \\cdots & df_{pq}(\\pmb{X}) \\end{bmatrix}_{p \\times q} \\end{align} \\\\\\ \\tag{21} \\]","title":"3\u3001\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u77e9\u9635\u51fd\u6570"},{"location":"math%20in%20ML/2.%20M%20derivation/#31_1","text":"","title":"3.1 \u56db\u4e2a\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#a","text":"\\[ dA_{m \\times n}=0_{m \\times n}\\\\\\ \\tag{22.1} \\] \u8bc1\u660e: \\(A\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u5e38\u6570, \u7531 \\((16.1)\\) \u5f97, \u6bcf\u4e2a\u5143\u7d20\u7684\u5fae\u5206\u90fd\u662f 0\u3002 \u8bc1\u6bd5\u3002","title":"a. \u5e38\u6570\u77e9\u9635\u7684\u77e9\u9635\u5fae\u5206"},{"location":"math%20in%20ML/2.%20M%20derivation/#b","text":"\u76f8\u52a0\u518d\u5fae\u5206\u7b49\u4e8e\u5fae\u5206\u518d\u76f8\u52a0, \u5e38\u6570\u63d0\u5916\u9762 \\[ d \\big(c_1F(X)+c_2G(X)\\big)=c_1dF(x)+c_2dG(x)\\\\\\ \\tag{22.2} \\] \u5176\u4e2d, \\(c_1,c_2\\) \u662f\u5e38\u6570\u3002 \u8bc1\u660e: \\(c_1F(X)+c_2G(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f \\(c_1f_{ij}(X)+c_2g_{ij}(X)\\) , \u7531 \\((16.2)\\) \u5f0f\u53ef\u77e5, \u6bcf\u4e2a\u5143\u7d20\u7684\u5168\u5fae\u5206\u662f \\(c_1df_{ij}(X)+c_2dg_{ij}(X)\\) \u3002 \u8bc1\u6bd5\u3002","title":"b. \u7ebf\u6027\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#c","text":"\u524d\u5fae\u540e\u4e0d\u5fae + \u524d\u4e0d\u5fae\u540e\u5fae \\[ d \\big(F(X)G(X) \\big)=d\\big(F(X)\\big)G(X)+F(X)dG(X)\\\\\\ \\tag{22.3.1} \\] \u5176\u4e2d, \\(F_{p \\times q}(X),\\ G_{p \\times q}(X)\\) \u3002 \u6ce8\u610f\uff1a \u6b64\u65f6\u7684\u5fae\u5206\u662f\u77e9\u9635, \u4e0d\u80fd \u4ea4\u6362\u4e58\u673a\u7684\u5de6\u53f3\u987a\u5e8f\u3002 \u8bc1\u660e\uff1a \\(F(X)G(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f , \u7531 \\((16.2)\\) \u5f0f\u3001 \\((16.3)\\) \u5f0f\u53ef\u77e5, \u6bcf\u4e2a\u5143\u7d20\u7684\u5168\u5fae\u5206\u662f \\[ \\begin{align} d\\left( \\sum_{k=1}^q[f_{ik}(\\pmb{X})g_{kj}(\\pmb{X})] \\right) & =\\sum_{k=1}^q d(f_{ik}(\\pmb{X})g_{kj}(\\pmb{X})) \\\\\\\\ & = \\sum_{k=1}^q[d(f_{ik}(\\pmb{X}))g_{kj}(\\pmb{X})+f_{ik}(\\pmb{X})dg_{kj}(\\pmb{X})] \\\\\\\\ & = \\sum_{k=1}^q[d(f_{ik}(\\pmb{X}))g_{kj}(\\pmb{X})]+ \\sum_{k=1}^q[f_{ik}(\\pmb{X})dg_{kj}(\\pmb{X})] \\end{align} \\\\\\ \\tag{22.3.1a} \\] \u7ed3\u679c\u5de6\u8fb9\u7684\u6c42\u548c\u5f0f\uff0c\u5c31\u662f \\(d\\big(F(X)\\big)G(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u7ed3\u679c\u53f3\u8fb9\u7684\u6c42\u548c\u5f0f\uff0c\u5c31\u662f \\(F(X)dG(X)\\) \u7684\u6bcf\u4e2a\u5143\u7d20\u3002 \u8bc1\u6bd5\u3002 \u7531\u6b64\uff0c\u5f88\u5bb9\u6613\u5f97\u5230\u66f4\u591a\u4e2a\u4e58\u79ef\u7684\u6cd5\u5219\uff1a \\[ \\mathbb{d}(\\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}))=\\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}) + \\pmb{F}(\\pmb{X})\\mathbb{d}(\\pmb{G}(\\pmb{X}))\\pmb{H}(\\pmb{X})+ \\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\mathbb{d}\\pmb{H}(\\pmb{X}) \\\\\\\\ \\tag{22.3.2} \\] \u8bc1\u660e\uff1a \\[ \\begin{align} \\mathbb{d}(\\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X})) &= \\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X})+\\pmb{F}(\\pmb{X})\\mathbb{d}(\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X})) \\\\\\\\ &= \\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}) +\\pmb{F}(\\pmb{X})[\\mathbb{d}(\\pmb{G}(\\pmb{X}))\\pmb{H}(\\pmb{X}) + \\pmb{G}(\\pmb{X})\\mathbb{d}\\pmb{H}(\\pmb{X})] \\\\\\\\ &=\\mathbb{d}(\\pmb{F}(\\pmb{X}))\\pmb{G}(\\pmb{X})\\pmb{H}(\\pmb{X}) + \\pmb{F}(\\pmb{X})\\mathbb{d}(\\pmb{G}(\\pmb{X}))\\pmb{H}(\\pmb{X})+ \\pmb{F}(\\pmb{X})\\pmb{G}(\\pmb{X})\\mathbb{d}\\pmb{H}(\\pmb{X}) \\end{align} \\\\\\\\ \\tag{22.3.2a} \\]","title":"c. \u4e58\u79ef\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#d","text":"\u8f6c\u7f6e\u7684\u77e9\u9635\u5fae\u5206\u7b49\u4e8e\u77e9\u9635\u5fae\u5206\u7684\u8f6c\u7f6e \\[ \\mathbb{d}\\pmb{F}_{p \\times q}^T(\\pmb{X})=(\\mathbb{d}\\pmb{F}_{p \\times q})^T\\\\\\ \\tag{22.4.1} \\] \u8bc1\u660e\uff1a \\[ \\begin{align} \\mathbb{d}\\pmb{F}^T_{p \\times q}(\\pmb{X}) &= \\mathbb{d} \\begin{bmatrix} f_{11}(\\pmb{X})& f_{21}(\\pmb{X}) & \\cdots & f_{p1}(\\pmb{X}) \\\\ f_{12}(\\pmb{X})& f_{22}(\\pmb{X}) & \\cdots & f_{p2}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ f_{1q}(\\pmb{X})&f_{2q}(\\pmb{X}) & \\cdots & f_{pq}(\\pmb{X}) \\end{bmatrix}_{q \\times p} \\\\\\\\ &= \\begin{bmatrix} \\mathbb{d}f_{11}(\\pmb{X})& \\mathbb{d}f_{21}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{p1}(\\pmb{X}) \\\\ \\mathbb{d}f_{12}(\\pmb{X})& \\mathbb{d}f_{22}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{p2}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}f_{1q}(\\pmb{X})&\\mathbb{d}f_{2q}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{pq}(\\pmb{X}) \\end{bmatrix}_{q \\times p} \\\\\\\\ &= \\begin{bmatrix} \\mathbb{d}f_{11}(\\pmb{X})& \\mathbb{d}f_{12}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{1q}(\\pmb{X}) \\\\ \\mathbb{d}f_{21}(\\pmb{X})& \\mathbb{d}f_{22}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{2q}(\\pmb{X}) \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}f_{p1}(\\pmb{X})& \\mathbb{d}f_{p2}(\\pmb{X}) & \\cdots & \\mathbb{d}f_{pq}(\\pmb{X}) \\end{bmatrix}_{p \\times q}^T \\\\\\\\ &= (\\mathbb{d}\\pmb{F}_{p \\times q}(\\pmb{X}))^T \\end{align} \\\\\\ \\tag{22.4.2} \\] \u8bc1\u6bd5\u3002","title":"d. \u8f6c\u7f6e\u6cd5\u5219"},{"location":"math%20in%20ML/2.%20M%20derivation/#32_1","text":"\\(\\pmb{X}_{m \\times n}\\) \u81ea\u5df1\u5c31\u662f\u77e9\u9635\u53d8\u5143 \\(\\pmb{X}_{m \\times n}\\) \u7684\u5b9e\u77e9\u9635\u51fd\u6570\uff0c\u4ed6\u7684\u6bcf\u4e2a\u5143\u7d20\u662f \\(x_{ij}\\) , \u6bcf\u4e2a\u5143\u7d20\u7684\u5168\u5fae\u5206\u662f \\(\\pmb{d}x_{ij}\\) \u3002 \u56e0\u6b64, \\(\\pmb{X}_{m \\times n}\\) \u7684\u77e9\u9635\u5fae\u5206\u662f: \\[ \\begin{align} \\mathbb{d}\\pmb{X}_{m \\times n} &= \\begin{bmatrix} \\mathbb{d}x_{11}& \\mathbb{d}x_{12} & \\cdots & \\mathbb{d}x_{1n} \\\\ \\mathbb{d}x_{21}& \\mathbb{d}x_{22} & \\cdots & \\mathbb{d}x_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}x_{m1}& \\mathbb{d}x_{m2} & \\cdots & \\mathbb{d}x_{mn} \\\\ \\end{bmatrix}_{m \\times n} \\end{align} \\\\\\ \\tag{23.1} \\] \u5411\u91cf \\(\\pmb{x}=[x_1,x_2,\\cdots,x_n]^T\\) \u7684\u77e9\u9635\u5fae\u5206\u662f: \\[ \\begin{align} \\mathbb{d}\\pmb{x} &= \\begin{bmatrix} \\mathbb{d}x_{1}\\\\ \\mathbb{d}x_{2}\\\\ \\vdots \\\\ \\mathbb{d}x_{n} \\\\ \\end{bmatrix}_{n \\times 1} \\end{align} \\\\\\ \\tag{23.2} \\] \u4e8e\u662f\uff0c\u6211\u4eec\u521a\u521a\u8bb2\u5230\u7684\u77e9\u9635\u5fae\u5206\u56db\u4e2a\u6cd5\u5219\uff0c\u5bf9\u4e8e \\(\\pmb{dX}_{m \\times n},\\pmb{dx}\\) \u4e5f\u662f\u9002\u7528\u7684\u3002 \u6211\u4eec\u73b0\u5728\u56de\u5230\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\uff0c\u5373 \\((20)\\) \u5f0f\uff1a \\[\\begin{align} df(\\pmb{X}) &=\\frac{\\partial f}{\\partial x_{11}}dx_{11}+\\frac{\\partial f}{\\partial x_{12}}dx_{12} + \\cdots+\\frac{\\partial f}{\\partial x_{1n}}dx_{1n}\\\\ &+\\frac{\\partial f}{\\partial x_{21}}dx_{21}+\\frac{\\partial f}{\\partial x_{22}}dx_{22} + \\cdots+\\frac{\\partial f}{\\partial x_{2n}}dx_{2n}\\\\ &+\\cdots\\\\ &+\\frac{\\partial f}{\\partial x_{m1}}dx_{m1}+\\frac{\\partial f}{\\partial x_{m2}}dx_{m2} + \\cdots+\\frac{\\partial f}{\\partial x_{mn}}dx_{mn} \\\\\\\\ &=tr( \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} \\end{bmatrix}_{n\\times m} \\begin{bmatrix} dx_{11} & dx_{12} & \\cdots & dx_{1n} \\\\ dx_{21} & dx_{22} & \\cdots & dx_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ dx_{m1} & dx_{m2} & \\cdots & dx_{mn} \\end{bmatrix}_{m \\times n} ) \\end{align} \\\\\\ \\tag{20}\\] \u89c2\u5bdf \\((20)\\) \u5f0f\u7684\u7ed3\u679c, \u53d1\u73b0\u5728 \\(tr\\) \u4e2d, \u5de6\u8fb9\u7684\u77e9\u9635\u5176\u5b9e\u5c31\u662f\u4e0a\u4e00\u7bc7 (\u672c\u8d28\u7bc7_9) \u5f0f: \\[ \\begin{align} \\text{D}_{\\pmb{X}}f(\\pmb{X})&= \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T_{m\\times n}} \\\\\\\\ &= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}}&\\frac{\\partial f}{\\partial x_{21}}&\\cdots&\\frac{\\partial f}{\\partial x_{m1}} \\\\ \\frac{\\partial f}{\\partial x_{12}}&\\frac{\\partial f}{\\partial x_{22}}& \\cdots & \\frac{\\partial f}{\\partial x_{m2}}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ \\frac{\\partial f} {\\partial x_{1n}}&\\frac{\\partial f}{\\partial x_{2n}}&\\cdots&\\frac{\\partial f}{\\partial x_{mn}} \\end{bmatrix}_{n\\times m} \\end{align} \\\\\\\\ \\tag{\u672c\u8d28\u7bc7_9} \\] \u800c\u53f3\u8fb9\u7684\u77e9\u9635\uff0c\u5176\u5b9e\u5c31\u662f \\((23.1)\\) \u5f0f: \\[ \\begin{align} \\mathbb{d}\\pmb{X}_{m \\times n} &= \\begin{bmatrix} \\mathbb{d}x_{11}& \\mathbb{d}x_{12} & \\cdots & \\mathbb{d}x_{1n} \\\\ \\mathbb{d}x_{21}& \\mathbb{d}x_{22} & \\cdots & \\mathbb{d}x_{2n} \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\\\ \\mathbb{d}x_{m1}& \\mathbb{d}x_{m2} & \\cdots & \\mathbb{d}x_{mn} \\\\ \\end{bmatrix}_{m \\times n} \\end{align} \\\\\\ \\tag{23.1} \\] \u56e0\u6b64, \u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\uff0c\u5373 \\((20)\\) \u5f0f\uff0c\u53ef\u4ee5\u5199\u6210\uff1a \\[ \\pmb{d}f(\\pmb{X})= \\operatorname{tr}(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\pmb{dX})\\\\\\\\ \\tag{24} \\] \u522b\u5fd8\u4e86\u6211\u4eec\u7684\u76ee\u6807\u662f\u4ec0\u4e48\uff0c\u5176\u5b9e\u5c31\u662f\u8981\u6c42 \\(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\) \u3002\u6240\u4ee5\uff0c\u53ea\u8981\u6211\u4eec\u53ef\u4ee5\u628a\u4e00\u4e2a\u77e9\u9635\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\u5199\u6210 \\((24)\\) \u5f0f\uff0c\u6211\u4eec\u5c31\u627e\u5230\u4e86\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c\u3002\uff08\u5df2\u7ecf\u6709\u4eba\u8bc1\u660e\uff0c\u8fd9\u6837\u7684\u7ed3\u679c\u662f\u552f\u4e00\u7684\u3002\u5373\u82e5 \\(\\pmb{d}f(\\pmb{X})=\\pmb{tr}(A_1\\pmb{dX})=\\pmb{tr}(A_2\\pmb{dX})\\) \uff0c\u5219 \\(A_1=A_2\\) ) \u5bf9\u4e8e\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u5168\u5fae\u5206\uff0c\u5373 \\((18)\\) \u5f0f\uff0c\u540c\u6837\u53ef\u4ee5\u5199\u6210\uff1a \\[ \\mathbb{d}f (\\pmb{x}) = \\operatorname{tr}(\\frac{\\partial f (\\pmb{x})}{\\partial \\pmb {x}^T} \\mathbb{d}\\pmb{x})\\\\\\\\ \\tag{25} \\] \u5f53\u77e9\u9635\u53d8\u5143 \\(X\\) \u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u5217\u5411\u91cf\u65f6 \\(\\pmb{x}\\) \u65f6 \\[ \\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}= \\frac{\\partial f (\\pmb{x})}{\\partial \\pmb {x}^T} \\] \u540c\u65f6\uff0c\u7531 \\((23.1)\\) \u5f0f\u3001 \\((23.2)\\) \u5f0f\uff0c\u5f53\u77e9\u9635 $\\pmb X $ \u672c\u8eab\u662f\u5217\u5411\u91cf \u3001 \\(\\pmb x\\) \u65f6\uff0c\u4e5f\u6709 \\[ \\mathbb{d} \\pmb X = \\mathbb{d} \\pmb x \\\\\\\\ \\tag{27} \\] \u6240\u4ee5\uff0c\u77e9\u9635\u53d8\u5143\u6216\u5411\u91cf\u53d8\u5143\u7684\u5b9e\u503c\u6807\u91cf\u51fd\u6570\u7684\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c\uff0c\u90fd\u53ef\u4ee5\u901a\u8fc7 \\((24)\\) \u5f0f\u5f97\u5230\uff1a \\[ \\pmb{d}f(\\pmb{X})= \\operatorname{tr}(\\frac{\\partial f(\\pmb{X})}{\\partial \\pmb{X}^T}\\pmb{dX})\\\\\\\\ \\tag{24} \\] \u90a3\u4e48\uff0c\u6211\u4eec\u8be5\u5982\u4f55\u5199\u6210\u5f62\u5982 \\((24)\\) \u5f0f\u7684\u7ed3\u679c\u5462\uff0c\u522b\u6025\uff0c\u8ba9\u6211\u4eec\u5148\u7ed9\u51fa \\(3\\times 2=6\\) \u4e2a\u4f60\u5e94\u8be5\u8bb0\u4f4f\u7684\u516c\u5f0f\uff08\u4ee5\u540e\u5c31\u76f4\u63a5\u7528\u4e86\uff09\u3002","title":"3.2 \u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u77e9\u9635\u5fae\u5206\u6c42\u5bfc"},{"location":"math%20in%20ML/2.%20M%20derivation/#321","text":"\\[ \\mathbb{d} (\\pmb {AXB})=\\pmb A \\mathbb{d}(\\pmb X)\\pmb B \\\\\\\\ \\tag{25.1.1} \\] \u5176\u4e2d, \\(\\pmb {A}_{p \\times m},\\ \\pmb B_{n \\times q}\\) \u662f\u5e38\u6570\u77e9\u9635\u3002 \u8bc1\u660e: \u7531\u4e58\u79ef\u6cd5\u5219 \\((22.3.2)\\) \u5f0f\u5f97: \\[ \\mathbb{d}(\\pmb {AXB})=\\mathbb{d}(\\pmb A)\\pmb {XB}+\\pmb A \\mathbb{d}(\\pmb X)\\pmb B+\\pmb {AX}\\mathbb{d}\\pmb B\\\\\\\\ \\tag{25.1.a} \\] \u7531\u5e38\u6570\u77e9\u9635\u5fae\u5206 \\((22.1)\\) \u5f0f\u5f97: \\[ \\mathbb{d} \\pmb A = \\pmb 0_{p \\times m}, \\mathbb{d} \\pmb B =\\pmb 0_{n \\times q}\\\\\\\\ \\tag{25.1.b} \\] \u8bc1\u6bd5\u3002 \\(\\pmb X_{m\\times n}\\) \u53ef\u4ee5\u4ee3\u5165\u5176\u4ed6\u4efb\u610f\u7684\u77e9\u9635\u51fd\u6570: \\[ \\mathbb{d}\\big( \\pmb{AF}(\\pmb X)\\pmb B \\big)=\\pmb A \\mathbb{d}\\big( \\pmb F(\\pmb X) \\big)\\pmb B\\\\\\\\ \\tag{25.1.2} \\]","title":"3.2.1 \u5939\u5c42\u997c"},{"location":"math%20in%20ML/2.%20M%20derivation/#322","text":"\\[ \\mathbb{d} |\\pmb X| = |\\pmb X|\\operatorname{tr}(\\pmb X^{-1}\\mathbb{d}\\pmb X) = \\operatorname{tr}(|\\pmb X|\\pmb X^{-1}\\mathbb{d}\\pmb X)\\\\\\\\ \\tag{25.2.1} \\] \u5176\u4e2d, \\(\\pmb X_{n \\times n}\\) \u3002 \u8bc1\u660e: \u9996\u5148\u660e\u786e, \u884c\u5217\u5f0f\u662f\u4e00\u4e2a\u5b9e\u503c\u6807\u91cf\u51fd\u6570, \u6545\u53ef\u4ee5\u4f7f\u7528 \\((24)\\) \u5f0f\u3002 \u6211\u4eec\u77e5\u9053, \u884c\u5217\u5f0f\u53ef\u4ee5\u6309\u7167\u4e00\u884c\u5c55\u5f00, \u5373\u4e00\u884c\u4e2d\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u4ed6\u7684 \u4ee3\u6570\u4f59\u5b50\u5f0f \u7136\u540e\u6c42\u548c\u3002 \u6211\u4eec\u6309\u7167\u5143\u7d20 \\(x_{ij}\\) \u6240\u5728\u7684\u7b2c \\(i\\) \u884c\u5c55\u5f00: \\[ |\\pmb X|=x_{i1}A_{i1}+x_{i2}A_{i2}+\\cdots +x_{in}A_{in}\\\\\\\\ \\tag{25.2.a} \\] \u56e0\u6b64\uff0c\u884c\u5217\u5f0f\u5bf9\u5143\u7d20 \\(x_{ij}\\) \u7684\u504f\u5bfc\uff0c\u5373\u4e3a\u8be5\u5143\u7d20\u5bf9\u5e94\u7684\u4ee3\u6570\u4f59\u5b50\u5f0f\u3002 \\[ \\frac{\\partial |\\pmb X|}{\\partial x_{ij}}=A_{ij}\\\\\\\\ \\tag{25.2.b} \\] \u56e0\u6b64, \u884c\u5217\u5f0f\u5bf9\u77e9\u9635\u6c42\u5bfc\u7684\u7ed3\u679c\u4e3a: \\[ \\begin{align} \\frac{\\partial |\\pmb{X}|}{\\partial \\pmb{X}^T} &= \\begin{bmatrix} A_{11} & A_{21} & \\cdots & A_{n1} \\\\ A_{12} & A_{22} & \\cdots & A_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{1n} & A_{2n} & \\cdots & A_{nn} \\\\ \\end{bmatrix} \\end{align} \\\\\\\\ \\tag{25.2.c} \\] \u8fd9\u4e2a\u7ed3\u679c\u5176\u5b9e\u5c31\u662f\u4f34\u968f\u77e9\u9635 \\(\\pmb X^*\\) \u3002 \u53c8\u56e0\u4e3a\u4f34\u968f\u77e9\u9635\u548c\u9006\u77e9\u9635\u7684\u5173\u7cfb: \\[ \\pmb X^{-1} = \\frac{\\pmb X^*}{|\\pmb X|}\\\\\\\\ \\tag{25.2.d} \\] \u4ee3\u5165 \\((24)\\) : \\[ \\begin{aligned} \\mathbb{d} |\\pmb X| & = \\operatorname{tr}(\\frac{\\partial |\\pmb X|}{\\partial \\pmb X^T})\\\\\\\\ & = \\operatorname{tr}(|\\pmb X|\\pmb X^{-1}\\mathbb{d}\\pmb X) \\end{aligned} \\] \u53c8\u56e0\u4e3a\u884c\u5217\u5f0f\u662f\u6807\u91cf\uff0c\u7531 \\((3)\\) \u5f0f\uff0c\u53ef\u4ee5\u63d0\u5230\u8ff9\u7684\u5916\u9762\uff0c\u5f97: \\[ \\mathbb{d} |\\pmb X| = |\\pmb X| \\operatorname{tr}(\\pmb X^{-1}\\mathbb{d}\\pmb X) = \\operatorname{tr}(|\\pmb X|\\pmb X^{-1}\\mathbb{d}\\pmb X)\\\\\\\\ \\tag{25.2.1} \\] \u8bc1\u6bd5\u3002 \\(\\pmb X_{n\\times n}\\) \u53ef\u4ee5\u4ee3\u5165\u5176\u4ed6\u4efb\u610f\u7684\u77e9\u9635\u51fd\u6570: \\[ \\mathbb{d} |\\pmb F (\\pmb X)| = |\\pmb F (\\pmb X)| \\operatorname{tr}(\\pmb F (\\pmb X)^{-1}\\mathbb{d}\\pmb F (\\pmb X)) = \\operatorname{tr}(|\\pmb F (\\pmb X)|\\pmb F (\\pmb X)^{-1}\\mathbb{d}\\pmb F (\\pmb X))\\\\\\\\ \\tag{25.2.2} \\]","title":"3.2.2 \u884c\u5217\u5f0f"},{"location":"math%20in%20ML/2.%20M%20derivation/#33_1","text":"\u5bf9\u4e8e\u5b9e\u503c\u6807\u91cf\u51fd\u6570 \\(f (\\pmb{X})\\) , \\(\\operatorname{tr}\\big( f(\\pmb X) \\big)=f(\\pmb X)\\) , \\(\\mathbb{d}f(\\pmb X)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb X) \\big)\\) \u6240\u4ee5\u6709 \\(\\mathbb{d}f(\\pmb X)=\\mathbb{d}\\big(\\operatorname{tr}f(\\pmb X)\\big)=\\operatorname{tr}\\big( \\mathbb{d}f(\\pmb X) \\big)\\)","title":"3.3 \u5982\u4f55\u4f7f\u7528\u77e9\u9635\u5fae\u5206\u6c42\u5bfc"}]}