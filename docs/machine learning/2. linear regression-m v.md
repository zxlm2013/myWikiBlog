## 1. å¤šç»´ç‰¹å¾

!!! info ""

    å‚è€ƒè§†é¢‘:
    4 - 1 - Multiple Features (8 min).mkv

ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ¢è®¨äº†å•å˜é‡/ç‰¹å¾çš„å›å½’æ¨¡å‹ï¼Œç°åœ¨æˆ‘ä»¬å¯¹æˆ¿ä»·æ¨¡å‹å¢åŠ æ›´å¤šçš„ç‰¹å¾ï¼Œä¾‹å¦‚æˆ¿é—´æ•°æ¥¼å±‚ç­‰ï¼Œæ„æˆä¸€ä¸ªå«æœ‰å¤šä¸ªå˜é‡çš„æ¨¡å‹ï¼Œæ¨¡å‹ä¸­çš„ç‰¹å¾ä¸º$\left( {x_{1}},{x_{2}},...,{x_{n}} \right)$ã€‚

| Size (feet2) | Number   of bedrooms | Number   of floors | Age of home (years) | Price   ($1000) |
| :----------: | :------------------: | :----------------: | :-----------------: | :-------------: |
|     2104     |          5           |         1          |         45          |       460       |
|     1416     |          3           |         2          |         40          |       232       |
|     1534     |          3           |         2          |         30          |       315       |
|     852      |          2           |         1          |         36          |       178       |
|      â€¦       |          â€¦           |         â€¦          |          â€¦          |        â€¦        |

å¢æ·»æ›´å¤šç‰¹å¾åï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ç³»åˆ—æ–°çš„æ³¨é‡Šï¼š

$n$ ä»£è¡¨ç‰¹å¾çš„æ•°é‡

${x^{\left( i \right)}}$ ä»£è¡¨ç¬¬ $i$ ä¸ªè®­ç»ƒå®ä¾‹ï¼Œæ˜¯ç‰¹å¾çŸ©é˜µä¸­çš„ç¬¬$i$è¡Œï¼Œæ˜¯ä¸€ä¸ª**å‘é‡**ï¼ˆ**vector**ï¼‰ã€‚

æ¯”æ–¹è¯´ï¼Œä¸Šå›¾çš„

${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}$ï¼Œ

${x}_{j}^{\left( i \right)}$ä»£è¡¨ç‰¹å¾çŸ©é˜µä¸­ç¬¬ $i$ è¡Œçš„ç¬¬ $j$ ä¸ªç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯ç¬¬ $i$ ä¸ªè®­ç»ƒå®ä¾‹çš„ç¬¬ $j$ ä¸ªç‰¹å¾ã€‚

å¦‚ä¸Šå›¾çš„$x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2$ï¼Œ

æ”¯æŒå¤šå˜é‡çš„å‡è®¾ $h$ è¡¨ç¤ºä¸ºï¼š$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ï¼Œ

è¿™ä¸ªå…¬å¼ä¸­æœ‰$n+1$ä¸ªå‚æ•°å’Œ$n$ä¸ªå˜é‡ï¼Œä¸ºäº†ä½¿å¾—å…¬å¼èƒ½å¤Ÿç®€åŒ–ä¸€äº›ï¼Œå¼•å…¥$x_{0}=1$ï¼Œåˆ™å…¬å¼è½¬åŒ–ä¸ºï¼š$h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

![2_1_1_theta_T_X](./../assets/images/2_1_1_theta_T_X.png)

æ­¤æ—¶æ¨¡å‹ä¸­çš„å‚æ•°æ˜¯ä¸€ä¸ª$n+1$ç»´çš„å‘é‡ï¼Œä»»ä½•ä¸€ä¸ªè®­ç»ƒå®ä¾‹ä¹Ÿéƒ½æ˜¯$n+1$ç»´çš„å‘é‡ï¼Œç‰¹å¾çŸ©é˜µ$X$çš„ç»´åº¦æ˜¯ $m*(n+1)$ã€‚ å› æ­¤å…¬å¼å¯ä»¥ç®€åŒ–ä¸ºï¼š$h_{\theta} \left( x \right)={\theta^{T}}X$ï¼Œå…¶ä¸­ä¸Šæ ‡  $T$ ä»£è¡¨çŸ©é˜µè½¬ç½®ã€‚

æ³¨æ„ï¼š
ä»ä¸Šå›¾å¯çŸ¥, $\theta$, $X$ æˆ‘ä»¬æ˜¯ç”¨**åˆ—å‘é‡**æ¥æ ‡è®°ã€‚è®¡ç®— $h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ çš„æ—¶å€™, å¯ä»¥å°† $h$ è¡¨ç¤ºä¸º $\theta^TX.$


## 2. å¤šå˜é‡æ¢¯åº¦ä¸‹é™

!!! info ""

    å‚è€ƒè§†é¢‘:
    4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv

å¿«é€Ÿå›é¡¾æˆ‘ä»¬çš„è®°å·ï¼Œå¹¶ç”¨å‘é‡ç®€åŒ–ã€‚

Hypothesis: $h_\theta(x)=\theta_0+\theta_1x+\theta_2x+...+\theta_nx$

ç®€è®°ä¸ºï¼š $h_\theta(x)=\theta^TX$

Parameters: $\theta_0,\theta_1,\theta_2,...\theta_n$

ç®€è®°ä¸ºï¼š$\theta$, $n+1\ dimension\ vector$

Cost Function: 

$$
J(\theta_0,\theta_1,\ \theta_2,...\theta_n)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

ç®€è®°ä¸ºï¼š

$$
J(\theta)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

Gradient descent:
**Repeat {**
            

â€‹                ${\theta_{j}}:={\theta_{j}}-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,...\theta_n)$

â€‹               **}** (simultaneously update for every j=0, 1, 2, ..., n)
ç®€è®°ä¸ºï¼šæŠŠä¸Šé¢çš„$J(\theta_0,...\theta_n)$æ¢æˆ$J(\theta)$

![2_2_1_gradient_descent_v2](./../assets/images/2_2_1_gradient_descent_v2.png)

ä¸å•å˜é‡çº¿æ€§å›å½’ç±»ä¼¼ï¼Œåœ¨å¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œï¼Œå³ï¼š$J\left( {\theta_{0}},{\theta_{1}}...{\theta_{n}} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}$ ï¼Œ

å…¶ä¸­ï¼š$h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ ï¼Œ

æˆ‘ä»¬çš„ç›®æ ‡å’Œå•å˜é‡çº¿æ€§å›å½’é—®é¢˜ä¸­ä¸€æ ·ï¼Œæ˜¯è¦æ‰¾å‡ºä½¿å¾—ä»£ä»·å‡½æ•°æœ€å°çš„ä¸€ç³»åˆ—å‚æ•°ã€‚
å¤šå˜é‡çº¿æ€§å›å½’çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸ºï¼š

å½“$n>=1$æ—¶ï¼Œ
${{\theta }_{0}}:={{\theta }_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}$

${{\theta }_{1}}:={{\theta }_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}$

${{\theta }_{2}}:={{\theta }_{2}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}$

...

${{\theta }_{n}}:={{\theta }_{n}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{n}^{(i)}$

æˆ‘ä»¬å¼€å§‹éšæœºé€‰æ‹©ä¸€ç³»åˆ—çš„å‚æ•°å€¼ï¼Œè®¡ç®—æ‰€æœ‰çš„é¢„æµ‹ç»“æœåï¼Œå†ç»™æ‰€æœ‰çš„å‚æ•°ä¸€ä¸ªæ–°çš„å€¼ï¼Œå¦‚æ­¤å¾ªç¯ç›´åˆ°æ”¶æ•›ã€‚

ä»£ç ç¤ºä¾‹ï¼š

è®¡ç®—ä»£ä»·å‡½æ•°
$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$
å…¶ä¸­ï¼š${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

**Python** ä»£ç ï¼š

```python
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```

<hr>

ä¸‹é¢ï¼Œæˆ‘ä»¬å¼€å§‹æ¨å¯¼$\frac{\partial}{\partial\theta}J(\theta)$ã€‚è¿™é‡Œä¸ºäº†æ™®éæ€§ï¼Œè¿˜åŸäº† $\theta$ çš„æ™®éæƒ…å†µï¼Œä¹Ÿå³æœ‰nä¸ª $\theta$ çš„æƒ…å†µã€‚

Hypothesis:         $h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$

Parameters:         $\theta_0,\theta_1,...,\theta_n$

Cost function: 

$$
J(\theta_0,\theta_1,...,\theta_n)=\frac {1} {2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

Gradient descent:
Repeat{
$$
\theta_j :=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n)
$$
â€‹	     }ï¼ˆsimultaneously update for every j=0, 1, 2, ..., n)ï¼‰

æ¨å¯¼ï¼š $\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n)=?$

Let's first work it for the case if we have only one training example $(x,y)$, so that we can neglect the sum in the definition $J$. We have:
$$
\begin{split}
\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,...,\theta_n) & =  \frac {1} {2}\frac{\partial}{\partial \theta_j}\Big(h_\theta(x)-y\Big)^2 \\\\
 & = 2\cdot\frac{1}{2}\cdot(h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(h_\theta(x)-y) \\\\
 & = (h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n) \\\\
 & = (h_\theta(x)-y)\cdot x_j
\end{split}
$$

## 3. æ¢¯åº¦ä¸‹é™æ³• - ç‰¹å¾ç¼©æ”¾

!!! info ""

    å‚è€ƒè§†é¢‘:
    4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv

åœ¨æˆ‘ä»¬é¢å¯¹å¤šç»´ç‰¹å¾é—®é¢˜çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¦ä¿è¯è¿™äº›ç‰¹å¾éƒ½å…·æœ‰ç›¸è¿‘çš„å°ºåº¦ï¼Œè¿™å°†å¸®åŠ©æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´å¿«åœ°æ”¶æ•›ã€‚

ä»¥æˆ¿ä»·é—®é¢˜ä¸ºä¾‹ï¼Œå‡è®¾æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç‰¹å¾ï¼Œæˆ¿å±‹çš„å°ºå¯¸å’Œæˆ¿é—´çš„æ•°é‡ï¼Œå°ºå¯¸çš„å€¼ä¸º 0-2000å¹³æ–¹è‹±å°ºï¼Œè€Œæˆ¿é—´æ•°é‡çš„å€¼åˆ™æ˜¯0-5ï¼Œä»¥ä¸¤ä¸ªå‚æ•°åˆ†åˆ«ä¸ºæ¨ªçºµåæ ‡ï¼Œç»˜åˆ¶ä»£ä»·å‡½æ•°çš„ç­‰é«˜çº¿å›¾èƒ½ï¼Œçœ‹å‡ºå›¾åƒä¼šæ˜¾å¾—å¾ˆæ‰ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•éœ€è¦éå¸¸å¤šæ¬¡çš„è¿­ä»£æ‰èƒ½æ”¶æ•›ã€‚
è§£å†³çš„æ–¹æ³•æ˜¯å°è¯•å°†æ‰€æœ‰ç‰¹å¾çš„å°ºåº¦éƒ½å°½é‡ç¼©æ”¾åˆ°-1åˆ°1ä¹‹é—´ã€‚å¦‚å›¾ï¼š

![2_3_1_feature_scaling](./../assets/images/2_3_1_feature_scaling.png)

è§†é¢‘é‡Œå´æ©è¾¾è€å¸ˆçš„æ–¹æ³•æ˜¯ä»¤ï¼š${{x}_{n}}=\frac{{{x}_{n}}-mean}{{max(x)-min(x)}}$, å…¶ä¸­ $mean$æ˜¯å¹³å‡å€¼ï¼Œ$max(x)-min(x)$åˆ†åˆ«æ˜¯æœ€å¤§å€¼å’Œæœ€å°å€¼ã€‚

æ›´é€šç”¨çš„æ˜¯ä»¤ï¼š${{x}_{n}}=\frac{{{x}_{n}}-{mean}}{\sigma}$ï¼Œå…¶ä¸­ $mean$æ˜¯å¹³å‡å€¼ï¼Œ$\sigma$æ˜¯æ ‡å‡†å·®ã€‚

pythoné‡Œçš„apiï¼š

- **sklearn.preprocessing.StandardScaler()**
 
    - å¤„ç†ä¹‹åæ¯åˆ—æ¥è¯´**æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨å‡å€¼0é™„è¿‘æ ‡å‡†å·®å·®ä¸º1**
    - **StandardScaler.fit_transform(X)**
        - X:numpy arrayæ ¼å¼çš„æ•°æ®[n_samples,n_features]
    - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array

éƒ¨åˆ†å‚è€ƒä»£ç ï¼š
```python
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


def linear_model1():
    # 1.è·å–æ•°æ®
    boston = load_boston()

    # 2. æ•°æ®é›†åˆ’åˆ†
    x_train, x_test, y_train, y_test = train_test_split(
        boston.data, boston.target, test_size=0.2)

    # 3. ç‰¹å¾å·¥ç¨‹-æ ‡å‡†åŒ–
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
```

## 4. æ¢¯åº¦ä¸‹é™æ³• - å­¦ä¹ ç‡

!!! info ""

    å‚è€ƒè§†é¢‘:
    4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv

æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚

![2_4_1_J_iterations](./../assets/images/2_4_1_J_iterations.png)

ä¹Ÿæœ‰ä¸€äº›è‡ªåŠ¨æµ‹è¯•æ˜¯å¦æ”¶æ•›çš„æ–¹æ³•ï¼Œä¾‹å¦‚å°†ä»£ä»·å‡½æ•°çš„å˜åŒ–å€¼ä¸æŸä¸ªé˜€å€¼ï¼ˆä¾‹å¦‚0.001ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œä½†é€šå¸¸çœ‹å·¦ä¸Šæ–¹è¿™æ ·çš„å›¾è¡¨æ›´å¥½ã€‚

ä¸æ­£ç¡®çš„å­¦ä¹ ç‡ï¼Œä¼šäº§ç”Ÿå·¦ä¾§ä¸Šä¸‹ä¸¤ä¸ªå›¾åƒã€‚
![2_4_2_no_proper_alpha](./../assets/images/2_4_2_no_proper_alpha.png)

æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ¯æ¬¡è¿­ä»£å—åˆ°å­¦ä¹ ç‡çš„å½±å“ï¼Œå¦‚æœå­¦ä¹ ç‡ $\alpha$ è¿‡å°ï¼Œåˆ™è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ä¼šéå¸¸é«˜ï¼›å¦‚æœå­¦ä¹ ç‡ $\alpha$ è¿‡å¤§ï¼Œæ¯æ¬¡è¿­ä»£å¯èƒ½ä¸ä¼šå‡å°ä»£ä»·å‡½æ•°ï¼Œå¯èƒ½ä¼šè¶Šè¿‡å±€éƒ¨æœ€å°å€¼å¯¼è‡´æ— æ³•æ”¶æ•›ã€‚

é€šå¸¸å¯ä»¥è€ƒè™‘å°è¯•äº›å­¦ä¹ ç‡ï¼š

$\alpha= 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10$

## 5. ç‰¹å¾å’Œå¤šé¡¹å¼å›å½’

!!! info ""

    å‚è€ƒè§†é¢‘:
    4 - 5 - Features and Polynomial Regression (8 min).mkv

å¦‚æˆ¿ä»·é¢„æµ‹é—®é¢˜ï¼Œ

![2_5_1_house](./../assets/images/2_5_1_house.png)

$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}$ 

å½“æˆ‘ä»¬çœŸæ­£åº”ç”¨çº¿æ€§å›å½’æ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›é€ è‡ªå·±çš„ç‰¹å¾å³ï¼š
${x_{1}}=frontage$ï¼ˆä¸´è¡—å®½åº¦ï¼‰ï¼Œ${x_{2}}=depth$ï¼ˆçºµå‘æ·±åº¦ï¼‰ï¼Œ$x=frontage*depth=area$ï¼ˆé¢ç§¯ï¼‰ï¼Œ
åˆ™ï¼š${h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x$ã€‚

![2_5_2_polynomial_regression](./../assets/images/2_5_2_polynomial_regression.png)

çº¿æ€§å›å½’å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰æ•°æ®ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦å…¶ä»–æ¨¡å‹æ¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ï¼Œæ¯”å¦‚ä¸€ä¸ªäºŒæ¬¡æ–¹æ¨¡å‹ï¼š$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}$
 æˆ–è€…ä¸‰æ¬¡æ–¹æ¨¡å‹ï¼š $h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}$ 

ä»ä¸Šé¢å›¾å³ä¾§ï¼Œå¯ä»¥çœ‹å‡ºå¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ç‰¹å¾ç¼©æ”¾çš„é‡è¦æ€§äº†ã€‚

é€šå¸¸æˆ‘ä»¬éœ€è¦å…ˆè§‚å¯Ÿæ•°æ®ç„¶åå†å†³å®šå‡†å¤‡å°è¯•æ€æ ·çš„æ¨¡å‹ã€‚ å¦å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä»¤ï¼š

${{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}$ï¼Œä»è€Œå°†æ¨¡å‹è½¬åŒ–ä¸ºçº¿æ€§å›å½’æ¨¡å‹ã€‚

æ ¹æ®å‡½æ•°å›¾å½¢ç‰¹æ€§ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ï¼š

${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta}_{2}}{{(size)}^{2}}$

æˆ–è€…:

${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta }_{2}}\sqrt{size}$

![2_5_3_choice_of_X](./../assets/images/2_5_3_choice_of_X.png)

## 6. æ­£è§„æ–¹ç¨‹

!!! info ""

    å‚è€ƒè§†é¢‘:
    4 - 6 - Normal Equation (16 min).mkv

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬éƒ½åœ¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†æ˜¯å¯¹äºæŸäº›çº¿æ€§å›å½’é—®é¢˜ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚å¦‚ï¼š

![2_6_1](./../assets/images/2_6_1.png)

æ­£è§„æ–¹ç¨‹æ˜¯é€šè¿‡æ±‚è§£ä¸‹é¢çš„æ–¹ç¨‹æ¥æ‰¾å‡ºä½¿å¾—ä»£ä»·å‡½æ•°æœ€å°çš„å‚æ•°çš„ï¼š$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ ã€‚
 å‡è®¾æˆ‘ä»¬çš„è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µä¸º $X$ï¼ˆåŒ…å«äº† ${{x}_{0}}=1$ï¼‰å¹¶ä¸”æˆ‘ä»¬çš„è®­ç»ƒé›†ç»“æœä¸ºå‘é‡ $y$ï¼Œåˆ™åˆ©ç”¨æ­£è§„æ–¹ç¨‹è§£å‡ºå‘é‡ $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ ã€‚

ä»¥ä¸‹é¢è¡¨æ ¼æ•°æ®ä¸ºä¾‹ $m=4$ï¼š

| $x_0$   | Size (feet2) | Number   of bedrooms | Number   of floors | Age of home (years) | Price   ($1000) |
| ---- | ------------ | -------------------- | ------------------ | ------------------- | --------------- |
| 1    | 2104         | 5                    | 1                  | 45                  | 460             |
| 1    | 1416         | 3                    | 2                  | 40                  | 232             |
| 1    | 1534         | 3                    | 2                  | 30                  | 315             |
| 1    | 852          | 2                    | 1                  | 36                  | 178             |

$X\ \text{=}\begin{bmatrix}1 & 2104 & 5 & 1 & 45\\1 & 1416 & 3 & 2 & 40\\ 1 &1534 & 3 & 2 & 30\\ 1 & 852 & 2 & 1 & 36\end{bmatrix}$ï¼Œ$y\ \text{=}\begin{bmatrix} 460\\232\\315\\178\end{bmatrix}$

$Xç»´åº¦ï¼š(m,n+1),\ yçš„ç»´åº¦ï¼š(m,1)$

è¿™æ—¶å€™æ±‚è§£ $\theta$ åªéœ€ä¸€æ­¥:  $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ã€‚

å°†ä¸Šé¢çš„ä¾‹å­æ¨å¹¿åˆ°ä¸€èˆ¬æƒ…å†µ

![2_6_2](./../assets/images/2_6_2.png)

æ³¨ï¼šå¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚
åƒä¸‡è¦æ³¨æ„è¿™é‡Œçš„è®¾è®¡çŸ©é˜µXå®ƒçš„æ„æˆï¼Œè®¾è®¡å®Œæˆåï¼Œå‡è®¾å‡½æ•°å¯ä»¥å‘é‡åŒ–ä¸º $h_\theta(x)=X\theta$

æ¢¯åº¦ä¸‹é™ä¸æ­£è§„æ–¹ç¨‹çš„æ¯”è¾ƒï¼š

| æ¢¯åº¦ä¸‹é™             | æ­£è§„æ–¹ç¨‹                                     |
| ---------------- | ---------------------------------------- |
| éœ€è¦é€‰æ‹©å­¦ä¹ ç‡$\alpha$  | ä¸éœ€è¦                                      |
| éœ€è¦å¤šæ¬¡è¿­ä»£           | ä¸€æ¬¡è¿ç®—å¾—å‡º                                   |
| å½“ç‰¹å¾æ•°é‡$n$å¤§æ—¶ä¹Ÿèƒ½è¾ƒå¥½é€‚ç”¨ | éœ€è¦è®¡ç®—${{\left( {{X}^{T}}X \right)}^{-1}}$ å¦‚æœç‰¹å¾æ•°é‡nè¾ƒå¤§åˆ™è¿ç®—ä»£ä»·å¤§ï¼Œå› ä¸ºçŸ©é˜µé€†çš„è®¡ç®—æ—¶é—´å¤æ‚åº¦ä¸º$O\left( {{n}^{3}} \right)$ï¼Œé€šå¸¸æ¥è¯´å½“$n$å°äº10000 æ—¶è¿˜æ˜¯å¯ä»¥æ¥å—çš„ |
| é€‚ç”¨äºå„ç§ç±»å‹çš„æ¨¡å‹       | åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹                  |

æ€»ç»“ä¸€ä¸‹ï¼Œåªè¦ç‰¹å¾å˜é‡çš„æ•°ç›®å¹¶ä¸å¤§ï¼Œæ ‡å‡†æ–¹ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®¡ç®—å‚æ•°$\theta $çš„æ›¿ä»£æ–¹æ³•ã€‚å…·ä½“åœ°è¯´ï¼Œ**åªè¦ç‰¹å¾å˜é‡æ•°é‡å°äºä¸€ä¸‡ï¼Œæˆ‘é€šå¸¸ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ³•ï¼Œè€Œä¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚**

éšç€æˆ‘ä»¬è¦è®²çš„å­¦ä¹ ç®—æ³•è¶Šæ¥è¶Šå¤æ‚ï¼Œä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬è®²åˆ°åˆ†ç±»ç®—æ³•ï¼Œåƒé€»è¾‘å›å½’ç®—æ³•ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œå®é™…ä¸Šå¯¹äºé‚£äº›ç®—æ³•ï¼Œå¹¶ä¸èƒ½ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ã€‚å¯¹äºé‚£äº›æ›´å¤æ‚çš„å­¦ä¹ ç®—æ³•ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»ç„¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚å› æ­¤ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„ç®—æ³•ï¼Œå¯ä»¥ç”¨åœ¨æœ‰å¤§é‡ç‰¹å¾å˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ã€‚æˆ–è€…æˆ‘ä»¬ä»¥ååœ¨è¯¾ç¨‹ä¸­ï¼Œä¼šè®²åˆ°çš„ä¸€äº›å…¶ä»–çš„ç®—æ³•ï¼Œå› ä¸ºæ ‡å‡†æ–¹ç¨‹æ³•ä¸é€‚åˆæˆ–è€…ä¸èƒ½ç”¨åœ¨å®ƒä»¬ä¸Šã€‚ä½†å¯¹äºè¿™ä¸ªç‰¹å®šçš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œæ ‡å‡†æ–¹ç¨‹æ³•æ˜¯ä¸€ä¸ªæ¯”æ¢¯åº¦ä¸‹é™æ³•æ›´å¿«çš„æ›¿ä»£ç®—æ³•ã€‚æ‰€ä»¥ï¼Œæ ¹æ®å…·ä½“çš„é—®é¢˜ï¼Œä»¥åŠä½ çš„ç‰¹å¾å˜é‡çš„æ•°é‡ï¼Œè¿™ä¸¤ç§ç®—æ³•éƒ½æ˜¯å€¼å¾—å­¦ä¹ çš„ã€‚

æ­£è§„æ–¹ç¨‹çš„**python**å®ç°ï¼š

```python
import numpy as np
    
 def normalEqn(X, y):
    
   theta = np.linalg.inv(X.T@X)@X.T@y #X.T@Xç­‰ä»·äºX.T.dot(X)
    
   return theta
```

<br>
<br>
<br>

!!! info ""

    **ä»¥ä¸‹æ˜¯æ­£è§„æ–¹ç¨‹è‹±æ–‡ç‰ˆ(æœ‰è¯¦ç»†æ¨å¯¼-è‡ªå·±åŠ ä¸Šçš„)ï¼Œå…¶ä»–å†…å®¹æ¥æºäº2006å¹´éº»çœç†å·¥cs 2009æœºå™¨å­¦ä¹ çš„noteã€‚**

## 7. The normal equations

<!--
ä¸‹é¢è¿™ä¸ªcssç”¨äºæ§åˆ¶pæ ‡ç­¾çš„ä¸¤ç«¯å¯¹é½
-->
<style type="text/css">
p {
    text-align: justify;  /*æ–‡æœ¬ä¸¤ç«¯å¯¹é½*/
}
</style>

Gradient descent gives one way of minimizing $J$. Lets discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In this method, we will minimize $J$ by explicitly taking its derivatives with respect to the $Î¸_j's$, and setting them to zero. To enable us to do this without having to write reams of algebra and pages full of matrices of derivatives, lets introduce some notation for doing calculus with matrices. 

### 7.1 Matrix derivatives

For a function $f$ : $\mathbb{R^{m\times n}} \rightarrow \mathbb{R}$ mapping from $\text{m-by-n}$ matrices to the real numbers, we define the derivative of f with respect to A to be:

$$
\nabla _Af(A)=\begin{bmatrix} \frac{\partial f}{\partial A_{11}} & \cdots  & \frac{\partial f}{\partial A_{1n}} \\\\ 
\vdots  & \ddots & \vdots \\\\
\frac{\partial f}{\partial A_{m1}} & \cdots  & \frac{\partial f}{\partial A_{mn}}\end{bmatrix}
$$

Thus, the gradient $\nabla _Af(A)$ is itself an $\text{m-by-n}$  matrix, whose $(i,j)$-element is $\frac{\partial f}{\partial A_{ij}}$. For example, suppose $A =\bigl( \begin{smallmatrix} A_{11}  & A_{12} \\ A_{21} & A_{22} \end{smallmatrix} \bigr)$ is a 2-by-2 matrix, and the function $f$: $\mathbb{R^{2\times 2}}\rightarrow \mathbb{R}$ is given by

$$
f(A)=\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22}
$$

Here, $A_{ij}$ denotes the $(i,j)$ entry of the matrix $A$. We then have

$$
\nabla _Af(A)=\begin{bmatrix} 
\frac{3}{2} & 10A_{12}\\\\
A_{22} & A_{21}
\end{bmatrix}
$$

&ensp; We also introduce the **trace** opertator, written by "tr.". For an $\text{n-by-n}$ (square) matrix A, the trace of A is defined to be the sum of its diagonal entries:

$$
\operatorname{tr}A=\sum_{i=1}^nA_{ii}
$$

If $a$ is a real number (i.e., a 1-by-1 matrix), then $\operatorname{tr} a = a$. (If you haven't seen this "opertator notation" before, you should think of the trace of $A$ as $\operatorname{tr}(A)$, or as application of the "trace" function to the matrix $A$. It's more commonly written without the parentheses, however.ï¼‰
&ensp; The trace opertator has the property that for two matrices $A$ and $B$ such that $AB$ is square, we have that $\operatorname{tr}(AB)=\operatorname{tr}(BA)$. <a href="../../math in ML/2. M derivation/#25">è¯æ˜è¯·æŸ¥çœ‹æœºå™¨å­¦ä¹ ä¸­çš„æ•°å­¦çŸ¥è¯†ä¸­å…³äºè¿¹çš„äº¤æ¢å¾‹çš„è¯æ˜ã€‚</a> As corollaries of this, we also have, e.g.,

$$ \operatorname{tr}(ABC)=\operatorname{tr}(CAB)=\operatorname{tr}(BCA)$$

$$ \operatorname{tr}(ABCD)=\operatorname{tr}(DABC)=\operatorname{tr}(CDAB)=\operatorname{tr}(BCDA)$$

The following properties of the trace operator are also easily verified. Here, $A$ and $B$ are square matrices, and $a$ is a real number:

$$ \operatorname{tr}(A) = \operatorname{tr}(A^T)$$

$$ \operatorname{tr}(A+B) = \operatorname{tr}(A)+\operatorname{tr}(B)$$

$$ \operatorname{tr}(aA) = a \operatorname{tr}(A)$$

&ensp; We now state without proof some facts of matrix derivatives (we wonâ€™t need some of these until later this quarter). Equation (4) applies only to non-singular square matrices A, where |A| denotes the determinant of A. We have:

$$ \nabla_A \operatorname{tr}(AB)=B^T\\\\ \tag{1}$$

$$ \nabla_{A^T}f(A) = \big(\nabla_Af(A)\big)^T \\\\\tag{2}$$

$$ \nabla_A \operatorname{tr}ABA^TC=CAB+C^TAB^T \\\\\tag{3} $$

$$\nabla_A|A|=|A|(A^{-1})^T\\\\\tag{4}$$

---

å´æ©è¾¾è€å¸ˆä¸ç»™ä½ ä»¬è¯æ˜ï¼Œæˆ‘æ¥ç»™ä½ ä»¬è¯æ˜:sunglasses:

è¯·åœ¨çœ‹ä¸‹é¢æ¨å¯¼ä¹‹å‰, åŠ¡å¿…å…ˆçœ‹æ‡‚æˆ‘è¿™ä¸ªçŸ¥è¯†åº“ä¸­**æœºå™¨å­¦ä¹ ä¸­çš„æ•°å­¦çŸ¥è¯†**&ensp;---&ensp;( **ä¸€. çŸ©é˜µæ±‚å¯¼æœ¬è´¨ &äºŒ. çŸ©é˜µæ±‚å¯¼**)ã€‚

![2_7_1](./../assets/images/2_7_1.png)

I. $\nabla_A \operatorname{tr}(AB)=B^T$

**è¯æ˜:**

ç”± <a href="../../math in ML/2. M derivation/#24">äºŒ.çŸ©é˜µæ±‚å¯¼ä¸­---(6)å¼çš„è¯æ˜</a> æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼š

å¯¹äºä¸¤ä¸ªé˜¶æ•°éƒ½æ˜¯ $m \times n$ çš„çŸ©é˜µ $C_{m \times n}, D_{m \times n}$ å…¶ä¸­**ä¸€ä¸ªçŸ©é˜µä¹˜ä»¥ï¼ˆå·¦ä¹˜å³ä¹˜éƒ½å¯ä»¥ï¼‰å¦ä¸€ä¸ªçŸ©é˜µçš„ ==è½¬ç½®== çš„è¿¹ï¼Œæœ¬è´¨æ˜¯ $C_{m \times n}, D_{m \times n}$ ä¸¤ä¸ªçŸ©é˜µå¯¹åº”ä½ç½®çš„å…ƒç´ ç›¸ä¹˜å¹¶ç›¸åŠ **ã€‚

æ‰€ä»¥è¿™é‡Œ $\operatorname{tr}(AB)$ ç›¸å½“äºå°±æ˜¯ $A$ å’Œ $B^T$  æ¯ä¸€ä¸ªä½ç½®å¯¹åº”å…ƒç´  ==ç›¸ä¹˜å¹¶ç›¸åŠ == ã€‚
å…¶ä¸­, $A_{m \times n}, B_{n \times m}$ã€‚

æ¥ç€, ç”± <a href="../../math in ML/1. M derivation essence/#23_1">äºŒ.çŸ©é˜µæ±‚å¯¼æœ¬è´¨---(11)å¼</a> æˆ‘ä»¬å¯ä»¥çŸ¥é“:

è¿™é‡Œæ±‚ $\nabla_A \operatorname{tr}(AB)$ ç›¸å½“äº $\operatorname{tr}(AB)$ æŒ‰ç…§ $A$ çŸ©é˜µåˆ†å¸ƒçš„æ¯ä¸ªä½ç½®å…ƒç´ æ±‚åå¯¼ã€‚

æ‰€ä»¥, ç»¼ä¸Šæ‰€è¿°, $\nabla_A \operatorname{tr}(AB)=B^T$ã€‚

**è¯æ¯•ã€‚**

II. $\nabla_{A^T}f(A) = \big(\nabla_Af(A)\big)^T$

**è¯æ˜:**

ç”± <a href="../../math in ML/1. M derivation essence/#23_1">äºŒ.çŸ©é˜µæ±‚å¯¼æœ¬è´¨---(11)å¼</a> å¯çŸ¥:

$$
\begin{align}
\nabla_{A^T}f(A) &=
\begin{bmatrix} \frac{\partial f}{\partial a_{11}} & \cdots  & \frac{\partial f}{\partial a_{m1}} 
\\ \vdots & \ddots & \vdots 
\\ \frac{\partial f}{\partial x_{1n}} & \cdots & \frac{\partial f}{\partial x_{mn}} \\
\end{bmatrix}\\\\
 & = \begin{bmatrix} \frac{\partial f}{\partial a_{11}} & \cdots  & \frac{\partial f}{\partial a_{1n}} 
\\ \vdots & \ddots & \vdots 
\\ \frac{\partial f}{\partial x_{m1}} & \cdots & \frac{\partial f}{\partial x_{mn}} \\
\end{bmatrix}\\\\
 & = \big(\nabla_Af(A)\big)^T
\end{align}
$$

**è¯æ¯•ã€‚**

III. $\nabla_A \operatorname{tr}ABA^TC=CAB+C^TAB^T$

**è¯æ˜:**

é¦–å…ˆ, æˆ‘ä»¬è¦æ˜ç¡®è¿™é‡Œçš„ $ABA^TC$ æ˜¯å…³äº $A$ çŸ©é˜µçš„å®å€¼æ ‡é‡å‡½æ•°, æ‰€ä»¥, æˆ‘ä»¬å¯ä»¥ä»¤ $f(A)=ABA^TC$ã€‚

!!! note ""

    **æ³¨æ„:** è¿™é‡Œçš„æ ‡è®°, ç”±äº $A$ å…¶å®æ˜¯çŸ©é˜µå˜å…ƒ, åº”è¯¥æ ‡è®°ä¸º $f(\pmb A)=\pmb A B \pmb A^TC$, æ‰€ä»¥åé¢çš„æ¨å¯¼è¿‡ç¨‹,æˆ‘ä»¬ä¸¥è°¨ä¸€äº›, å°†çŸ©é˜µå˜å…ƒ $A$ æ ‡è®°ä¸º $\pmb A$ã€‚

ä»”ç»†æƒ³ä½ ä¼šå‘ç°ï¼Œå¯¹äºå®å€¼æ ‡é‡å‡½æ•° $f (\pmb{A})$, $\operatorname{tr}\big( f(\pmb A) \big)=f(\pmb A)$ , $\mathbb{d}f(\pmb A)=\operatorname{tr}\big( \mathbb{d}f(\pmb A) \big)$

æ‰€ä»¥æœ‰ $\mathbb{d}f(\pmb A)=\mathbb{d}\big(\operatorname{tr}f(\pmb A)\big)=\operatorname{tr}\big( \mathbb{d}f(\pmb A) \big)$ ã€‚

ç”± <a href="../../math in ML/2. M derivation/#32_1">äºŒ.çŸ©é˜µæ±‚å¯¼æœ¬è´¨---(24)å¼</a> å³:

$$
\mathbb{d}f(\pmb{X})= \operatorname{tr}\Big(\frac{\partial f(\pmb{X})}{\partial \pmb{X}^T}\mathbb{d}\pmb{X}\Big)
$$

æˆ‘ä»¬å¯ä»¥æŠŠä¸€ä¸ªçŸ©é˜µå˜å…ƒçš„å®å€¼æ ‡é‡å‡½æ•°çš„å…¨å¾®åˆ†å†™æˆä¸Šå¼ï¼Œæˆ‘ä»¬å°±æ‰¾åˆ°äº†çŸ©é˜µæ±‚å¯¼çš„ç»“æœ, ä¹Ÿå³: 

$$
\mathbb{d}f(\pmb{A})= \operatorname{tr}\Big(\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}\mathbb{d}\pmb{A}\Big)\\\\
\tag{II.1}
$$

ç”±æˆ‘ä»¬è¯æ˜çš„ II. $\nabla_{A^T}f(A) = \big(\nabla_Af(A)\big)^T$å¾—:

$$
\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T} = \nabla _{\pmb A^T}f(\pmb A)=\big(\nabla _{\pmb A}f(\pmb A)\big)^T
$$

æ‰€ä»¥æˆ‘ä»¬è¦æ±‚çš„:

$$
\nabla _{\pmb A}f(\pmb A)=\Big(\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}\Big)^T \\\\
\tag{II.2}
$$

æœ€ç»ˆæˆ‘ä»¬çš„ä»»åŠ¡å°±æ˜¯è½¬åŒ–ä¸ºæ±‚ $\mathbb{d}f(\pmb{A})$ çš„å…¨å¾®åˆ†, ä¸‹é¢å¼€å§‹æ¨å¯¼ï¼š

$$
\begin{aligned}
\mathbb{d}f(\pmb{A}) & = \mathbb{d}\pmb A B \pmb A^TC \\\\
 & = \mathbb{d}\operatorname{tr}(\pmb A B \pmb A^TC)\\\\
 & = \mathbb{d}\operatorname{tr}(C \pmb A B \pmb A^T)\\\\
 & = \operatorname{tr}\mathbb{d}(C \pmb A B \pmb A^T)\\\\
 & = \operatorname{tr}\Big(\mathbb{d}(C \pmb A) B \pmb A^T +  C \pmb A \mathbb{d}(B \pmb A^T)\Big)\\\\
 & = \operatorname{tr}\Big(C(\mathbb{d}\pmb A) B \pmb A^T +  C \pmb A B \mathbb{d}\pmb A^T\Big)\\\\
 & = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big( C \pmb A B \mathbb{d}\pmb A^T\Big)\\\\
 & = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big( C \pmb A B (\mathbb{d}\pmb A)^T\Big)\\\\
 & = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big(\mathbb{d}\pmb A (B^T \pmb A^T C^T )\Big)\\\\
 & = \operatorname{tr}\Big(B \pmb A^T C\mathbb{d}\pmb A \Big) + \operatorname{tr}\Big( B^T\pmb A^T C^T  \mathbb{d}\pmb A\Big)\\\\
 & = \operatorname{tr}\Big[(B \pmb A^T C + B^T\pmb A^T C^T) \mathbb{d}\pmb A\Big]\\\\
\end{aligned}
$$

> æ•°å­—æ˜¯æ­¥éª¤ æ±‰å­—æ˜¯æ¯ä¸€æ­¥ä¾æ® <br>
> 01 -> 02 å®å€¼æ ‡é‡å‡½æ•°çš„æ€§è´¨ <br>
> 02 -> 03 è¿¹çš„äº¤æ¢å¾‹ <br>
> 03 -> 04 å®å€¼æ ‡é‡å‡½æ•°çš„æ€§è´¨ <br>
> 04 -> 05 çŸ©é˜µå¾®åˆ†çš„ä¹˜ç§¯æ³•åˆ™ <br>
> 05 -> 06 å¤¹å±‚é¥¼ <br>
> 06 -> 07 è¿¹çš„çº¿æ€§æ³•åˆ™, è¿¹çš„äº¤æ¢å¾‹ <br>
> 07 -> 08 çŸ©é˜µå¾®åˆ†çš„è½¬ç½®æ³•åˆ™ <br>
> 08 -> 09 è½¬ç½®çš„è¿¹ç­‰äºåŸçŸ©é˜µçš„è¿¹ <br>
> 09 -> 10 è¿¹çš„äº¤æ¢å¾‹ <br>
> 10 -> 11 è¿¹çš„çº¿æ€§æ³•åˆ™ <br>

ç»“åˆå‰é¢çš„ $(II.1)$ å¼å¯å¾—:

$$
\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}=B \pmb A^T C + B^T\pmb A^T C^T
$$

å†ç»“åˆå‰é¢çš„ $(II.2)$ å¼å¯å¾—:

$$
\begin{aligned}
\nabla _{\pmb A}f(\pmb A) & = \Big(\frac{\partial f(\pmb{A})}{\partial \pmb{A}^T}\Big)^T \\\\
& = (B \pmb A^T C + B^T\pmb A^T C^T)^T\\\\
& = C^T \pmb A B^T + C \pmb A B 
\end{aligned}
$$

**è¯æ¯•ã€‚**

IV. $\nabla_A|A|=|A|(A^{-1})^T\\\\$

**è¯æ˜:**

ç”± <a href="../../math in ML/2. M derivation/#322">çŸ©é˜µè¡Œåˆ—å¼å¾®åˆ† å³(25.2.1)å¼</a>çš„è¯æ˜å¯çŸ¥:

$$
\mathbb{d} |\pmb A| = \operatorname{tr}(|\pmb A|\pmb A^{-1}\mathbb{d}\pmb A)
$$

å†ç”± <a href="../../math in ML/2. M derivation/#32_1">äºŒ.çŸ©é˜µæ±‚å¯¼æœ¬è´¨---(24)å¼</a>å¯å¾—ï¼š

$$
\frac{\partial |\pmb A|}{\partial \pmb A^T}=|\pmb A|\pmb A^{-1}
$$

å› æ­¤,

$$
\begin{aligned}
\nabla _{\pmb A}|\pmb A| & = (\frac{\partial |\pmb A|}{\partial \pmb A^T})^T \\\\
&=(|\pmb A|\pmb A^{-1})^T\\\\
&=|\pmb A|(\pmb A^{-1})^T
\end{aligned}
$$

**è¯æ¯•ã€‚**

è‡³æ­¤, å´æ©è¾¾è€å¸ˆçœç•¥çš„è¯æ˜, å…¨éƒ¨è¯æ˜å®Œæ¯•ã€‚ä¸‹é¢ç»§ç»­è®°ç¬”è®°ã€‚

---

To make our martix notation more concrete, let us now explain in detail the meaning of the first of these equations. Suppose we have some fixed matrix $B \in \mathbb{R}^{n\times m}$ . We can then define a function $f :\mathbb{R}^{m\times n}\rightarrow \mathbb{R}$ according to $f(A)=AB$. Note that this definition makes sense, because if $A \in \mathbb{R}^{m\times n}$, then $AB$ is a square matrix, and we can apply the trace operator to it; thus, $f$ does indeed map from $\mathbb{R}^{m\times n}$ to $\mathbb{R}$. We can then apply our definition of matrix derivatives to find $\nabla _Af(A)$, which will itself by an m-by-n matrix. Equation (1) above states that the $(i,j)$ entry of this matrix will given by the $(i,j)$-entry of $B^T$ , or equivalently, by $B_{j,i}$ .

&ensp; The proofs of Equation (1-3) are reasonably simply, and are left as an exercise to the reader. Equation (4) can be derived using adjoint representation of the inverse of a martix.

### 7.2 Least squares revisited

Armed with the tools of matrix derivatives, let us now proceed to find in closed-form the value of $\theta$ that minimizes $J(\theta)$. We begin by re-writing $J$ in matrix-vectorial notation.

&ensp;Giving a training set, define **the design matrix** $X$ to be the m-by-n matrix (actually m-by-n+1, if we include the intercept term) that contains the training examples' input values in its row:

$$
X = \begin{bmatrix} --- & (x^{(1)})^T & --- \\\\ --- & (x^{(2)})^T &  ---
\\\\ & \vdots &
\\\\ --- & (x^{(m)})^T & ---
\end{bmatrix}
$$

Also, let $\vec{y}$ be the m-dimensional vector containing all the target values from the training set:

$$
\vec{y}=\begin{bmatrix} 
y^{(1)}\\\\
y^{(2)}\\\\
\vdots\\\\
y^{(m)}
\end{bmatrix}
$$

Now, since $h_\theta\big(x^{(i)}\big)=(x^{(i)})^T\theta$, we can easily verify that

$$
\begin{aligned}
X\theta-\vec{y} &= 
\begin{bmatrix} (x^{(1)})^T\theta \\\\  \vdots  \\\\
(x^{(m)})^T\theta
\end{bmatrix} - \begin{bmatrix}  y^{(1)}\\\\
\vdots\\\\
y^{(m)}
\end{bmatrix}\\\\
& = \begin{bmatrix}
(x^{(1)})^T\theta- y^{(1)} \\\\
  \vdots \\\\
(x^{(m)})^T\theta- y^{(m)} 
\end{bmatrix}
\end{aligned}
$$

Thus, using the fact for a vector $z$ , we have that $z^Tz=\sum_{i}z_i^2$ .

$$
\begin{aligned}
\frac{1}{2}(X\theta-\vec{y})^T(X\theta-\vec{y})&=\frac{1}{2}\sum_{i=1}^m\Big(h_\theta(x^{(i)})- y^{(i)} \Big)^2\\\\
&=J(\theta)
\end{aligned}
$$

Finally , to minimize $J$ , lets find its derivatives with respect to $\theta$ . Combining Equations (2) and (3) , we find that

$$
\nabla _{A^T}\operatorname{tr}(ABA^TC)=B^TA^TC^T+BA^TC \\\\
\tag{5}
$$

Hence,

$$
\begin{aligned}
\nabla _\theta J(\theta) & = \nabla _\theta \frac{1}{2}(X\theta-\vec{y})^T(X\theta-\vec{y}) \\\\
& = \frac{1}{2} \nabla _\theta(\theta^TX^TX\theta-\theta^TX^T \vec{y}-\vec{y}^TX \theta+\vec{y}^T \vec{y})\\\\
& = \frac{1}{2} \nabla _\theta \operatorname{tr}(\theta^TX^TX\theta-\theta^TX^T \vec{y}-\vec{y}^TX \theta+\vec{y}^T \vec{y})\\\\
& = \frac{1}{2} \nabla _\theta\Big(\operatorname{tr}(\theta^TX^TX\theta)-2\operatorname{tr}(\vec{y}^TX \theta)\Big)\\\\
& = \frac{1}{2}(X^TX \theta+X^TX \theta-2X^T \vec{y})\\\\
& = X^TX \theta-X^T \vec{y}
\end{aligned}
$$

In the third step, we used the fact that the trace if a real number is just the real number; the fourth step used the fact that $\operatorname{tr}(A)=\operatorname{tr}(A^T)$ , and the fifth step used Equation (5) with $A^T=\theta$, $B=B^T=X^TX$ , and $C=I$, and Equation (1). To minimize $J$ , we set its derivatives to zero, and obtain the normal equations:

$$
X^TX \theta=X^T \vec{y}
$$

Thus, the value of $\theta$ that minimize $J(\theta)$ is given in closed form by the equation

$$
\theta = (X^TX)^{-1}X^T \vec{y}
$$

## 8. æ­£è§„æ–¹ç¨‹åŠä¸å¯é€†æ€§

!!! note ""

    å‚è€ƒè§†é¢‘: 4 - 7 - Normal Equation Noninvertibility (Optional) (6 min).mkv

åœ¨è¿™æ®µè§†é¢‘ä¸­è°ˆè°ˆæ­£è§„æ–¹ç¨‹ ( **normal equation** )ï¼Œä»¥åŠå®ƒä»¬çš„ä¸å¯é€†æ€§ã€‚

æˆ‘ä»¬è¦è®²çš„é—®é¢˜å¦‚ä¸‹ï¼š$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$ å½“è®¡ç®— $\theta$=`inv(X'X ) X'y` ï¼Œé‚£å¯¹äºçŸ©é˜µ $X'X$ çš„ç»“æœæ˜¯ä¸å¯é€†çš„æƒ…å†µå’‹åŠå‘¢?

æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œæœ‰äº›çŸ©é˜µå¯é€†(**invertible**)ï¼Œè€Œæœ‰äº›çŸ©é˜µä¸å¯é€†(**non-invertible**)ã€‚æˆ‘ä»¬ç§°é‚£äº›ä¸å¯é€†çŸ©é˜µä¸ºå¥‡å¼‚(**singular**)æˆ–é€€åŒ–(**dgenerate**)çŸ©é˜µã€‚

é¦–å…ˆ, è¯´ä¸€ä¸‹  **$\pmb X'\pmb X$ ä¸å¯é€†çš„åŸå› **ã€‚

- 1. **ç‰¹å¾å€¼çº¿æ€§ç›¸å…³**

    ä¾‹å¦‚ï¼Œåœ¨é¢„æµ‹ä½æˆ¿ä»·æ ¼æ—¶ï¼Œå¦‚æœ${x_{1}}$æ˜¯ä»¥è‹±å°ºä¸ºå°ºå¯¸è§„æ ¼è®¡ç®—çš„æˆ¿å­ï¼Œ${x_{2}}$æ˜¯ä»¥å¹³æ–¹ç±³ä¸ºå°ºå¯¸è§„æ ¼è®¡ç®—çš„æˆ¿å­ï¼ŒåŒæ—¶ï¼Œä½ ä¹ŸçŸ¥é“1ç±³ç­‰äº3.28è‹±å°º ( å››èˆäº”å…¥åˆ°ä¸¤ä½å°æ•° )ï¼Œè¿™æ ·ï¼Œä½ çš„è¿™ä¸¤ä¸ªç‰¹å¾å€¼å°†å§‹ç»ˆæ»¡è¶³çº¦æŸï¼š${x_{1}}={x_{2}}*{{\left( 3.28 \right)}^{2}}$ã€‚ å®é™…ä¸Šï¼Œå¦‚æœä½ ç”¨è¿™æ ·çš„ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ï¼Œæ¥å±•ç¤ºé‚£ä¸¤ä¸ªç›¸å…³è”çš„ç‰¹å¾å€¼ï¼ŒçŸ©é˜µ$X'X$å°†æ˜¯ä¸å¯é€†çš„ã€‚

- 2. **ç‰¹å¾å€¼çš„æ•°é‡å°äºè®­ç»ƒé›†çš„æ•°é‡**

    å…·ä½“åœ°è¯´ï¼Œåœ¨$m$å°äºæˆ–ç­‰äºnçš„æ—¶å€™ï¼Œä¾‹å¦‚ï¼Œæœ‰$m$ç­‰äº10ä¸ªçš„è®­ç»ƒæ ·æœ¬ä¹Ÿæœ‰$n$ç­‰äº100çš„ç‰¹å¾æ•°é‡ã€‚è¦æ‰¾åˆ°é€‚åˆçš„$(n +1)$ ç»´å‚æ•°çŸ¢é‡$\theta$ï¼Œè¿™å°†ä¼šå˜æˆä¸€ä¸ª101ç»´çš„çŸ¢é‡ï¼Œå°è¯•ä»10ä¸ªè®­ç»ƒæ ·æœ¬ä¸­æ‰¾åˆ°æ»¡è¶³101ä¸ªå‚æ•°çš„å€¼ï¼Œè¿™å·¥ä½œå¯èƒ½ä¼šè®©ä½ èŠ±ä¸Šä¸€é˜µå­æ—¶é—´ï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚å› ä¸ºï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°ä½ åªæœ‰10ä¸ªæ ·æœ¬ï¼Œä»¥é€‚åº”è¿™100æˆ–101ä¸ªå‚æ•°ï¼Œæ•°æ®è¿˜æ˜¯æœ‰äº›å°‘ã€‚**è¿™ç›¸å½“äºè¯´æ˜¯å¤šå…ƒæ–¹ç¨‹ç»„ä¸­æœªçŸ¥æ•°çš„ä¸ªæ•°è¿œå¤§äºæ–¹ç¨‹çš„ä¸ªæ•°ã€‚**

ç¨åæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œ**å¦‚ä½•ä½¿ç”¨å°æ•°æ®æ ·æœ¬ä»¥å¾—åˆ°è¿™100æˆ–101ä¸ªå‚æ•°**ï¼Œ**é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨**ä¸€ç§å«åš**æ­£åˆ™åŒ–**çš„çº¿æ€§ä»£æ•°æ–¹æ³•ï¼Œ**é€šè¿‡åˆ é™¤æŸäº›ç‰¹å¾æˆ–è€…æ˜¯ä½¿ç”¨æŸäº›æŠ€æœ¯ï¼Œæ¥è§£å†³å½“$m$æ¯”$n$å°çš„æ—¶å€™çš„é—®é¢˜**ã€‚å³ä½¿ä½ æœ‰ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„è®­ç»ƒé›†ï¼Œä¹Ÿå¯ä½¿ç”¨å¾ˆå¤šçš„ç‰¹å¾æ¥æ‰¾åˆ°å¾ˆå¤šåˆé€‚çš„å‚æ•°ã€‚

æ€»ä¹‹å½“ä½ å‘ç°çš„çŸ©é˜µ$X'X$çš„ç»“æœæ˜¯å¥‡å¼‚çŸ©é˜µï¼Œæˆ–è€…æ‰¾åˆ°çš„å…¶å®ƒçŸ©é˜µæ˜¯ä¸å¯é€†çš„ï¼Œæˆ‘ä¼šå»ºè®®ä½ è¿™ä¹ˆåšã€‚

é¦–å…ˆï¼Œ**çœ‹ç‰¹å¾å€¼é‡Œæ˜¯å¦æœ‰ä¸€äº›å¤šä½™çš„ç‰¹å¾**ï¼Œåƒè¿™äº›${x_{1}}$å’Œ${x_{2}}$æ˜¯**çº¿æ€§ç›¸å…³**çš„ï¼Œäº’ä¸ºçº¿æ€§å‡½æ•°ã€‚åŒæ—¶ï¼Œå½“æœ‰ä¸€äº›å¤šä½™çš„ç‰¹å¾æ—¶ï¼Œå¯ä»¥**åˆ é™¤**è¿™ä¸¤ä¸ªé‡å¤ç‰¹å¾é‡Œçš„**å…¶ä¸­ä¸€ä¸ª**ï¼Œæ— é¡»ä¸¤ä¸ªç‰¹å¾åŒæ—¶ä¿ç•™ï¼Œå°†è§£å†³ä¸å¯é€†æ€§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œé¦–å…ˆåº”è¯¥é€šè¿‡è§‚å¯Ÿæ‰€æœ‰ç‰¹å¾æ£€æŸ¥æ˜¯å¦æœ‰å¤šä½™çš„ç‰¹å¾ï¼Œå¦‚æœæœ‰å¤šä½™çš„å°±åˆ é™¤æ‰ï¼Œç›´åˆ°ä»–ä»¬ä¸å†æ˜¯å¤šä½™çš„ä¸ºæ­¢ï¼Œå¦‚æœ**ç‰¹å¾æ•°é‡**å®åœ¨**å¤ªå¤š**ï¼Œæˆ‘ä¼š**ç”¨è¾ƒå°‘çš„ç‰¹å¾**æ¥**åæ˜ å°½å¯èƒ½å¤šå†…å®¹**ï¼Œ**å¦åˆ™**æˆ‘ä¼šè€ƒè™‘**ä½¿ç”¨æ­£è§„åŒ–æ–¹æ³•**ã€‚

å¦‚æœçŸ©é˜µ$X'X$æ˜¯ä¸å¯é€†çš„ï¼Œï¼ˆé€šå¸¸æ¥è¯´ï¼Œä¸ä¼šå‡ºç°è¿™ç§æƒ…å†µï¼‰ï¼Œå¦‚æœåœ¨**Octave**é‡Œï¼Œå¯ä»¥ç”¨ä¼ªé€†å‡½æ•°`pinv()` æ¥å®ç°ã€‚è¿™ç§ä½¿ç”¨ä¸åŒçš„çº¿æ€§ä»£æ•°åº“çš„æ–¹æ³•è¢«ç§°ä¸ºä¼ªé€†ã€‚å³ä½¿$X'X$çš„ç»“æœæ˜¯ä¸å¯é€†çš„ï¼Œä½†ç®—æ³•æ‰§è¡Œçš„æµç¨‹æ˜¯æ­£ç¡®çš„ã€‚æ€»ä¹‹ï¼Œå‡ºç°ä¸å¯é€†çŸ©é˜µçš„æƒ…å†µæå°‘å‘ç”Ÿï¼Œæ‰€ä»¥åœ¨å¤§å¤šæ•°å®ç°çº¿æ€§å›å½’ä¸­ï¼Œå‡ºç°ä¸å¯é€†çš„é—®é¢˜ä¸åº”è¯¥è¿‡å¤šçš„å…³æ³¨${X^{T}}X$æ˜¯ä¸å¯é€†çš„ã€‚

## ä¹ é¢˜ && å‚è€ƒç­”æ¡ˆ
### ç¬¬ä¸€é¢˜

å‡è®¾m=4ä¸ªå­¦ç”Ÿä¸Šäº†ä¸€èŠ‚è¯¾, æœ‰æœŸä¸­è€ƒè¯•å’ŒæœŸæœ«è€ƒè¯•ã€‚ä½ å·²ç»æ”¶é›†äº†ä»–ä»¬åœ¨ä¸¤æ¬¡è€ƒè¯•ä¸­çš„åˆ†æ•°æ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

| æœŸä¸­å¾—åˆ† | (æœŸä¸­å¾—åˆ†)^2 | æœŸæœ«å¾—åˆ† |
| -------- | ------------ | -------- |
| 89       | 7921         | 96       |
| 72       | 5184         | 74       |
| 94       | 8836         | 87       |
| 69       | 4761         | 78       |

ä½ æƒ³ç”¨å¤šé¡¹å¼å›å½’æ¥é¢„æµ‹ä¸€ä¸ªå­¦ç”Ÿçš„æœŸä¸­è€ƒè¯•æˆç»©ã€‚å…·ä½“åœ°
è¯´, å‡è®¾ä½ æƒ³æ‹Ÿåˆä¸€ä¸ª $h_ \theta (x) = \theta _0+\theta _1x_1++\theta _2x_1$ çš„æ¨¡å‹, å…¶ä¸­x1æ˜¯æœŸä¸­å¾—åˆ†, x2æ˜¯ï¼ˆæœŸä¸­å¾—åˆ†ï¼‰^2ã€‚æ­¤å¤–, ä½ è®¡åˆ’åŒæ—¶ä½¿ç”¨ç‰¹å¾ç¼©æ”¾ï¼ˆé™¤ä»¥ç‰¹å¾çš„â€œæœ€å¤§å€¼-æœ€å°å€¼â€æˆ–èŒƒå›´ï¼‰å’Œå‡å€¼å½’ä¸€åŒ–ã€‚

é‚£ä¹ˆæ ‡å‡†åŒ–åçš„ $x_2^{(4)}$ ç‰¹å¾å€¼æ˜¯å¤šå°‘ï¼Ÿï¼ˆæç¤ºï¼šæœŸä¸­=89ï¼ŒæœŸæœ«=96æ˜¯è®­ç»ƒç¤ºä¾‹1ï¼‰

### ç¬¬äºŒé¢˜

ç”¨ $\alpha=0.3$ è¿›è¡Œ15æ¬¡æ¢¯åº¦ä¸‹é™è¿­ä»£, æ¯æ¬¡è¿­ä»£ $j(\theta)$ åè®¡ç®—ã€‚ä½ ä¼šå‘ç° $j(\theta)$ çš„å€¼ä¸‹é™ç¼“æ…¢, å¹¶ä¸”åœ¨15æ¬¡è¿­ä»£åä»åœ¨ä¸‹é™ã€‚åŸºäºæ­¤, ä»¥ä¸‹å“ªä¸ªç»“è®ºä¼¼ä¹æœ€å¯ä¿¡ï¼Ÿ

A.  $\alpha=0.3$ æ˜¯å­¦ä¹ ç‡çš„æœ‰æ•ˆé€‰æ‹©ã€‚

B. ä¸å…¶ä½¿ç”¨  $\alpha$ å½“å‰å€¼, ä¸å¦‚å°è¯•æ›´å°çš„  $\alpha$ å€¼ï¼ˆæ¯”å¦‚  $\alpha=0.1$ ï¼‰

C. ä¸å…¶ä½¿ç”¨  $\alpha$ å½“å‰å€¼, ä¸å¦‚å°è¯•æ›´å¤§çš„  $\alpha$ å€¼ï¼ˆæ¯”å¦‚  $\alpha=1.0$ ï¼‰

### ç¬¬ä¸‰é¢˜

å‡è®¾æ‚¨æœ‰m=14ä¸ªè®­ç»ƒç¤ºä¾‹, æœ‰n=3ä¸ªç‰¹æ€§ï¼ˆä¸åŒ…æ‹¬éœ€è¦å¦å¤–æ·»åŠ çš„æ’ä¸º1çš„æˆªè·é¡¹ï¼‰, æ­£è§„æ–¹ç¨‹æ˜¯ $\theta=(X^TX)^{-1}X^Ty$ã€‚å¯¹äºç»™å®šmå’Œnçš„å€¼, è¿™ä¸ªæ–¹ç¨‹ä¸­ $\theta, X, y$ çš„ç»´æ•°åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ

A. $X \  14 \times 3, y \  14\times 1, \  \theta 3 \times 3$

B. $X \  14 \times 4, y \  14\times 1, \  \theta 4 \times 1$

C. $X \  14 \times 3, y \  14\times 1, \  \theta 3 \times 1$

D. $X \  14 \times 4, y \  14\times 4, \  \theta 4 \times 4$

### ç¬¬ å›› é¢˜

å‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªç¤ºä¾‹æœ‰m=1000000ä¸ªç¤ºä¾‹å’Œn=200000ä¸ªç‰¹æ€§ã€‚ä½ æƒ³ç”¨å¤šå…ƒçº¿æ€§å›å½’æ¥æ‹Ÿåˆå‚æ•° $\theta$ åˆ°æˆ‘ä»¬çš„æ•°æ®ã€‚ä½ æ›´åº”è¯¥ç”¨æ¢¯åº¦ä¸‹é™è¿˜æ˜¯æ­£è§„æ–¹ç¨‹ï¼Ÿ

A. æ¢¯åº¦ä¸‹é™ï¼Œå› ä¸ºæ­£è§„æ–¹ç¨‹ä¸­ $\theta=(X^TX)^{-1}$ ä¸­è®¡ç®—éå¸¸æ…¢

B. æ­£è§„æ–¹ç¨‹ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸€ç§ç›´æ¥æ±‚è§£çš„æœ‰æ•ˆæ–¹æ³•

C. æ¢¯åº¦ä¸‹é™ï¼Œå› ä¸ºå®ƒæ€»æ˜¯æ”¶æ•›åˆ°æœ€ä¼˜ $\theta$

D. æ­£è§„æ–¹ç¨‹ï¼Œå› ä¸ºæ¢¯åº¦ä¸‹é™å¯èƒ½æ— æ³•æ‰¾åˆ°æœ€ä¼˜ $\theta$

### ç¬¬ äº” é¢˜

ä»¥ä¸‹å“ªäº›æ˜¯ä½¿ç”¨ç‰¹å¾ç¼©æ”¾çš„åŸå› ï¼Ÿ

A. å®ƒå¯ä»¥é˜²æ­¢æ¢¯åº¦ä¸‹é™é™·å…¥å±€éƒ¨æœ€ä¼˜

B. å®ƒé€šè¿‡é™ä½æ¢¯åº¦ä¸‹é™çš„æ¯æ¬¡è¿­ä»£çš„è®¡ç®—æˆæœ¬æ¥åŠ é€Ÿæ¢¯åº¦ä¸‹é™

C. å®ƒé€šè¿‡å‡å°‘è¿­ä»£æ¬¡æ•°æ¥è·å¾—ä¸€ä¸ªå¥½çš„è§£ï¼Œä»è€ŒåŠ å¿«äº†æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦

D. å®ƒé˜²æ­¢çŸ©é˜µ $X^TX$ï¼ˆç”¨äºæ­£è§„æ–¹ç¨‹ï¼‰ä¸å¯é€†ï¼ˆå¥‡å¼‚/é€€åŒ–ï¼‰

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
### å‚è€ƒç­”æ¡ˆ

ç¬¬ä¸€é¢˜ï¼š-0.47

<br/>
ç¬¬äºŒé¢˜ï¼šC

<br/>

ç¬¬ä¸‰é¢˜ï¼šB

<br/>

ç¬¬å››é¢˜ï¼šA

<br/>

ç¬¬äº”é¢˜ï¼šC


## ä¸Šæœºç»ƒä¹ 

In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.

&ensp; The file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.

### 1ã€Feature Normalization

``` py linenums="1" title="åšä¹‹å‰çœ‹ä¸€ä¸‹æ•°æ®çš„å¤§æ¦‚æ ·å­"
import pandas as pd
# path : æ•°æ®å­˜æ”¾çš„è·¯å¾„
path = "ex1data2.txt"
# names : æŒ‡å®šåˆ—å; head=None : åŸæ•°æ®æ²¡æœ‰åˆ—å
data = pd.read_csv(path,header=None,names=["Size", "Bedrooms","Price"])
# å±•ç¤ºæ•°æ®çš„åäº”è¡Œ
data.tail()
```

By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When feature differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.

- Substract the value of each feature from the dataset.
- After substracting the mean, additionally scale (divide) the feature values by their repective "standar deviations".

&ensp; The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within $\pm 2$ standard deviation of the mean); this is an alternative to taking the range of values(max-min). 

&ensp; You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix X corresponds to one feature.

``` py linenums="1" title="å¯¹æ•°æ®æ ‡å‡†åŒ–å¹¶æ’å…¥x0"
# å¯¹æ•°æ®è¿›è¡Œç‰¹å¾-æ ‡å‡†åŒ–
# æ³¨æ„pandasæŒ‰åˆ—åå–åˆ—æ˜¯ä¸¤ä¸ªä¸­æ‹¬å·
data2[["Size","Bedrooms"]] = (data2[["Size","Bedrooms"]]-data2[["Size","Bedrooms"]].mean())/data2[["Size","Bedrooms"]].std()
# æ„Ÿè§‰æŠŠæˆ¿ä»·ç¨å¾®ç¼©æ”¾ä¸€ä¸‹ï¼Œä¸ç„¶å¯¹æ¢¯åº¦å½±å“å¤ªå¤§
data2[["Price"]]=data2[["Price"]]/100000

# æ’å…¥x0
data2.insert(0, 'Ones',1)
data2.tail()
```

### 2ã€Gradient Descent

Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged.

!!! note "Implement Note:"

    In the multivariate case, the cost function can also be written in the following vectorized form: $J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)$ 
    where $X=\bigl( \begin{bmatrix} --- & (x^{(1)})^T & --- \\\\ --- & (x^{(2)})^T & ---\\\\ & \vdots & \\\\
    --- & (x^{(m)})^T & --- \end{bmatrix} \bigr)$ $y= \begin{bmatrix} y^{(1)} \\\\  y^{(2)} \\\\  \vdots \\\\ y^{(m)} \end{bmatrix}$.
    
    The vectorized version is efficient when you are working with numerical cumputing tools like numpy. If you are an expert with matrix operations, you can prove to yourself that the two forms are equivalent.



``` py linenums="1" title="è¿è¡Œå¤šå˜é‡æ¢¯åº¦ä¸‹é™"
import numpy as np
# è·å–æ•°æ®çš„æ€»åˆ—æ•°
cols = data2.shape[1]

# å°†æ•°æ®æ‹†åˆ†å¹¶è½¬ä¸ºndarryåˆ†åˆ«æ”¾åˆ°X,yä¸­
X = np.array(data2.iloc[:, 0:cols-1].values)
y = np.array(data2.iloc[:, cols-1].values)

# ä¸­é—´çœ‹ä¸€ä¸‹Xï¼Œyä»¥åŠXï¼Œyçš„ç»´åº¦æ˜¯å¦æ­£ç¡®
# X.shape,y.shape
# X, y

# åˆå§‹åŒ–theta, å­¦ä¹ ç‡alphaï¼Œè¿­ä»£æ¬¡æ•°iters
theta = np.zeros([3,])
alpha = 0.01
iters = 1000

# X.shape,y.shape,theta.shape

# å®šä¹‰ä»£ä»·å‡½æ•°å¹¶è®¡ç®—
def computeCost(X, y, theta):
    m = X.shape[0]
    # print((np.dot(X,theta)-y).shape)
    inner = np.dot((np.dot(X, theta)-y).T, (np.dot(X, theta)-y))
    return np.sum(inner)/(2*m)

# å®šä¹‰æ¢¯åº¦ä¸‹é™ç®—æ³•
def gradientDescent(X, y, theta, alpha, iterations):
    m = X.shape[0]  # m: æ ·æœ¬çš„æ€»ä¸ªæ•°
    n = len(theta)  # n: thetaçš„æ€»ä¸ªæ•°
    # ç”¨ä¸€ä¸ªå‘é‡æ¥è®°å½•è¿­ä»£è¿‡ç¨‹ä¸­æ‰€æœ‰çš„costå€¼
    costs = np.zeros(iterations)
    for i in range(iterations):
        costs[i] = computeCost(X, y, theta)
        # thetaæ˜¯å‡ ä¸ªå°±è¦æ›´æ–°å‡ ä¸ª
        for j in range(n):
            theta[j] = theta[j]-alpha * \
                (1/m)*np.sum((np.dot(X, theta)-y)*X[:, j])

    return theta, costs


# è¿è¡Œæ¢¯åº¦ä¸‹é™ï¼Œåˆ†åˆ«æ¥æ”¶æ›´æ–°åçš„thetaå€¼å’Œæ¯ä¸€æ­¥è¿­ä»£çš„costå€¼
theta_hat, costs = gradientDescent(X, y, theta, alpha, iters)

# get the cost (error) of the model
costs[iters-1], theta_hat
```

è¾“å‡º
``` py
(0.20435384903675252, array([ 3.40397964,  1.09859063, -0.05879178]))
``` 

#### 2.1 Selecting learning rates

In this part of the exercise, you will get to try out different learning rates for the dataset and find a learning rate that converages quickly.

&ensp; We recommend trying values of the **learning rate** $\alpha$ on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., **0.3, 0.1, 0.03, 0.01** and so on). You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve.

&ensp; Notice the changes in the convergence curves as the learning rate changes. With a small learning rate, you should find that gradient descent takes a very long time to converge to the optimal value. Conversely, with a large learning rate, gradient descent might not converge or might even diverge!

![Fig3_Convergence_of_gradient_descent with an appropriate learning rate](./../assets/images/Fig3_Convergence_of_gradient_descent.png)

### 3ã€Noramal Equation

In the lecture videos, you learned that the closed-form solution to linear regression is

$$
\theta = (X^TX)^{-1}X^Ty
$$

&ensp; Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no â€œloop until convergenceâ€ like in gradient descent.

**å†æ¬¡æé†’ï¼š**
**æ¢¯åº¦ä¸‹é™**ï¼šéœ€è¦é€‰æ‹©å­¦ä¹ ç‡ $\alpha$ï¼Œéœ€è¦å¤šæ¬¡è¿­ä»£ï¼Œå½“ç‰¹å¾æ•°é‡nå¤§æ—¶ä¹Ÿèƒ½è¾ƒå¥½é€‚ç”¨ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„æ¨¡å‹

**æ­£è§„æ–¹ç¨‹**ï¼šä¸éœ€è¦é€‰æ‹©å­¦ä¹ ç‡ $\alpha$ï¼Œä¸€æ¬¡è®¡ç®—å¾—å‡ºï¼Œéœ€è¦è®¡ç®— $X^TX$ï¼Œå¦‚æœç‰¹å¾æ•°é‡nè¾ƒå¤§åˆ™è¿ç®—ä»£ä»·å¤§ï¼Œå› ä¸ºçŸ©é˜µé€†çš„è®¡ç®—æ—¶é—´å¤æ‚åº¦ä¸ºğ‘‚(ğ‘›3)ï¼Œ**é€šå¸¸æ¥è¯´å½“ğ‘›å°äº10000 æ—¶**è¿˜æ˜¯å¯ä»¥æ¥å—çš„ï¼Œåªé€‚ç”¨äº**çº¿æ€§æ¨¡å‹**ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹ã€‚

``` py linenums="1" title="æ­£è§„æ–¹ç¨‹è®¡ç®—theta"
# åˆ©ç”¨å…¬å¼ä¸€æ­¥è®¡ç®—å‡ºå‚æ•°theta
theta_ne = np.linalg.inv(X.T@X)@X.T@y #X.T@Xç­‰ä»·äºX.T.dot(X)
theta_ne
```

**è¾“å‡º**

``` py
array([ 3.4041266 ,  1.1063105 , -0.06649474])
```

æˆ‘ä»¬ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•è®¡ç®—çš„thetaä¸ºï¼š
[ 3.40397964,  1.09859063, -0.05879178]

ä¸¤è€…ç›¸å·®ä¸æ˜¯ç‰¹åˆ«å¤§ï¼Œè¯´æ˜æˆ‘ä»¬æ¢¯åº¦ä¸‹é™æ¨¡å‹è¿˜æ˜¯æ¯”è¾ƒå‡†ç¡®çš„ï¼Œå“ˆå“ˆå“ˆã€‚è‡³æ­¤ï¼Œæˆ‘å·²ç»æŒæ¡äº†ä¸¤ä¸ªç®—æ³•ï¼Œæ¢¯åº¦ä¸‹é™æ¨¡å‹å’Œæ­£è§„æ–¹ç¨‹æ±‚è§£çº¿æ€§å›å½’æ¨¡å‹ã€‚æˆ‘çœŸæ£’ï¼Œæ˜å¤©å¼€å§‹å­¦ä¹ æ–°çš„é€»è¾‘å›å½’æ¨¡å‹ï¼ŒåŠ æ²¹ï¼ï¼ï¼

